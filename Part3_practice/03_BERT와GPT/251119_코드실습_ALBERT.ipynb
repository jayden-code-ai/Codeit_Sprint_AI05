{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a6d9fb",
   "metadata": {},
   "source": [
    "# ALBERT ëª¨ë¸\n",
    "\n",
    "ê¸°ë³¸ ì ìœ¼ë¡œ BERT ëª¨ë¸ì€ ì¸ì½”ë”© ë¸”ë¡ì´ ê¹Šì–´ì§ˆ ìˆ˜ ë¡ íŒŒë¼ë¯¸í„°ì˜ ê·œëª¨ê°€ ë§¤ìš° ì»¤ì§ˆ ë¿ë§Œ ì•„ë‹ˆë¼, ë§¤ìš° í° ì–´íœ˜ ì‚¬ì „(vocab)ê³¼ ì„ë² ë”© ê¸¸ì´ë¥¼ ì‚¬ìš©í•  ë•Œ, ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ì»¤ì ¸ ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.\n",
    "\n",
    "ALBERTëŠ” í¬ê²Œ 2ê°€ì§€ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì´ë¥¼ í¬ê²Œ ë‚®ì¶”ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„° ê·œëª¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "- **ì„ë² ë”© íŒŒë¼ë¯¸í„°ì˜ ë¶„ë¦¬(Factorized Embedding Parameterization)**\n",
    "\n",
    "    * (ë‹¨ì–´ ì„ë² ë”© ì°¨ì›) â†’ (í”„ë¡œì ì…˜ ë ˆì´ì–´) â†’ (Transformer hidden ì°¨ì›)ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© í–‰ë ¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í¬ê²Œ ê°ì†Œ\n",
    "\n",
    "    * ë‹¨ì–´ ì„ë² ë”©ì€ ì‘ì€ ì°¨ì›(ì˜ˆ: 128ì°¨ì›)ìœ¼ë¡œ ë‘ê³ , ì´í›„ ì„ í˜• ë ˆì´ì–´ë¥¼ í†µí•´ Transformer ë¸”ë¡ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìˆ¨ê¹€(hidden) ì°¨ì›(ì˜ˆ: 256~768ì°¨ì› ë“±)ìœ¼ë¡œ ë§¤í•‘\n",
    "\n",
    "- **í¬ë¡œìŠ¤-ë ˆì´ì–´ íŒŒë¼ë¯¸í„° ê³µìœ (Cross-Layer Parameter Sharing)**\n",
    "\n",
    "    * ì—¬ëŸ¬ ê°œì˜ Transformer ë ˆì´ì–´ê°€ ëª¨ë‘ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ì—¬, í•œ ê°œì˜ Transformer ë¸”ë¡ì„ Në²ˆ ë°˜ë³µí•´ì„œ í˜¸ì¶œ(ë ˆì´ì–´ë¥¼ ë³µì‚¬í•´ì„œ ì“°ëŠ” ê²ƒì´ ì•„ë‹Œ, â€˜ê°™ì€â€™ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°)\n",
    "\n",
    "ë˜í•œ, ALBERTëŠ” BERTì˜ NSP ì‚¬ì „í•™ìŠµ ëŒ€ì‹  ë‘ ë¬¸ì¥ì˜ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ë¥¼ ë§íˆëŠ” SOP ì‚¬ì „í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. SOPëŠ” NSPì˜ ëœë¤ ë°©ì‹ì— ë¹„í•´ ì–¸ì–´ì  ì¼ê´€ì„±ê³¼ ë¬¸ë§¥ ì—°ê²°ì„±ì„ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c3fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb45c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24dee27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS01</th>\n",
       "      <th>SS01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.</td>\n",
       "      <td>ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
       "      <td>ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...</td>\n",
       "      <td>íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...</td>\n",
       "      <td>ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
       "      <td>ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51623</th>\n",
       "      <td>ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .</td>\n",
       "      <td>ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51624</th>\n",
       "      <td>ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.</td>\n",
       "      <td>ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51625</th>\n",
       "      <td>ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.</td>\n",
       "      <td>ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51626</th>\n",
       "      <td>ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.</td>\n",
       "      <td>ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51627</th>\n",
       "      <td>ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.</td>\n",
       "      <td>ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51628 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    HS01  \\\n",
       "0                              ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.   \n",
       "1         ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
       "2      íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤...   \n",
       "3      ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ...   \n",
       "4                  ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
       "...                                                  ...   \n",
       "51623     ë‚˜ì´ê°€ ë¨¹ê³  ì´ì œ ëˆë„ ëª» ë²Œì–´ ì˜¤ë‹ˆê¹Œ ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ì§€ ë§‰ë§‰í•´. ëŠ¥ë ¥ë„ ì—†ê³ .   \n",
       "51624        ëª¸ì´ ë§ì´ ì•½í•´ì¡Œë‚˜ ë´. ì´ì œ ì „ê³¼ ê°™ì´ ì¼í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ ë„ˆë¬´ ì§œì¦ ë‚˜.   \n",
       "51625   ì´ì œ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´. ë‚¨í¸ë„ ê·¸ë ‡ê³  ë…¸í›„ ì¤€ë¹„ë„ ì•ˆ ë˜ì–´ì„œ ë¯¸ë˜ê°€ ê±±ì •ë¼.   \n",
       "51626  ëª‡ì‹­ ë…„ì„ í•¨ê»˜ ì‚´ì•˜ë˜ ë‚¨í¸ê³¼ ì´í˜¼í–ˆì–´. ê·¸ë™ì•ˆì˜ ì„¸ì›”ì— ë°°ì‹ ê°ì„ ëŠë¼ê³  ë„ˆë¬´ í™”ê°€ ë‚˜.   \n",
       "51627  ë‚¨í¸ê³¼ ê²°í˜¼í•œ ì§€ ì‚¬ì‹­ ë…„ì´ì•¼. ì´ì œ ì‚¬ëŒ ë§Œë‚˜ëŠ” ê²ƒë„ ë²„ê²ê³  ì•Œë˜ ì‚¬ëŒë„ ì ì  ì‚¬ë¼ì ¸.   \n",
       "\n",
       "                                                    SS01  \n",
       "0                            ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?  \n",
       "1               ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?  \n",
       "2      íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ ...  \n",
       "3      ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆ...  \n",
       "4      ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ”...  \n",
       "...                                                  ...  \n",
       "51623                 ê²½ì œì ì¸ ë¬¸ì œ ë•Œë¬¸ì— ë§‰ë§‰í•˜ì‹œêµ°ìš”. ë§ˆìŒì´ í¸ì¹˜ ì•Šìœ¼ì‹œê² ì–´ìš”.  \n",
       "51624              ê±´ê°•ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹œêµ°ìš”. ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
       "51625                      ë…¸í›„ ì¤€ë¹„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ì´ ë§ìœ¼ì‹œê² ì–´ìš”.  \n",
       "51626                               ê°€ì¡±ê³¼ì˜ ë¬¸ì œ ë•Œë¬¸ì— ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
       "51627                    ëŒ€ì¸ê´€ê³„ì— ëŒ€í•œ ì–´ë ¤ì›€ ë•Œë¬¸ì— ê±±ì •ë˜ì‹œê³  ì†ìƒí•˜ì‹œê² ì–´ìš”.  \n",
       "\n",
       "[51628 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df[['HS01', 'SS01']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17cbe7",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ë°ì´í„°ì„¸íŠ¸ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9207d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train.txt\n",
      "  input_format: \n",
      "  model_prefix: ./bpe/spm_krsent\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: train.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51628 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=1801728\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=985\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51628 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=743763\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 45742 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51628\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 53987\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 53987 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25413 obj=11.1354 num_tokens=102939 num_tokens/piece=4.05064\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20986 obj=10.1275 num_tokens=103491 num_tokens/piece=4.93143\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15738 obj=10.1657 num_tokens=109176 num_tokens/piece=6.93709\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15725 obj=10.1329 num_tokens=109182 num_tokens/piece=6.94321\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11793 obj=10.3601 num_tokens=117126 num_tokens/piece=9.93182\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11793 obj=10.3309 num_tokens=117134 num_tokens/piece=9.9325\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8844 obj=10.6352 num_tokens=125939 num_tokens/piece=14.24\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8844 obj=10.5972 num_tokens=125942 num_tokens/piece=14.2404\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6633 obj=10.966 num_tokens=134876 num_tokens/piece=20.3341\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6633 obj=10.9212 num_tokens=134877 num_tokens/piece=20.3342\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4974 obj=11.3932 num_tokens=144116 num_tokens/piece=28.9739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4974 obj=11.3374 num_tokens=144116 num_tokens/piece=28.9739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3730 obj=11.8971 num_tokens=154191 num_tokens/piece=41.3381\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3730 obj=11.8276 num_tokens=154190 num_tokens/piece=41.3378\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2797 obj=12.4268 num_tokens=165297 num_tokens/piece=59.098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2797 obj=12.3372 num_tokens=165301 num_tokens/piece=59.0994\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=12.928 num_tokens=175716 num_tokens/piece=79.8709\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=12.7634 num_tokens=176159 num_tokens/piece=80.0723\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ./bpe/spm_krsent.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ./bpe/spm_krsent.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train.txt\n",
      "  input_format: \n",
      "  model_prefix: ./bpe/spm_krsent\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: train.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 51628 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=1801728\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=985\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 51628 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=743763\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 45742 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 51628\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 53987\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 53987 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25413 obj=11.1354 num_tokens=102939 num_tokens/piece=4.05064\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20986 obj=10.1275 num_tokens=103491 num_tokens/piece=4.93143\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15738 obj=10.1657 num_tokens=109176 num_tokens/piece=6.93709\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15725 obj=10.1329 num_tokens=109182 num_tokens/piece=6.94321\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11793 obj=10.3601 num_tokens=117126 num_tokens/piece=9.93182\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11793 obj=10.3309 num_tokens=117134 num_tokens/piece=9.9325\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8844 obj=10.6352 num_tokens=125939 num_tokens/piece=14.24\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8844 obj=10.5972 num_tokens=125942 num_tokens/piece=14.2404\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6633 obj=10.966 num_tokens=134876 num_tokens/piece=20.3341\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6633 obj=10.9212 num_tokens=134877 num_tokens/piece=20.3342\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4974 obj=11.3932 num_tokens=144116 num_tokens/piece=28.9739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4974 obj=11.3374 num_tokens=144116 num_tokens/piece=28.9739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4400 obj=11.5884 num_tokens=148372 num_tokens/piece=33.7209\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4400 obj=11.5639 num_tokens=148371 num_tokens/piece=33.7207\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ./bpe/spm_krsent.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ./bpe/spm_krsent.vocab\n"
     ]
    }
   ],
   "source": [
    "# BPE :ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©(Byte Pair Encoding, BPE)\n",
    "import os, re\n",
    "\n",
    "# ì¶”ê°€ ì“°ê¸°ëª¨ë“œë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ì—´ê¸°\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "  for text in df['HS01']:\n",
    "        text = str(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)     # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "        text = re.sub(r'[\\n\\t]', ' ', text)     # ì¤„ë°”ê¿ˆ, íƒ­ ì œê±°\n",
    "        text = re.sub(r'\\s+', ' ', text)        # ì—°ì†ëœ ê³µë°± ì œê±°\n",
    "        text= text.strip()                      # ë¬¸ì¥ ì–‘ë ê³µë°± ì œê±°\n",
    "        try:\n",
    "            f.write(text+'\\n')\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ ìƒì„±\n",
    "os.makedirs('./bpe', exist_ok=True)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='train.txt',                      # í…ìŠ¤íŠ¸ ë­‰ì¹˜ íŒŒì¼\n",
    "    model_prefix='./bpe/spm_krsent',        # ì¶œë ¥ ëª¨ë¸ íŒŒì¼ ì´ë¦„\n",
    "    vocab_size=2000                         # í† í° ê°œìˆ˜\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(input='train.txt',               # í…ìŠ¤íŠ¸ ë­‰ì¹˜ íŒŒì¼\n",
    "                            model_prefix='./bpe/spm_krsent',    # ì¶œë ¥ ëª¨ë¸ íŒŒì¼ ì´ë¦„\n",
    "                            vocab_size=4000,                    # í† í° ê°œìˆ˜\n",
    "                            bos_id=1,\n",
    "                            eos_id=2,\n",
    "                            unk_id=3,\n",
    "                            pad_id=0\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ce4aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 'ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 0), ('ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 'ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 1), ('ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 0), ('ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 'ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1), ('íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 'íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 0), ('íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 'íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 1), ('ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 'ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 0), ('ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 'ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 1), ('ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 0), ('ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 'ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1)]\n",
      "ì…ë ¥ í† í° : tensor([[   1,  604,    6, 3112,    4, 3774,  308, 1124,  945,  322,   20,  162,\n",
      "           55, 2212,    3,  685,   55,    5, 2169,    3,    2,   55,    5, 1335,\n",
      "          278,  256,  245,  143,  760,  783,    3,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1, 1212, 1130, 2044,   53,  385, 3675,  245,  342,   18,   22,  783,\n",
      "            3,    2,   16, 1938,   59, 1371,   14, 1344,  102,   10,   44,    3,\n",
      "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "SOP ë¼ë²¨ : tensor([0, 1])\n",
      "íŒ¨ë”© ë§ˆìŠ¤í¬ : tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "class SPDataSet(Dataset):\n",
    "    def __init__(self, sp, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(f'./train.csv')\n",
    "        self.sp = sp\n",
    "\n",
    "        # SOP íƒœìŠ¤í¬ë¥¼ ìœ„í•œ (sent1, sent2, label) ë¦¬ìŠ¤íŠ¸\n",
    "        self.pairs = []\n",
    "\n",
    "        # \"ì˜¬ë°”ë¥¸ ìˆœì„œ\" => label = 0\n",
    "        # \"ë’¤ì§‘íŒ ìˆœì„œ\" => label = 1\n",
    "        for _, item in self.df.iterrows():\n",
    "            sent1 = item['HS01']\n",
    "            sent2 = item['SS01']\n",
    "\n",
    "            # ì›ë˜ ìˆœì„œ\n",
    "            self.pairs.append((sent1, sent2, 0))\n",
    "            # ë’¤ì§‘íŒ ìˆœì„œ\n",
    "            self.pairs.append((sent2, sent1, 1))\n",
    "\n",
    "        # 10ê°œì˜ ë¬¸ì¥ìŒë§Œ í™•ì¸ (ë””ë²„ê·¸ ìš©ë„)\n",
    "        print(self.pairs[:10])\n",
    "\n",
    "    def zero_pad(self, tok):\n",
    "        \"\"\"í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ max_len ê¸¸ì´ì— ë§ì¶° ì œë¡œ íŒ¨ë”©\"\"\"\n",
    "        if len(tok) >= self.max_len:\n",
    "            return tok[:self.max_len]\n",
    "        else:\n",
    "            padding = np.zeros(self.max_len, dtype=np.int64)\n",
    "            padding[:len(tok)] = tok\n",
    "            return padding\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent1, sent2, label = self.pairs[idx]\n",
    "\n",
    "        # SentencePieceë¡œ ë¬¸ì¥ â†’ ID ì‹œí€€ìŠ¤\n",
    "        sent1_ids = self.sp.encode_as_ids(sent1)\n",
    "        sent2_ids = self.sp.encode_as_ids(sent2)\n",
    "\n",
    "        # ë‘ ë¬¸ì¥ì„ [BOS] + sent1 + [EOS] + sent2 + [EOS]ë¡œ ì´ì–´ ë¶™ì„\n",
    "        inp = [self.sp.bos_id()] + sent1_ids + [self.sp.eos_id()] \\\n",
    "              + sent2_ids + [self.sp.eos_id()]\n",
    "\n",
    "        inp = self.zero_pad(inp)  # (max_len,) numpy array\n",
    "        inp_tensor = torch.tensor(inp, dtype=torch.long)\n",
    "\n",
    "        # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "        mask = inp_tensor.eq(0)   # True/False í…ì„œ\n",
    "\n",
    "        return inp_tensor, torch.tensor(label, dtype=torch.long), mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "\n",
    "# ì˜ˆì‹œ ì‚¬ìš©\n",
    "sp = spm.SentencePieceProcessor(model_file=f'./bpe/spm_krsent.model')\n",
    "dataset = SPDataSet(sp, max_len=60)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for inp, tar, mask in dataloader:\n",
    "    print(f'ì…ë ¥ í† í° : {inp}')\n",
    "    print(f'SOP ë¼ë²¨ : {tar}')\n",
    "    print(f'íŒ¨ë”© ë§ˆìŠ¤í¬ : {mask}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947bd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens_for_mlm(input_ids, special_tokens_ids, mask_token_id, vocab_size, mlm_prob=0.15):\n",
    "    device = input_ids.device\n",
    "    B, L = input_ids.shape\n",
    "\n",
    "    # ì…ë ¥ í† í°ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    # ë¼ë²¨ í† í°ì„ ìœ„í•´ ë™ì¼ í˜•ìƒì˜ -100 í…ì„œ ìƒì„±\n",
    "    mlm_labels = torch.full_like(input_ids, -100)  # -100ì€ ë¼ë²¨ì—ì„œ ë¬´ì‹œë˜ëŠ” í† í°ìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "    # ìŠ¤í˜ì…œ í† í° ìœ„ì¹˜ë¥¼ True\n",
    "    special_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "    for sp_id in special_tokens_ids:\n",
    "        special_mask |= (input_ids == sp_id)\n",
    "\n",
    "    # 0~1 ì‚¬ì´ì˜ ë¬´ì‘ìœ„ í™•ë¥  ë¶„í¬ ìƒì„±\n",
    "    rand_vals = torch.rand_like(input_ids.float())\n",
    "\n",
    "    # mlm_prob(0.15) ë³´ë‹¤ ì‘ê³  ìŠ¤í˜ì…œ í† í°ì¸ ì•„ë‹Œê²½ìš°ë§Œ\n",
    "    to_mask = (rand_vals < mlm_prob) & (~special_mask)\n",
    "\n",
    "    # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ ì •ë‹µ ë ˆì´ë¸” = ì›ë³¸ í† í°\n",
    "    mlm_labels[to_mask] = input_ids[to_mask]\n",
    "\n",
    "    # BERT ë…¼ë¬¸ ë¹„ìœ¨: 80% -> [MASK], 10% -> random, 10% -> ì›ë³¸\n",
    "    mask_choices = torch.rand_like(input_ids.float())\n",
    "\n",
    "    # mask_choicesì—ì„œ 80%ì— í•´ë‹¹í•˜ê³  ë§ˆìŠ¤í¬ ì¸ë±ìŠ¤ì¸ ê²½ìš° ë§ˆìŠ¤í¬ í† í°ìœ¼ë¡œ\n",
    "    mask_1 = (mask_choices < 0.8) & to_mask\n",
    "    masked_input_ids[mask_1] = mask_token_id\n",
    "\n",
    "    # mask_choicesì—ì„œ 10%ì— í•´ë‹¹í•˜ê³  ë§ˆìŠ¤í¬ ì¸ë±ìŠ¤ì¸ ê²½ìš° ëœë¤ í† í° ë³€í™˜\n",
    "    mask_2 = (mask_choices >= 0.8) & (mask_choices < 0.9) & to_mask\n",
    "    random_tokens = torch.randint(0, vocab_size, size=(), device=device)\n",
    "    masked_input_ids[mask_2] = random_tokens\n",
    "\n",
    "    return masked_input_ids, mlm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf737a",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aea2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ğŸ—ï¸ Multi-Head Transformer Block\n",
    "    - Transformerì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ\n",
    "    - Self-Attention + Feed Forward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, nhead, feed_dim=512, gelu=False, dropout=0.):\n",
    "        super(MTBlock, self).__init__()\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(hidden_dim, nhead, dropout=dropout, batch_first=True)\n",
    "        if gelu:\n",
    "            self.active = nn.GELU()\n",
    "        else:\n",
    "            self.active = nn.ReLU()\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, feed_dim),\n",
    "            self.active,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feed_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # ì…€í”„ ì–´í…ì…˜\n",
    "        attn_output, _ = self.mha(x, x, x, key_padding_mask=mask, need_weights=False)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(x + attn_output)\n",
    "\n",
    "        # í”¼ë“œí¬ì›Œë“œ\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.norm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645177eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_pos=10000):\n",
    "        super().__init__()\n",
    "        # ìœ„ì¹˜ ì¸ë±ìŠ¤\n",
    "        position = torch.arange(max_pos).unsqueeze(1)\n",
    "        # ìœ„ì¹˜ì— ë”°ë¥¸ ê°ë„ ì¶”ì¶œ\n",
    "        # ì§€ìˆ˜(exp)ì™€ ë¡œê·¸(log)ë¥¼ í•¨ê»˜ì¨ ì§€ìˆ˜ìŠ¹ í˜•íƒœ(**)ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "\n",
    "        # ì œë¡œ í…ì„œë¥¼ ë§Œë“¤ê³  sin,cos ê²°ê³¼ í• ë‹¹\n",
    "        pe = torch.zeros(1, max_pos, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe) # í•™ìŠµë˜ì§€ ì•ŠëŠ” ëª¨ë“ˆë¡œ ì €ì¥\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ì„ë² ë”©ì´ ì…ë ¥ë˜ë©´ í¬ì§€ì…˜ê³¼ ë”í•˜ì—¬ ë°˜í™˜\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d235680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 60, 4000])\n",
      "tensor([[ 0.5360,  0.3382],\n",
      "        [-0.0013, -0.1895]])\n"
     ]
    }
   ],
   "source": [
    "class SimpleALBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256,\n",
    "        num_heads=4,\n",
    "        feed_dim=512,\n",
    "        num_layers=4,\n",
    "        num_classes=2,   # SOP ë˜í•œ ì´ì§„ ë¶„ë¥˜\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ì‘ì€ ì°¨ì›ì˜ ë‹¨ì–´ ì„ë² ë”©\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # ì„ë² ë”© í”„ë¡œì ì…˜ â†’ hidden_dim\n",
    "        self.embedding_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”©\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ALBERT: ëª¨ë“  ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ê³µìœ \n",
    "        self.transformer_block = MTBlock(hidden_dim, num_heads, feed_dim, dropout=dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # MLM í—¤ë“œ\n",
    "        self.mlm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # SOP í—¤ë“œ\n",
    "        self.sop_head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # ì„ë² ë”© + í”„ë¡œì ì…˜\n",
    "        emb = self.word_embeddings(x)            # (B, L, embedding_dim)\n",
    "        emb = self.embedding_proj(emb)           # (B, L, hidden_dim)\n",
    "\n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”© & ë“œë¡­ì•„ì›ƒ\n",
    "        emb = self.pos_encoding(emb)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        # ë™ì¼í•œ Transformer ë¸”ë¡ì„ num_layersë²ˆ ë°˜ë³µ(ê°€ì¤‘ì¹˜ ê³µìœ )\n",
    "        x_out = emb\n",
    "        for _ in range(self.num_layers):\n",
    "            x_out = self.transformer_block(x_out, mask)\n",
    "\n",
    "        # MLM ë¡œì§“\n",
    "        mlm_logits = self.mlm_head(x_out)        # (B, L, vocab_size)\n",
    "\n",
    "        # CLS í† í°(ì²« ë²ˆì§¸ í† í°)ìœ¼ë¡œ SOP ë¡œì§“\n",
    "        cls_emb = x_out[:, 0]                    # (B, hidden_dim)\n",
    "        sop_logits = self.sop_head(cls_emb)      # (B, 2)\n",
    "\n",
    "        return mlm_logits, sop_logits\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "model = SimpleALBERT(vocab_size=vocab_size)\n",
    "with torch.no_grad():\n",
    "    for inp, tar, mask in dataloader:\n",
    "        mlm_logits, nsp_logits = model(inp.long(), mask)\n",
    "        print(mlm_logits.shape)\n",
    "        print(nsp_logits)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c188f3",
   "metadata": {},
   "source": [
    "## ëª¨ë¸í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf06516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 'ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 0), ('ë§ì´ í˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìˆë‚˜ìš”?', 'ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.', 1), ('ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 0), ('ê¸‰ì—¬ê°€ ì¤„ì–´ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì›”ê¸‰ì´ ì¤„ì–´ë“  ê²ƒì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ì‹¤ ê±´ê°€ìš”?', 'ì´ë²ˆ ë‹¬ì— ë˜ ê¸‰ì—¬ê°€ ê¹ì˜€ì–´! ë¬¼ê°€ëŠ” ì˜¤ë¥´ëŠ”ë° ì›”ê¸‰ë§Œ ìê¾¸ ê¹ì´ë‹ˆê¹Œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1), ('íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 'íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 0), ('íšŒì‚¬ ë™ë£Œ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ëŠ” ê²ƒ ê°™ì•„ìš”. ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ë…¸ë ¥ì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”?', 'íšŒì‚¬ì— ì‹ ì…ì´ ë“¤ì–´ì™”ëŠ”ë° ë§íˆ¬ê°€ ê±°ìŠ¬ë ¤. ê·¸ëŸ° ì• ë¥¼ ë§¤ì¼ ë´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„. ', 1), ('ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 'ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 0), ('ê´€ë ¨ ì—†ëŠ” ì‹¬ë¶€ë¦„ì„ ëª¨ë‘ í•˜ê²Œ ë˜ì–´ì„œ ë…¸ì—¬ìš°ì‹œêµ°ìš”. ì–´ë–¤ ê²ƒì´ ìƒí™©ì„ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆê²Œ ë„ì›€ì„ ì¤„ê¹Œìš”?', 'ì§ì¥ì—ì„œ ë§‰ë‚´ë¼ëŠ” ì´ìœ ë¡œ ë‚˜ì—ê²Œë§Œ ì˜¨ê°– ì‹¬ë¶€ë¦„ì„ ì‹œì¼œ. ì¼ë„ ë§ì€ ë° ì •ë§ ë¶„í•˜ê³  ì„­ì„­í•´.', 1), ('ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 'ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 0), ('ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì€ íƒœë„ì— í™”ê°€ ë‚˜ì…¨êµ°ìš”. ìƒëŒ€ë°©ì˜ ì–´ë–¤ í–‰ë™ì´ ê·¸ëŸ° ê°ì •ì„ ìœ ë°œí•˜ëŠ” ê²ƒì¼ê¹Œìš”?', 'ì–¼ë§ˆ ì „ ì…ì‚¬í•œ ì‹ ì…ì‚¬ì›ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë„ˆë¬´ í™”ê°€ ë‚˜.', 1)]\n",
      "train dataset size: 82604\n",
      "test dataset size: 20652\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
    "seq_len = 100\n",
    "embed_dim = 64\n",
    "hidden_dim=128\n",
    "num_heads = 4\n",
    "feed_dim = 256\n",
    "num_layers = 4\n",
    "num_classes = 2\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "\n",
    "# MLM ë³€í™˜ í•¨ìˆ˜ì— ë„£ì–´ì¤„ ìŠ¤í˜ì…œ í† í° ì •ì˜\n",
    "mask_id = sp.get_piece_size()\n",
    "special_tokens_ids = [sp.bos_id(), sp.eos_id(), 0]\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í† í°ì´ ì¶”ê°€ë˜ë¯€ë¡œ í† í°ê°œìˆ˜ì— +1\n",
    "sp = spm.SentencePieceProcessor(model_file=f'./bpe/spm_krsent.model')\n",
    "vocab_size = sp.get_piece_size() + 1\n",
    "\n",
    "# ë°ì´í„°ì„¸íŠ¸ ë¶„í• \n",
    "dataset = SPDataSet(sp, seq_len)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "test_dataset, train_dataset = random_split(dataset, [0.2, 0.8], generator=generator1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'train dataset size: {len(train_dataset)}')\n",
    "print(f'test dataset size: {len(test_dataset)}')\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleALBERT(vocab_size, embed_dim, hidden_dim, num_heads, feed_dim, num_layers, num_classes).to(device)\n",
    "\n",
    "# 2ê°œì˜ ì†ì‹¤ ìƒì„±\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # MLM\n",
    "sop_loss_fn = nn.CrossEntropyLoss() # SOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0581000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 200, MLM Loss: 7.3187127733230595, SOP Loss:0.6283463606238365\n",
      "Epoch: 0, Batch: 400, MLM Loss: 6.927656067609787, SOP Loss:0.4134444233123213\n",
      "Epoch: 0, Batch: 600, MLM Loss: 6.747443060874939, SOP Loss:0.30776403459720314\n",
      "Epoch: 0, Batch: 800, MLM Loss: 6.651004012823105, SOP Loss:0.24826305798138493\n",
      "Epoch: 0, Batch: 1000, MLM Loss: 6.585040906906128, SOP Loss:0.21020435294508935\n",
      "Epoch: 0, Batch: 1200, MLM Loss: 6.536348661184311, SOP Loss:0.1836749174426465\n",
      "Train ===> Epoch 1 Loss: 6.692058617170499\n",
      "Val ===> Epoch 1, Val_SOP_Accuracy: 0.9972426295280457\n",
      "------------------------------\n",
      "Epoch: 1, Batch: 200, MLM Loss: 6.259123477935791, SOP Loss:0.04738148829317652\n",
      "Epoch: 1, Batch: 400, MLM Loss: 6.241565569639206, SOP Loss:0.04564002159342635\n",
      "Epoch: 1, Batch: 600, MLM Loss: 6.2229901091257736, SOP Loss:0.044550782042594314\n",
      "Epoch: 1, Batch: 800, MLM Loss: 6.2064737159013745, SOP Loss:0.04221549267982482\n",
      "Epoch: 1, Batch: 1000, MLM Loss: 6.188907267093659, SOP Loss:0.0404952072922606\n",
      "Epoch: 1, Batch: 1200, MLM Loss: 6.172088462909063, SOP Loss:0.03859416975057684\n",
      "Train ===> Epoch 2 Loss: 6.202557846361131\n",
      "Val ===> Epoch 2, Val_SOP_Accuracy: 0.9982584714889526\n",
      "------------------------------\n",
      "Epoch: 2, Batch: 200, MLM Loss: 6.03374546289444, SOP Loss:0.033047688705846665\n",
      "Epoch: 2, Batch: 400, MLM Loss: 6.017869786024094, SOP Loss:0.028179449963208755\n",
      "Epoch: 2, Batch: 600, MLM Loss: 5.996006206671397, SOP Loss:0.026917247423455894\n",
      "Epoch: 2, Batch: 800, MLM Loss: 5.970956178307533, SOP Loss:0.025387237464237843\n",
      "Epoch: 2, Batch: 1000, MLM Loss: 5.949099884986877, SOP Loss:0.025428408355975988\n",
      "Epoch: 2, Batch: 1200, MLM Loss: 5.9300405077139535, SOP Loss:0.02467678289608254\n",
      "Train ===> Epoch 3 Loss: 5.943627798802755\n",
      "Val ===> Epoch 3, Val_SOP_Accuracy: 0.9987422227859497\n",
      "------------------------------\n",
      "Epoch: 3, Batch: 200, MLM Loss: 5.757482211589814, SOP Loss:0.023201143449696246\n",
      "Epoch: 3, Batch: 400, MLM Loss: 5.740622180700302, SOP Loss:0.021348257413337707\n",
      "Epoch: 3, Batch: 600, MLM Loss: 5.722808945973714, SOP Loss:0.02125923924924185\n",
      "Epoch: 3, Batch: 800, MLM Loss: 5.7073332524299625, SOP Loss:0.020907212018355496\n",
      "Epoch: 3, Batch: 1000, MLM Loss: 5.691150812149048, SOP Loss:0.020647288431297057\n",
      "Epoch: 3, Batch: 1200, MLM Loss: 5.679716454744339, SOP Loss:0.020130733888945544\n",
      "Train ===> Epoch 4 Loss: 5.69322704363769\n",
      "Val ===> Epoch 4, Val_SOP_Accuracy: 0.9988873600959778\n",
      "------------------------------\n",
      "Epoch: 4, Batch: 200, MLM Loss: 5.570201182365418, SOP Loss:0.020124477112549357\n",
      "Epoch: 4, Batch: 400, MLM Loss: 5.561396698951722, SOP Loss:0.019444418092025444\n",
      "Epoch: 4, Batch: 600, MLM Loss: 5.545832094351451, SOP Loss:0.018812938610014197\n",
      "Epoch: 4, Batch: 800, MLM Loss: 5.531885566115379, SOP Loss:0.01766460715334688\n",
      "Epoch: 4, Batch: 1000, MLM Loss: 5.517016053199768, SOP Loss:0.017067689470481127\n",
      "Epoch: 4, Batch: 1200, MLM Loss: 5.507797283728918, SOP Loss:0.01626014684981783\n",
      "Train ===> Epoch 5 Loss: 5.518033964744339\n",
      "Val ===> Epoch 5, Val_SOP_Accuracy: 0.9989840984344482\n",
      "------------------------------\n",
      "Epoch: 5, Batch: 200, MLM Loss: 5.427964239120484, SOP Loss:0.012901481482840609\n",
      "Epoch: 5, Batch: 400, MLM Loss: 5.412247089147567, SOP Loss:0.013640461677277926\n",
      "Epoch: 5, Batch: 600, MLM Loss: 5.402735834916433, SOP Loss:0.013463692964918058\n",
      "Epoch: 5, Batch: 800, MLM Loss: 5.396026005744934, SOP Loss:0.014389456446479017\n",
      "Epoch: 5, Batch: 1000, MLM Loss: 5.385702020168305, SOP Loss:0.013676922782266046\n",
      "Epoch: 5, Batch: 1200, MLM Loss: 5.382288851737976, SOP Loss:0.013342013936902125\n",
      "Train ===> Epoch 6 Loss: 5.39194650998223\n",
      "Val ===> Epoch 6, Val_SOP_Accuracy: 0.9989840984344482\n",
      "------------------------------\n",
      "Epoch: 6, Batch: 200, MLM Loss: 5.286185896396637, SOP Loss:0.015012176576710771\n",
      "Epoch: 6, Batch: 400, MLM Loss: 5.293792624473571, SOP Loss:0.01369259959741612\n",
      "Epoch: 6, Batch: 600, MLM Loss: 5.281364436944326, SOP Loss:0.013523758413017882\n",
      "Epoch: 6, Batch: 800, MLM Loss: 5.278553029894828, SOP Loss:0.013729523482143123\n",
      "Epoch: 6, Batch: 1000, MLM Loss: 5.268740997791291, SOP Loss:0.012752274683327415\n",
      "Epoch: 6, Batch: 1200, MLM Loss: 5.260309522549312, SOP Loss:0.012426084784989749\n",
      "Train ===> Epoch 7 Loss: 5.2688989908536845\n",
      "Val ===> Epoch 7, Val_SOP_Accuracy: 0.9990324974060059\n",
      "------------------------------\n",
      "Epoch: 7, Batch: 200, MLM Loss: 5.204663343429566, SOP Loss:0.011955777646799106\n",
      "Epoch: 7, Batch: 400, MLM Loss: 5.191984959840775, SOP Loss:0.010874927490222034\n",
      "Epoch: 7, Batch: 600, MLM Loss: 5.1841280619303385, SOP Loss:0.01197376372719494\n",
      "Epoch: 7, Batch: 800, MLM Loss: 5.172233710885048, SOP Loss:0.011453293409176694\n",
      "Epoch: 7, Batch: 1000, MLM Loss: 5.167135212421417, SOP Loss:0.011149577267933636\n",
      "Epoch: 7, Batch: 1200, MLM Loss: 5.158997270266215, SOP Loss:0.010906501957748938\n",
      "Train ===> Epoch 8 Loss: 5.165976176086402\n",
      "Val ===> Epoch 8, Val_SOP_Accuracy: 0.9989840984344482\n",
      "------------------------------\n",
      "Epoch: 8, Batch: 200, MLM Loss: 5.092473332881927, SOP Loss:0.010193712245963979\n",
      "Epoch: 8, Batch: 400, MLM Loss: 5.077090843915939, SOP Loss:0.012612306060254923\n",
      "Epoch: 8, Batch: 600, MLM Loss: 5.068487099011739, SOP Loss:0.01220939588965848\n",
      "Epoch: 8, Batch: 800, MLM Loss: 5.061454361081124, SOP Loss:0.012067603843388498\n",
      "Epoch: 8, Batch: 1000, MLM Loss: 5.053128093719482, SOP Loss:0.01148267454811139\n",
      "Epoch: 8, Batch: 1200, MLM Loss: 5.043540681997935, SOP Loss:0.010974223219818668\n",
      "Train ===> Epoch 9 Loss: 5.0496221527955765\n",
      "Val ===> Epoch 9, Val_SOP_Accuracy: 0.9991292357444763\n",
      "------------------------------\n",
      "Epoch: 9, Batch: 200, MLM Loss: 4.988665735721588, SOP Loss:0.010102676984242862\n",
      "Epoch: 9, Batch: 400, MLM Loss: 4.977775316238404, SOP Loss:0.009484045654317015\n",
      "Epoch: 9, Batch: 600, MLM Loss: 4.96954273780187, SOP Loss:0.010089876576482007\n",
      "Epoch: 9, Batch: 800, MLM Loss: 4.96033119738102, SOP Loss:0.009676604932756162\n",
      "Epoch: 9, Batch: 1000, MLM Loss: 4.952539278030396, SOP Loss:0.00986206858677906\n",
      "Epoch: 9, Batch: 1200, MLM Loss: 4.9424473651250205, SOP Loss:0.009693137174690492\n",
      "Train ===> Epoch 10 Loss: 4.948380744646841\n",
      "Val ===> Epoch 10, Val_SOP_Accuracy: 0.9990324974060059\n",
      "------------------------------\n",
      "Epoch: 10, Batch: 200, MLM Loss: 4.884313433170319, SOP Loss:0.008338410787837347\n",
      "Epoch: 10, Batch: 400, MLM Loss: 4.85957392334938, SOP Loss:0.008425051649282977\n",
      "Epoch: 10, Batch: 600, MLM Loss: 4.852881724834442, SOP Loss:0.009395139620367749\n",
      "Epoch: 10, Batch: 800, MLM Loss: 4.847712063789368, SOP Loss:0.009008003905328223\n",
      "Epoch: 10, Batch: 1000, MLM Loss: 4.840915156364441, SOP Loss:0.008784290480762139\n",
      "Epoch: 10, Batch: 1200, MLM Loss: 4.840702129205068, SOP Loss:0.009484277907455787\n",
      "Train ===> Epoch 11 Loss: 4.846160952001731\n",
      "Val ===> Epoch 11, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 11, Batch: 200, MLM Loss: 4.737479267120361, SOP Loss:0.00741468796346453\n",
      "Epoch: 11, Batch: 400, MLM Loss: 4.746228926181793, SOP Loss:0.00788513746279932\n",
      "Epoch: 11, Batch: 600, MLM Loss: 4.734810253779093, SOP Loss:0.008051786605137749\n",
      "Epoch: 11, Batch: 800, MLM Loss: 4.731088976860047, SOP Loss:0.007897983912662311\n",
      "Epoch: 11, Batch: 1000, MLM Loss: 4.727340053081512, SOP Loss:0.008060407070093789\n",
      "Epoch: 11, Batch: 1200, MLM Loss: 4.723346510132154, SOP Loss:0.008397586410210352\n",
      "Train ===> Epoch 12 Loss: 4.728918275298903\n",
      "Val ===> Epoch 12, Val_SOP_Accuracy: 0.9990808367729187\n",
      "------------------------------\n",
      "Epoch: 12, Batch: 200, MLM Loss: 4.6874031472206115, SOP Loss:0.008839349769696128\n",
      "Epoch: 12, Batch: 400, MLM Loss: 4.685050300359726, SOP Loss:0.009597449429420521\n",
      "Epoch: 12, Batch: 600, MLM Loss: 4.67444089571635, SOP Loss:0.008873338687068705\n",
      "Epoch: 12, Batch: 800, MLM Loss: 4.667467208206654, SOP Loss:0.008291232843130275\n",
      "Epoch: 12, Batch: 1000, MLM Loss: 4.662382801294327, SOP Loss:0.008109690346056596\n",
      "Epoch: 12, Batch: 1200, MLM Loss: 4.651821366349856, SOP Loss:0.00797445144107769\n",
      "Train ===> Epoch 13 Loss: 4.655215601270715\n",
      "Val ===> Epoch 13, Val_SOP_Accuracy: 0.9989840984344482\n",
      "------------------------------\n",
      "Epoch: 13, Batch: 200, MLM Loss: 4.62150134563446, SOP Loss:0.007035611984683783\n",
      "Epoch: 13, Batch: 400, MLM Loss: 4.602625460624695, SOP Loss:0.007299528043622558\n",
      "Epoch: 13, Batch: 600, MLM Loss: 4.592627991040548, SOP Loss:0.007638066354678207\n",
      "Epoch: 13, Batch: 800, MLM Loss: 4.587465769052505, SOP Loss:0.007201793562271633\n",
      "Epoch: 13, Batch: 1000, MLM Loss: 4.5775911226272585, SOP Loss:0.007241402040308458\n",
      "Epoch: 13, Batch: 1200, MLM Loss: 4.573119490941366, SOP Loss:0.007652772107867349\n",
      "Train ===> Epoch 14 Loss: 4.575803163607008\n",
      "Val ===> Epoch 14, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 14, Batch: 200, MLM Loss: 4.5112855052948, SOP Loss:0.006901610875647748\n",
      "Epoch: 14, Batch: 400, MLM Loss: 4.514103162288666, SOP Loss:0.008145246987405699\n",
      "Epoch: 14, Batch: 600, MLM Loss: 4.504240682125092, SOP Loss:0.008613163313517969\n",
      "Epoch: 14, Batch: 800, MLM Loss: 4.500226008594036, SOP Loss:0.008292772605655046\n",
      "Epoch: 14, Batch: 1000, MLM Loss: 4.4940060422420505, SOP Loss:0.008031565668788971\n",
      "Epoch: 14, Batch: 1200, MLM Loss: 4.483807662725448, SOP Loss:0.008305933679451604\n",
      "Train ===> Epoch 15 Loss: 4.490000312850946\n",
      "Val ===> Epoch 15, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 15, Batch: 200, MLM Loss: 4.4164682900905605, SOP Loss:0.00494828938957653\n",
      "Epoch: 15, Batch: 400, MLM Loss: 4.431002663373947, SOP Loss:0.005937328432883078\n",
      "Epoch: 15, Batch: 600, MLM Loss: 4.430781024694443, SOP Loss:0.005695207865865086\n",
      "Epoch: 15, Batch: 800, MLM Loss: 4.421893916428089, SOP Loss:0.00621415128688568\n",
      "Epoch: 15, Batch: 1000, MLM Loss: 4.416114671468735, SOP Loss:0.006469742421533738\n",
      "Epoch: 15, Batch: 1200, MLM Loss: 4.405703682303429, SOP Loss:0.007010724468073022\n",
      "Train ===> Epoch 16 Loss: 4.410213028549674\n",
      "Val ===> Epoch 16, Val_SOP_Accuracy: 0.9991775751113892\n",
      "------------------------------\n",
      "Epoch: 16, Batch: 200, MLM Loss: 4.363535043001175, SOP Loss:0.006018084238894517\n",
      "Epoch: 16, Batch: 400, MLM Loss: 4.351788811087609, SOP Loss:0.00597817762671184\n",
      "Epoch: 16, Batch: 600, MLM Loss: 4.340870757102966, SOP Loss:0.005936005329543453\n",
      "Epoch: 16, Batch: 800, MLM Loss: 4.337881731986999, SOP Loss:0.0060733840967805005\n",
      "Epoch: 16, Batch: 1000, MLM Loss: 4.337898713827133, SOP Loss:0.006123348671695567\n",
      "Epoch: 16, Batch: 1200, MLM Loss: 4.333470140894254, SOP Loss:0.006434700474225489\n",
      "Train ===> Epoch 17 Loss: 4.338408194261726\n",
      "Val ===> Epoch 17, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 17, Batch: 200, MLM Loss: 4.28017923116684, SOP Loss:0.005469846897030947\n",
      "Epoch: 17, Batch: 400, MLM Loss: 4.281272795200348, SOP Loss:0.006567601102615299\n",
      "Epoch: 17, Batch: 600, MLM Loss: 4.279021256367366, SOP Loss:0.006599625795497559\n",
      "Epoch: 17, Batch: 800, MLM Loss: 4.272300899922848, SOP Loss:0.006585966134571209\n",
      "Epoch: 17, Batch: 1000, MLM Loss: 4.267233836650848, SOP Loss:0.006861966363576357\n",
      "Epoch: 17, Batch: 1200, MLM Loss: 4.265450242757797, SOP Loss:0.006944605375465472\n",
      "Train ===> Epoch 18 Loss: 4.270827527124575\n",
      "Val ===> Epoch 18, Val_SOP_Accuracy: 0.9989840984344482\n",
      "------------------------------\n",
      "Epoch: 18, Batch: 200, MLM Loss: 4.196278192996979, SOP Loss:0.005470765977079282\n",
      "Epoch: 18, Batch: 400, MLM Loss: 4.1990063560009006, SOP Loss:0.00577191314096126\n",
      "Epoch: 18, Batch: 600, MLM Loss: 4.2046668322881064, SOP Loss:0.006221363756073212\n",
      "Epoch: 18, Batch: 800, MLM Loss: 4.201435607075691, SOP Loss:0.005953875127597712\n",
      "Epoch: 18, Batch: 1000, MLM Loss: 4.202199757575989, SOP Loss:0.006354362355035846\n",
      "Epoch: 18, Batch: 1200, MLM Loss: 4.1972632269064585, SOP Loss:0.0063045038692750195\n",
      "Train ===> Epoch 19 Loss: 4.198386909620723\n",
      "Val ===> Epoch 19, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 19, Batch: 200, MLM Loss: 4.167286002635956, SOP Loss:0.007459888460944058\n",
      "Epoch: 19, Batch: 400, MLM Loss: 4.151773791909218, SOP Loss:0.007721423877701455\n",
      "Epoch: 19, Batch: 600, MLM Loss: 4.149507133960724, SOP Loss:0.0075593029006025365\n",
      "Epoch: 19, Batch: 800, MLM Loss: 4.149998484253883, SOP Loss:0.007856161688487191\n",
      "Epoch: 19, Batch: 1000, MLM Loss: 4.143717887401581, SOP Loss:0.007661488291385467\n",
      "Epoch: 19, Batch: 1200, MLM Loss: 4.138481022318204, SOP Loss:0.0074829126867067925\n",
      "Train ===> Epoch 20 Loss: 4.143337377478233\n",
      "Val ===> Epoch 20, Val_SOP_Accuracy: 0.9991292357444763\n",
      "------------------------------\n",
      "Epoch: 20, Batch: 200, MLM Loss: 4.087798882722854, SOP Loss:0.007644058575606323\n",
      "Epoch: 20, Batch: 400, MLM Loss: 4.082122446894646, SOP Loss:0.006672406181314728\n",
      "Epoch: 20, Batch: 600, MLM Loss: 4.078105835914612, SOP Loss:0.006645079989611986\n",
      "Epoch: 20, Batch: 800, MLM Loss: 4.078847831785679, SOP Loss:0.006877661134058144\n",
      "Epoch: 20, Batch: 1000, MLM Loss: 4.073018186807633, SOP Loss:0.006891838024515892\n",
      "Epoch: 20, Batch: 1200, MLM Loss: 4.07221671640873, SOP Loss:0.006776049187319586\n",
      "Train ===> Epoch 21 Loss: 4.0784918866202\n",
      "Val ===> Epoch 21, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 21, Batch: 200, MLM Loss: 4.027158594131469, SOP Loss:0.005006703994076815\n",
      "Epoch: 21, Batch: 400, MLM Loss: 4.027397461533546, SOP Loss:0.0054331419445770735\n",
      "Epoch: 21, Batch: 600, MLM Loss: 4.018755710522334, SOP Loss:0.0059928172370200625\n",
      "Epoch: 21, Batch: 800, MLM Loss: 4.012947216331959, SOP Loss:0.005808159008583971\n",
      "Epoch: 21, Batch: 1000, MLM Loss: 4.013264601945877, SOP Loss:0.0058267982878969634\n",
      "Epoch: 21, Batch: 1200, MLM Loss: 4.008343951106071, SOP Loss:0.006411196618494917\n",
      "Train ===> Epoch 22 Loss: 4.0132104656180525\n",
      "Val ===> Epoch 22, Val_SOP_Accuracy: 0.9991292357444763\n",
      "------------------------------\n",
      "Epoch: 22, Batch: 200, MLM Loss: 3.9596384596824645, SOP Loss:0.007063033295999048\n",
      "Epoch: 22, Batch: 400, MLM Loss: 3.966297906041145, SOP Loss:0.005385256138870318\n",
      "Epoch: 22, Batch: 600, MLM Loss: 3.9631554738680523, SOP Loss:0.0052518095824537646\n",
      "Epoch: 22, Batch: 800, MLM Loss: 3.9607725048065188, SOP Loss:0.005066044075401805\n",
      "Epoch: 22, Batch: 1000, MLM Loss: 3.9596905968189238, SOP Loss:0.0057802288127350035\n",
      "Epoch: 22, Batch: 1200, MLM Loss: 3.958140185872714, SOP Loss:0.0057341766380159245\n",
      "Train ===> Epoch 23 Loss: 3.963228011260767\n",
      "Val ===> Epoch 23, Val_SOP_Accuracy: 0.9991292357444763\n",
      "------------------------------\n",
      "Epoch: 23, Batch: 200, MLM Loss: 3.9290582966804504, SOP Loss:0.005016776236479927\n",
      "Epoch: 23, Batch: 400, MLM Loss: 3.9177755564451218, SOP Loss:0.006101911315599864\n",
      "Epoch: 23, Batch: 600, MLM Loss: 3.9175241231918334, SOP Loss:0.006300430635553009\n",
      "Epoch: 23, Batch: 800, MLM Loss: 3.915186934173107, SOP Loss:0.006602918439839413\n",
      "Epoch: 23, Batch: 1000, MLM Loss: 3.9119975287914275, SOP Loss:0.00637928671517875\n",
      "Epoch: 23, Batch: 1200, MLM Loss: 3.9085610312223436, SOP Loss:0.006193381971661438\n",
      "Train ===> Epoch 24 Loss: 3.91340187754242\n",
      "Val ===> Epoch 24, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 24, Batch: 200, MLM Loss: 3.8801083648204804, SOP Loss:0.007997245065234892\n",
      "Epoch: 24, Batch: 400, MLM Loss: 3.8712965726852415, SOP Loss:0.006175558696013468\n",
      "Epoch: 24, Batch: 600, MLM Loss: 3.8704826629161833, SOP Loss:0.005753965882055733\n",
      "Epoch: 24, Batch: 800, MLM Loss: 3.866046480834484, SOP Loss:0.006527667066275171\n",
      "Epoch: 24, Batch: 1000, MLM Loss: 3.86298216176033, SOP Loss:0.006372159048245521\n",
      "Epoch: 24, Batch: 1200, MLM Loss: 3.8592972687880196, SOP Loss:0.006017112622236406\n",
      "Train ===> Epoch 25 Loss: 3.864032001249932\n",
      "Val ===> Epoch 25, Val_SOP_Accuracy: 0.9995162487030029\n",
      "------------------------------\n",
      "Epoch: 25, Batch: 200, MLM Loss: 3.8252363955974578, SOP Loss:0.006672982944401155\n",
      "Epoch: 25, Batch: 400, MLM Loss: 3.8327391821146013, SOP Loss:0.005979073496946512\n",
      "Epoch: 25, Batch: 600, MLM Loss: 3.826372428337733, SOP Loss:0.005763471146177229\n",
      "Epoch: 25, Batch: 800, MLM Loss: 3.822576744258404, SOP Loss:0.0058710378858268086\n",
      "Epoch: 25, Batch: 1000, MLM Loss: 3.814495824098587, SOP Loss:0.006353065197559772\n",
      "Epoch: 25, Batch: 1200, MLM Loss: 3.8110098055998485, SOP Loss:0.006119200788204277\n",
      "Train ===> Epoch 26 Loss: 3.815652543520539\n",
      "Val ===> Epoch 26, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 26, Batch: 200, MLM Loss: 3.7963818979263304, SOP Loss:0.006188014177314472\n",
      "Epoch: 26, Batch: 400, MLM Loss: 3.792569295167923, SOP Loss:0.006604303994517977\n",
      "Epoch: 26, Batch: 600, MLM Loss: 3.784255895614624, SOP Loss:0.006457137117940874\n",
      "Epoch: 26, Batch: 800, MLM Loss: 3.7839429080486298, SOP Loss:0.005719077286667016\n",
      "Epoch: 26, Batch: 1000, MLM Loss: 3.77957523727417, SOP Loss:0.005967824353436299\n",
      "Epoch: 26, Batch: 1200, MLM Loss: 3.7736199261744816, SOP Loss:0.005868502616879899\n",
      "Train ===> Epoch 27 Loss: 3.7805599047590355\n",
      "Val ===> Epoch 27, Val_SOP_Accuracy: 0.9994194507598877\n",
      "------------------------------\n",
      "Epoch: 27, Batch: 200, MLM Loss: 3.7425187110900877, SOP Loss:0.006191352973219182\n",
      "Epoch: 27, Batch: 400, MLM Loss: 3.7462901401519777, SOP Loss:0.0061999300874413165\n",
      "Epoch: 27, Batch: 600, MLM Loss: 3.744520679314931, SOP Loss:0.005377170760196653\n",
      "Epoch: 27, Batch: 800, MLM Loss: 3.7477800288796423, SOP Loss:0.0052617651290711364\n",
      "Epoch: 27, Batch: 1000, MLM Loss: 3.7518599524497986, SOP Loss:0.005403434771724278\n",
      "Epoch: 27, Batch: 1200, MLM Loss: 3.744170739253362, SOP Loss:0.005935588785408375\n",
      "Train ===> Epoch 28 Loss: 3.7465957459969945\n",
      "Val ===> Epoch 28, Val_SOP_Accuracy: 0.9991775751113892\n",
      "------------------------------\n",
      "Epoch: 28, Batch: 200, MLM Loss: 3.72049534201622, SOP Loss:0.005611408017866779\n",
      "Epoch: 28, Batch: 400, MLM Loss: 3.715582017302513, SOP Loss:0.005874721781874541\n",
      "Epoch: 28, Batch: 600, MLM Loss: 3.7003790644804635, SOP Loss:0.005474398974814297\n",
      "Epoch: 28, Batch: 800, MLM Loss: 3.6923574900627134, SOP Loss:0.005133748787648074\n",
      "Epoch: 28, Batch: 1000, MLM Loss: 3.6924512243270873, SOP Loss:0.00564469448885211\n",
      "Epoch: 28, Batch: 1200, MLM Loss: 3.6873163147767385, SOP Loss:0.005715967892871656\n",
      "Train ===> Epoch 29 Loss: 3.693439296816986\n",
      "Val ===> Epoch 29, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 29, Batch: 200, MLM Loss: 3.6581956040859223, SOP Loss:0.00705084066838026\n",
      "Epoch: 29, Batch: 400, MLM Loss: 3.651943176984787, SOP Loss:0.0061751015282970915\n",
      "Epoch: 29, Batch: 600, MLM Loss: 3.6545764136314394, SOP Loss:0.006104656180189826\n",
      "Epoch: 29, Batch: 800, MLM Loss: 3.654045022726059, SOP Loss:0.00569590034157045\n",
      "Epoch: 29, Batch: 1000, MLM Loss: 3.6503337993621825, SOP Loss:0.005750860027001181\n",
      "Epoch: 29, Batch: 1200, MLM Loss: 3.650214256644249, SOP Loss:0.006124679714442512\n",
      "Train ===> Epoch 30 Loss: 3.6509938355379705\n",
      "Val ===> Epoch 30, Val_SOP_Accuracy: 0.9994194507598877\n",
      "------------------------------\n",
      "Epoch: 30, Batch: 200, MLM Loss: 3.6002077674865722, SOP Loss:0.004737135167561064\n",
      "Epoch: 30, Batch: 400, MLM Loss: 3.5957873392105104, SOP Loss:0.005826800517806987\n",
      "Epoch: 30, Batch: 600, MLM Loss: 3.6057196605205535, SOP Loss:0.00550873667451621\n",
      "Epoch: 30, Batch: 800, MLM Loss: 3.6027450010180475, SOP Loss:0.005844969676227265\n",
      "Epoch: 30, Batch: 1000, MLM Loss: 3.6010813767910004, SOP Loss:0.005679840760800289\n",
      "Epoch: 30, Batch: 1200, MLM Loss: 3.6013757689793904, SOP Loss:0.005593760709380149\n",
      "Train ===> Epoch 31 Loss: 3.6065200776936326\n",
      "Val ===> Epoch 31, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 31, Batch: 200, MLM Loss: 3.5894652652740477, SOP Loss:0.004196662668437057\n",
      "Epoch: 31, Batch: 400, MLM Loss: 3.601525845527649, SOP Loss:0.005410768620658928\n",
      "Epoch: 31, Batch: 600, MLM Loss: 3.592484689950943, SOP Loss:0.005007049610127675\n",
      "Epoch: 31, Batch: 800, MLM Loss: 3.586316633224487, SOP Loss:0.00549487962166495\n",
      "Epoch: 31, Batch: 1000, MLM Loss: 3.584455495595932, SOP Loss:0.005457815875539382\n",
      "Epoch: 31, Batch: 1200, MLM Loss: 3.5790334979693097, SOP Loss:0.005539437754177925\n",
      "Train ===> Epoch 32 Loss: 3.580760033877559\n",
      "Val ===> Epoch 32, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 32, Batch: 200, MLM Loss: 3.552907191514969, SOP Loss:0.006119120009243488\n",
      "Epoch: 32, Batch: 400, MLM Loss: 3.5454770493507386, SOP Loss:0.006200267398671713\n",
      "Epoch: 32, Batch: 600, MLM Loss: 3.53611670255661, SOP Loss:0.005522193695166304\n",
      "Epoch: 32, Batch: 800, MLM Loss: 3.5362721660733225, SOP Loss:0.0053547538184648144\n",
      "Epoch: 32, Batch: 1000, MLM Loss: 3.534010136127472, SOP Loss:0.005309558056578681\n",
      "Epoch: 32, Batch: 1200, MLM Loss: 3.536179220875104, SOP Loss:0.005736453637485587\n",
      "Train ===> Epoch 33 Loss: 3.5418570973297725\n",
      "Val ===> Epoch 33, Val_SOP_Accuracy: 0.9993711113929749\n",
      "------------------------------\n",
      "Epoch: 33, Batch: 200, MLM Loss: 3.5098991799354553, SOP Loss:0.005250497573870234\n",
      "Epoch: 33, Batch: 400, MLM Loss: 3.508605210781097, SOP Loss:0.005128383817591385\n",
      "Epoch: 33, Batch: 600, MLM Loss: 3.5094488378365836, SOP Loss:0.004793904327052587\n",
      "Epoch: 33, Batch: 800, MLM Loss: 3.511495860517025, SOP Loss:0.005159440704919689\n",
      "Epoch: 33, Batch: 1000, MLM Loss: 3.510084618330002, SOP Loss:0.005081702440256777\n",
      "Epoch: 33, Batch: 1200, MLM Loss: 3.5133868636687597, SOP Loss:0.005055547520169057\n",
      "Train ===> Epoch 34 Loss: 3.5172746219614166\n",
      "Val ===> Epoch 34, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 34, Batch: 200, MLM Loss: 3.4869533240795136, SOP Loss:0.0034783983508168605\n",
      "Epoch: 34, Batch: 400, MLM Loss: 3.4884866136312485, SOP Loss:0.004831433749059215\n",
      "Epoch: 34, Batch: 600, MLM Loss: 3.4808934064706167, SOP Loss:0.00560597224720065\n",
      "Epoch: 34, Batch: 800, MLM Loss: 3.4783220735192297, SOP Loss:0.005674509404561831\n",
      "Epoch: 34, Batch: 1000, MLM Loss: 3.477980199813843, SOP Loss:0.005870309079968138\n",
      "Epoch: 34, Batch: 1200, MLM Loss: 3.477136431137721, SOP Loss:0.005802167116083486\n",
      "Train ===> Epoch 35 Loss: 3.4830279432378304\n",
      "Val ===> Epoch 35, Val_SOP_Accuracy: 0.9994194507598877\n",
      "------------------------------\n",
      "Epoch: 35, Batch: 200, MLM Loss: 3.434967271089554, SOP Loss:0.0056064806490758205\n",
      "Epoch: 35, Batch: 400, MLM Loss: 3.433997756242752, SOP Loss:0.00517428454641049\n",
      "Epoch: 35, Batch: 600, MLM Loss: 3.443146517276764, SOP Loss:0.005644689791261044\n",
      "Epoch: 35, Batch: 800, MLM Loss: 3.437875103354454, SOP Loss:0.005506329327599815\n",
      "Epoch: 35, Batch: 1000, MLM Loss: 3.4404233360290526, SOP Loss:0.0054687235668679935\n",
      "Epoch: 35, Batch: 1200, MLM Loss: 3.4391335153579714, SOP Loss:0.005506262149289493\n",
      "Train ===> Epoch 36 Loss: 3.4468109795257083\n",
      "Val ===> Epoch 36, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 36, Batch: 200, MLM Loss: 3.4206401610374453, SOP Loss:0.004663869836294907\n",
      "Epoch: 36, Batch: 400, MLM Loss: 3.4149821388721464, SOP Loss:0.004198092118022032\n",
      "Epoch: 36, Batch: 600, MLM Loss: 3.4230005999406177, SOP Loss:0.004806650717497784\n",
      "Epoch: 36, Batch: 800, MLM Loss: 3.4207022297382355, SOP Loss:0.004566829866680564\n",
      "Epoch: 36, Batch: 1000, MLM Loss: 3.4184978115558624, SOP Loss:0.004912472680371138\n",
      "Epoch: 36, Batch: 1200, MLM Loss: 3.4161352906624476, SOP Loss:0.004908520860523519\n",
      "Train ===> Epoch 37 Loss: 3.419316010182919\n",
      "Val ===> Epoch 37, Val_SOP_Accuracy: 0.9991775751113892\n",
      "------------------------------\n",
      "Epoch: 37, Batch: 200, MLM Loss: 3.400839147567749, SOP Loss:0.0057460465483382\n",
      "Epoch: 37, Batch: 400, MLM Loss: 3.384889632463455, SOP Loss:0.005429180927985726\n",
      "Epoch: 37, Batch: 600, MLM Loss: 3.3827372920513152, SOP Loss:0.004853396762458336\n",
      "Epoch: 37, Batch: 800, MLM Loss: 3.3862468871474265, SOP Loss:0.005043032346857217\n",
      "Epoch: 37, Batch: 1000, MLM Loss: 3.38281228017807, SOP Loss:0.004805510822603537\n",
      "Epoch: 37, Batch: 1200, MLM Loss: 3.382226829926173, SOP Loss:0.004877992875014267\n",
      "Train ===> Epoch 38 Loss: 3.3877185817829183\n",
      "Val ===> Epoch 38, Val_SOP_Accuracy: 0.9991775751113892\n",
      "------------------------------\n",
      "Epoch: 38, Batch: 200, MLM Loss: 3.3530576968193055, SOP Loss:0.004653084546407627\n",
      "Epoch: 38, Batch: 400, MLM Loss: 3.3689479357004166, SOP Loss:0.0043557029864678045\n",
      "Epoch: 38, Batch: 600, MLM Loss: 3.362253302335739, SOP Loss:0.004523128637483751\n",
      "Epoch: 38, Batch: 800, MLM Loss: 3.361383035480976, SOP Loss:0.005175712936706986\n",
      "Epoch: 38, Batch: 1000, MLM Loss: 3.3618946249485018, SOP Loss:0.004957395834411727\n",
      "Epoch: 38, Batch: 1200, MLM Loss: 3.3590734901030856, SOP Loss:0.005170796558747194\n",
      "Train ===> Epoch 39 Loss: 3.3630007486241706\n",
      "Val ===> Epoch 39, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 39, Batch: 200, MLM Loss: 3.3371769630908967, SOP Loss:0.0035896291137760272\n",
      "Epoch: 39, Batch: 400, MLM Loss: 3.3318468767404554, SOP Loss:0.004283729836206476\n",
      "Epoch: 39, Batch: 600, MLM Loss: 3.3363814266522724, SOP Loss:0.0043724067838168896\n",
      "Epoch: 39, Batch: 800, MLM Loss: 3.333852970302105, SOP Loss:0.0046217725824590165\n",
      "Epoch: 39, Batch: 1000, MLM Loss: 3.3318311111927033, SOP Loss:0.004898603119407198\n",
      "Epoch: 39, Batch: 1200, MLM Loss: 3.325398364663124, SOP Loss:0.005066799531420353\n",
      "Train ===> Epoch 40 Loss: 3.3299026392744797\n",
      "Val ===> Epoch 40, Val_SOP_Accuracy: 0.9993711113929749\n",
      "------------------------------\n",
      "Epoch: 40, Batch: 200, MLM Loss: 3.3336898505687715, SOP Loss:0.0055912967378390025\n",
      "Epoch: 40, Batch: 400, MLM Loss: 3.3195048928260804, SOP Loss:0.005567731959799858\n",
      "Epoch: 40, Batch: 600, MLM Loss: 3.299884721040726, SOP Loss:0.005394461591295112\n",
      "Epoch: 40, Batch: 800, MLM Loss: 3.29656476855278, SOP Loss:0.005330977029002497\n",
      "Epoch: 40, Batch: 1000, MLM Loss: 3.299930061817169, SOP Loss:0.0049768012275199\n",
      "Epoch: 40, Batch: 1200, MLM Loss: 3.301695878704389, SOP Loss:0.0051691847405678955\n",
      "Train ===> Epoch 41 Loss: 3.307341596727759\n",
      "Val ===> Epoch 41, Val_SOP_Accuracy: 0.9994678497314453\n",
      "------------------------------\n",
      "Epoch: 41, Batch: 200, MLM Loss: 3.2814395236968994, SOP Loss:0.004538985371746094\n",
      "Epoch: 41, Batch: 400, MLM Loss: 3.268593276143074, SOP Loss:0.004956683085511031\n",
      "Epoch: 41, Batch: 600, MLM Loss: 3.2722169391314186, SOP Loss:0.005714202497159325\n",
      "Epoch: 41, Batch: 800, MLM Loss: 3.2782535389065743, SOP Loss:0.005404682258094908\n",
      "Epoch: 41, Batch: 1000, MLM Loss: 3.277744358062744, SOP Loss:0.0056004909547591524\n",
      "Epoch: 41, Batch: 1200, MLM Loss: 3.279597741365433, SOP Loss:0.005407099388530696\n",
      "Train ===> Epoch 42 Loss: 3.2847049892256774\n",
      "Val ===> Epoch 42, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 42, Batch: 200, MLM Loss: 3.248246434926987, SOP Loss:0.005074103030528932\n",
      "Epoch: 42, Batch: 400, MLM Loss: 3.257994827628136, SOP Loss:0.004561205724703541\n",
      "Epoch: 42, Batch: 600, MLM Loss: 3.249932722647985, SOP Loss:0.003969903535950531\n",
      "Epoch: 42, Batch: 800, MLM Loss: 3.247445544600487, SOP Loss:0.004280197090424736\n",
      "Epoch: 42, Batch: 1000, MLM Loss: 3.2466769697666167, SOP Loss:0.004376163151584478\n",
      "Epoch: 42, Batch: 1200, MLM Loss: 3.2447761217753093, SOP Loss:0.00453144524600854\n",
      "Train ===> Epoch 43 Loss: 3.2476974895657427\n",
      "Val ===> Epoch 43, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 43, Batch: 200, MLM Loss: 3.2525847613811494, SOP Loss:0.005005400296904554\n",
      "Epoch: 43, Batch: 400, MLM Loss: 3.248113448023796, SOP Loss:0.0056953909631738495\n",
      "Epoch: 43, Batch: 600, MLM Loss: 3.2378737684090932, SOP Loss:0.005244510913665484\n",
      "Epoch: 43, Batch: 800, MLM Loss: 3.234177357852459, SOP Loss:0.005781375162059703\n",
      "Epoch: 43, Batch: 1000, MLM Loss: 3.233385822057724, SOP Loss:0.0056480604968091935\n",
      "Epoch: 43, Batch: 1200, MLM Loss: 3.2291536569595336, SOP Loss:0.005740230047546598\n",
      "Train ===> Epoch 44 Loss: 3.2345148515637323\n",
      "Val ===> Epoch 44, Val_SOP_Accuracy: 0.9993711113929749\n",
      "------------------------------\n",
      "Epoch: 44, Batch: 200, MLM Loss: 3.2200691747665404, SOP Loss:0.007717395008367021\n",
      "Epoch: 44, Batch: 400, MLM Loss: 3.2032207298278808, SOP Loss:0.006064599855690176\n",
      "Epoch: 44, Batch: 600, MLM Loss: 3.2034135282039644, SOP Loss:0.005927213410820211\n",
      "Epoch: 44, Batch: 800, MLM Loss: 3.1978326442837717, SOP Loss:0.005667809575570572\n",
      "Epoch: 44, Batch: 1000, MLM Loss: 3.1980261080265047, SOP Loss:0.005751664927905949\n",
      "Epoch: 44, Batch: 1200, MLM Loss: 3.1960565839211146, SOP Loss:0.005694980060973951\n",
      "Train ===> Epoch 45 Loss: 3.2014479274290495\n",
      "Val ===> Epoch 45, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 45, Batch: 200, MLM Loss: 3.1683865010738375, SOP Loss:0.005666974675223173\n",
      "Epoch: 45, Batch: 400, MLM Loss: 3.1706638622283934, SOP Loss:0.005619888003493543\n",
      "Epoch: 45, Batch: 600, MLM Loss: 3.175996780792872, SOP Loss:0.005797052396264917\n",
      "Epoch: 45, Batch: 800, MLM Loss: 3.181091833114624, SOP Loss:0.005458477030801987\n",
      "Epoch: 45, Batch: 1000, MLM Loss: 3.1807167592048646, SOP Loss:0.005651639308005542\n",
      "Epoch: 45, Batch: 1200, MLM Loss: 3.1799031094710033, SOP Loss:0.005483954882314113\n",
      "Train ===> Epoch 46 Loss: 3.1826423069460947\n",
      "Val ===> Epoch 46, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 46, Batch: 200, MLM Loss: 3.1196594071388244, SOP Loss:0.006737453839086811\n",
      "Epoch: 46, Batch: 400, MLM Loss: 3.130276888012886, SOP Loss:0.006547309372672316\n",
      "Epoch: 46, Batch: 600, MLM Loss: 3.131965073744456, SOP Loss:0.006045941286659703\n",
      "Epoch: 46, Batch: 800, MLM Loss: 3.1394963592290877, SOP Loss:0.005548745464657259\n",
      "Epoch: 46, Batch: 1000, MLM Loss: 3.1438159968852997, SOP Loss:0.005580085858848179\n",
      "Epoch: 46, Batch: 1200, MLM Loss: 3.145539569060008, SOP Loss:0.005503846170437706\n",
      "Train ===> Epoch 47 Loss: 3.1512134549333375\n",
      "Val ===> Epoch 47, Val_SOP_Accuracy: 0.9992259740829468\n",
      "------------------------------\n",
      "Epoch: 47, Batch: 200, MLM Loss: 3.1229966819286346, SOP Loss:0.004976391784075531\n",
      "Epoch: 47, Batch: 400, MLM Loss: 3.1282996129989624, SOP Loss:0.005717622217616736\n",
      "Epoch: 47, Batch: 600, MLM Loss: 3.1218160653114317, SOP Loss:0.004966720195686018\n",
      "Epoch: 47, Batch: 800, MLM Loss: 3.1155150559544564, SOP Loss:0.004680933788067705\n",
      "Epoch: 47, Batch: 1000, MLM Loss: 3.11527370429039, SOP Loss:0.00516214707502877\n",
      "Epoch: 47, Batch: 1200, MLM Loss: 3.116054990092913, SOP Loss:0.005025869788408575\n",
      "Train ===> Epoch 48 Loss: 3.1208689322870504\n",
      "Val ===> Epoch 48, Val_SOP_Accuracy: 0.9993227124214172\n",
      "------------------------------\n",
      "Epoch: 48, Batch: 200, MLM Loss: 3.1008913171291352, SOP Loss:0.0036524188174007575\n",
      "Epoch: 48, Batch: 400, MLM Loss: 3.102017498612404, SOP Loss:0.004477662499111829\n",
      "Epoch: 48, Batch: 600, MLM Loss: 3.106187643210093, SOP Loss:0.005079167584832855\n",
      "Epoch: 48, Batch: 800, MLM Loss: 3.111556243300438, SOP Loss:0.005278383385111738\n",
      "Epoch: 48, Batch: 1000, MLM Loss: 3.1131334495544434, SOP Loss:0.00514026279256359\n",
      "Epoch: 48, Batch: 1200, MLM Loss: 3.109352091550827, SOP Loss:0.004914670286616456\n",
      "Train ===> Epoch 49 Loss: 3.112008995809168\n",
      "Val ===> Epoch 49, Val_SOP_Accuracy: 0.9992743730545044\n",
      "------------------------------\n",
      "Epoch: 49, Batch: 200, MLM Loss: 3.077625195980072, SOP Loss:0.004222053278936073\n",
      "Epoch: 49, Batch: 400, MLM Loss: 3.0878542286157606, SOP Loss:0.004978447355988465\n",
      "Epoch: 49, Batch: 600, MLM Loss: 3.0913974408308666, SOP Loss:0.004982556783603893\n",
      "Epoch: 49, Batch: 800, MLM Loss: 3.081557909846306, SOP Loss:0.0049946175532932105\n",
      "Epoch: 49, Batch: 1000, MLM Loss: 3.079957325220108, SOP Loss:0.005164360382193991\n",
      "Epoch: 49, Batch: 1200, MLM Loss: 3.0831773328781127, SOP Loss:0.005079368589376827\n",
      "Train ===> Epoch 50 Loss: 3.0878308741686116\n",
      "Val ===> Epoch 50, Val_SOP_Accuracy: 0.9993711113929749\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isdir('./checkpoint'):\n",
    "    os.mkdir('./checkpoint')\n",
    "\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(num_epochs):\n",
    "    t_mlm_loss = 0.0\n",
    "    t_sop_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (inp, labels, mask) in enumerate(train_loader):\n",
    "        inp = inp.long().to(device)\n",
    "        nsp_label = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "        mlm_inp, mlm_labels = mask_tokens_for_mlm(inp.long(), special_tokens_ids, mask_id, vocab_size)\n",
    "        mlm_logits, sop_logits = model(mlm_inp, mask)\n",
    "\n",
    "        # MLMì€ batch*seq í˜•ìƒ ë³€í™˜ í•˜ì—¬ ì†ì‹¤ ê³„ì‚°\n",
    "        mlm_loss = mlm_loss_fn(mlm_logits.view(-1, vocab_size), mlm_labels.view(-1))\n",
    "        # SOP ì†ì‹¤ ê³„ì‚°\n",
    "        sop_loss = sop_loss_fn(sop_logits, nsp_label)\n",
    "\n",
    "        # ë‘ ì†ì‹¤ì„ ë”í•˜ì—¬ ì—­ì „íŒŒ\n",
    "        loss = mlm_loss + sop_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_mlm_loss += mlm_loss.item()\n",
    "        t_sop_loss += sop_loss.item()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i+1}, MLM Loss: {t_mlm_loss/(i+1)}, SOP Loss:{t_sop_loss/(i+1)}\")\n",
    "\n",
    "    avg_loss = (t_mlm_loss + t_sop_loss) / len(train_loader)\n",
    "    print(f\"Train ===> Epoch {epoch+1} Loss: {avg_loss}\")\n",
    "    train_loss.append(avg_loss)\n",
    "    checkpoint_path = f\"checkpoint/ckpt{epoch}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    # í‰ê°€ ê³¼ì •(NSP ì •í™•ë„ë§Œ í™•ì¸)\n",
    "    v_acc = 0.0\n",
    "    v_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, labels, mask) in enumerate(val_loader):\n",
    "            inp = inp.long().to(device)\n",
    "            labels = labels.to(device)\n",
    "            mask = mask.to(device)\n",
    "            mlm_logits, sop_logits = model(inp, mask)\n",
    "\n",
    "            # SOP ì •í™•ë„ ì—°ì‚°\n",
    "            acc = (sop_logits.argmax(dim=1) == labels).sum() / len(labels)\n",
    "            v_acc += acc\n",
    "\n",
    "        avg_acc = v_acc / len(val_loader)\n",
    "        print(f\"Val ===> Epoch {epoch+1}, Val_SOP_Accuracy: {avg_acc}\")\n",
    "        test_acc.append(avg_acc.cpu().numpy())\n",
    "\n",
    "    print('-'*30)\n",
    "torch.save(model, \"model1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b592c",
   "metadata": {},
   "source": [
    "BERT vs ALBERT\n",
    "\n",
    "| êµ¬ë¶„          | **BERT (ì‹¤ìŠµ)**                            | **ALBERT (ì‹¤ìŠµ)**                                      |\n",
    "| ----------- | ---------------------------------------- | ---------------------------------------------------- |\n",
    "| **ì„ë² ë”© êµ¬ì¡°**  | `Embedding(vocab, 128)` â†’ ë°”ë¡œ Transformer | `Embedding(vocab, 64)` â†’ `Linear(64â†’128)` projection |\n",
    "| **ë ˆì´ì–´ êµ¬ì¡°**  | `MTBlock(128)` Ã— **4ê°œ (ê°ì ë‹¤ë¥¸ ê°€ì¤‘ì¹˜)**      | `MTBlock(128)` Ã— **1ê°œë¥¼ 4ë²ˆ ê³µìœ **                       |\n",
    "| **íŒŒë¼ë¯¸í„° ê·œëª¨** | í¼                                        | ì‘ìŒ (ì„ë² ë”© ì ˆë°˜ + block ê³µìœ )                               |\n",
    "| **ì†ë„**      | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼                                 | ë” ë¹ ë¥´ê³  ê°€ë²¼ì›€                                            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af8908",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- ê°€ì¤‘ì¹˜/ë©”ëª¨ë¦¬/ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "| í•­ëª©        | BERT          | ALBERT           |\n",
    "| --------- | ------------- | ---------------- |\n",
    "| layer ê°€ì¤‘ì¹˜ | ëª¨ë“  blockì´ ë…ë¦½ì  | í•˜ë‚˜ì˜ blockì„ 4ë²ˆ ë°˜ë³µ |\n",
    "| ë©”ëª¨ë¦¬       | í¼             | ë§¤ìš° ì‘ìŒ            |\n",
    "| ì„±ëŠ¥        | ì•ˆì •ì            | ê°€ë²¼ìš°ë©° ë¹ ë¦„          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75641a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai5_project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
