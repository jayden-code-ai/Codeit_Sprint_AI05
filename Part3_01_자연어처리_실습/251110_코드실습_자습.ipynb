{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b5ca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am a student.</td>\n",
       "      <td>나는 학생이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He plays football.</td>\n",
       "      <td>그는 축구를 한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She likes coffee.</td>\n",
       "      <td>그녀는 커피를 좋아한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We study English.</td>\n",
       "      <td>우리는 영어를 공부한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are friends.</td>\n",
       "      <td>그들은 친구이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I like pizza.</td>\n",
       "      <td>나는 피자를 좋아한다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   en             ko\n",
       "0     I am a student.       나는 학생이다.\n",
       "1  He plays football.     그는 축구를 한다.\n",
       "2   She likes coffee.  그녀는 커피를 좋아한다.\n",
       "3   We study English.  우리는 영어를 공부한다.\n",
       "4   They are friends.      그들은 친구이다.\n",
       "5       I like pizza.   나는 피자를 좋아한다."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.실습데이터 만들기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"사용 디바이스:\", device)\n",
    "\n",
    "# 간단한 영→한 문장 데이터 (원하면 더 추가해도 됨!)\n",
    "data = [\n",
    "    (\"I am a student.\", \"나는 학생이다.\"),\n",
    "    (\"He plays football.\", \"그는 축구를 한다.\"),\n",
    "    (\"She likes coffee.\", \"그녀는 커피를 좋아한다.\"),\n",
    "    (\"We study English.\", \"우리는 영어를 공부한다.\"),\n",
    "    (\"They are friends.\", \"그들은 친구이다.\"),\n",
    "    (\"I like pizza.\", \"나는 피자를 좋아한다.\"),\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"en\", \"ko\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aa0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'plays', 'football']\n",
      "['그', '는', ' ', '축', '구', '를', ' ', '한', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. 토크나이저 직접 만들기 (영어/한국어)\n",
    "# 영어: 소문자 + 특수문자 제거 + 공백 기준 나누기\n",
    "def tokenize_en(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)   # 알파벳/숫자/공백만 남기기\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# 한국어: 글자 단위로 단순 분리 (예: \"나는 학생이다.\" -> [\"나\",\"는\",\" \",\"학\",\"생\",\"이\",\"다\",\".\"])\n",
    "def tokenize_ko(text: str):\n",
    "    return list(text)\n",
    "\n",
    "print(tokenize_en(\"He plays football.\"))\n",
    "print(tokenize_ko(\"그는 축구를 한다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119c7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 vocab size: 22\n",
      "한글 vocab size: 32\n"
     ]
    }
   ],
   "source": [
    "# Vocab(단어 사전) 직접 만들기\n",
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "def build_vocab(texts, tokenizer):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenizer(t))\n",
    "\n",
    "    # 토큰들을 정렬해서 넣어도 되고, 순서대로 넣어도 괜찮음\n",
    "    vocab = {}\n",
    "    idx = 0\n",
    "    for token in SPECIAL_TOKENS:\n",
    "        vocab[token] = idx\n",
    "        idx += 1\n",
    "\n",
    "    for token in counter.keys():\n",
    "        if token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab_en = build_vocab(df[\"en\"], tokenize_en)\n",
    "vocab_ko = build_vocab(df[\"ko\"], tokenize_ko)\n",
    "\n",
    "print(\"영어 vocab size:\", len(vocab_en))\n",
    "print(\"한글 vocab size:\", len(vocab_ko))\n",
    "\n",
    "# 인덱스 상수\n",
    "PAD_IDX_EN = vocab_en[\"<pad>\"]\n",
    "PAD_IDX_KO = vocab_ko[\"<pad>\"]\n",
    "BOS_IDX_EN = vocab_en[\"<bos>\"]\n",
    "EOS_IDX_EN = vocab_en[\"<eos>\"]\n",
    "BOS_IDX_KO = vocab_ko[\"<bos>\"]\n",
    "EOS_IDX_KO = vocab_ko[\"<eos>\"]\n",
    "\n",
    "# 나중에 디코딩용 역매핑\n",
    "rev_vocab_ko = {idx: token for token, idx in vocab_ko.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade41f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 5, 6, 7, 3, 1, 1, 1, 1])\n",
      "tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11,  3,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# 문장을 숫자 시퀀스로 바꾸는 함수 (encode)\n",
    "def encode_en(text, max_len=10):\n",
    "    tokens = [\"<bos>\"] + tokenize_en(text) + [\"<eos>\"]\n",
    "    ids = [vocab_en.get(t, vocab_en[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_IDX_EN] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def encode_ko(text, max_len=12):\n",
    "    tokens = [\"<bos>\"] + tokenize_ko(text) + [\"<eos>\"]\n",
    "    ids = [vocab_ko.get(t, vocab_ko[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_IDX_KO] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# 테스트\n",
    "print(encode_en(\"I am a student.\"))\n",
    "print(encode_ko(\"나는 학생이다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e230e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch shape: torch.Size([2, 10])\n",
      "trg_batch shape: torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "# Dataset & DataLoader 정의\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, df, max_len_en=10, max_len_ko=12):\n",
    "        self.df = df\n",
    "        self.max_len_en = max_len_en\n",
    "        self.max_len_ko = max_len_ko\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.df.iloc[idx][\"en\"]\n",
    "        trg_text = self.df.iloc[idx][\"ko\"]\n",
    "\n",
    "        src_ids = encode_en(src_text, self.max_len_en)\n",
    "        trg_ids = encode_ko(trg_text, self.max_len_ko)\n",
    "\n",
    "        return src_ids, trg_ids\n",
    "\n",
    "dataset = Seq2SeqDataset(df)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(\"src_batch shape:\", src_batch.shape)  # [batch, src_len]\n",
    "    print(\"trg_batch shape:\", trg_batch.shape)  # [batch, trg_len]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3afa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder / Decoder / Seq2Seq 모델 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX_EN)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.embedding(src)           # [batch, src_len, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)     # outputs: [batch, src_len, hidden_dim]\n",
    "        return hidden                            # hidden: [1, batch, hidden_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX_KO)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: [batch] (현재 시점 토큰 인덱스)\n",
    "        input = input.unsqueeze(1)               # [batch, 1]\n",
    "        embedded = self.embedding(input)         # [batch, 1, emb_dim]\n",
    "        output, hidden = self.rnn(embedded, hidden)   # output: [batch, 1, hidden_dim]\n",
    "        prediction = self.fc_out(output.squeeze(1))   # [batch, output_dim]\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # trg: [batch, trg_len]\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden = self.encoder(src)\n",
    "        # 디코더 첫 입력: 타깃 시퀀스의 <bos>\n",
    "        input = trg[:, 0]  # [batch]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # 가장 높은 확률 토큰 선택\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # teacher forcing 여부\n",
    "            use_tf = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if use_tf else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e211de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터 수: 608032\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 & 학습 루프\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_ko)\n",
    "EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_KO)\n",
    "\n",
    "print(\"학습 가능한 파라미터 수:\",\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fcb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] train loss: 0.4491\n",
      "[Epoch 2/50] train loss: 0.5150\n",
      "[Epoch 3/50] train loss: 0.4019\n",
      "[Epoch 4/50] train loss: 0.3340\n",
      "[Epoch 5/50] train loss: 0.5879\n",
      "[Epoch 6/50] train loss: 0.4286\n",
      "[Epoch 7/50] train loss: 0.3526\n",
      "[Epoch 8/50] train loss: 0.2668\n",
      "[Epoch 9/50] train loss: 0.4562\n",
      "[Epoch 10/50] train loss: 0.2005\n",
      "[Epoch 11/50] train loss: 0.1806\n",
      "[Epoch 12/50] train loss: 0.1623\n",
      "[Epoch 13/50] train loss: 0.1426\n",
      "[Epoch 14/50] train loss: 0.1263\n",
      "[Epoch 15/50] train loss: 0.1123\n",
      "[Epoch 16/50] train loss: 0.0984\n",
      "[Epoch 17/50] train loss: 0.0911\n",
      "[Epoch 18/50] train loss: 0.0816\n",
      "[Epoch 19/50] train loss: 0.0728\n",
      "[Epoch 20/50] train loss: 0.0663\n",
      "[Epoch 21/50] train loss: 0.0603\n",
      "[Epoch 22/50] train loss: 0.0552\n",
      "[Epoch 23/50] train loss: 0.0508\n",
      "[Epoch 24/50] train loss: 0.0472\n",
      "[Epoch 25/50] train loss: 0.0437\n",
      "[Epoch 26/50] train loss: 0.0404\n",
      "[Epoch 27/50] train loss: 0.0380\n",
      "[Epoch 28/50] train loss: 0.0359\n",
      "[Epoch 29/50] train loss: 0.0339\n",
      "[Epoch 30/50] train loss: 0.0320\n",
      "[Epoch 31/50] train loss: 0.0300\n",
      "[Epoch 32/50] train loss: 0.0289\n",
      "[Epoch 33/50] train loss: 0.0273\n",
      "[Epoch 34/50] train loss: 0.0261\n",
      "[Epoch 35/50] train loss: 0.0250\n",
      "[Epoch 36/50] train loss: 0.0240\n",
      "[Epoch 37/50] train loss: 0.0231\n",
      "[Epoch 38/50] train loss: 0.0221\n",
      "[Epoch 39/50] train loss: 0.0212\n",
      "[Epoch 40/50] train loss: 0.0204\n",
      "[Epoch 41/50] train loss: 0.0198\n",
      "[Epoch 42/50] train loss: 0.0190\n",
      "[Epoch 43/50] train loss: 0.0183\n",
      "[Epoch 44/50] train loss: 0.0177\n",
      "[Epoch 45/50] train loss: 0.0172\n",
      "[Epoch 46/50] train loss: 0.0166\n",
      "[Epoch 47/50] train loss: 0.0161\n",
      "[Epoch 48/50] train loss: 0.0156\n",
      "[Epoch 49/50] train loss: 0.0151\n",
      "[Epoch 50/50] train loss: 0.0147\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        # output: [batch, trg_len, output_dim]\n",
    "\n",
    "        output_dim = output.size(-1)\n",
    "\n",
    "        # <bos> 토큰을 제외하고 loss 계산\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)  # [batch*(trg_len-1), output_dim]\n",
    "        trg_y = trg[:, 1:].reshape(-1)                     # [batch*(trg_len-1)]\n",
    "\n",
    "        loss = criterion(output, trg_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "N_EPOCHS = 50\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, criterion, device,\n",
    "                           teacher_forcing_ratio=0.5)\n",
    "    print(f\"[Epoch {epoch}/{N_EPOCHS}] train loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2da42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[원문] I am a student.\n",
      "[정답] 나는 학생이다.\n",
      "[모델] 나는 학생이다.\n",
      "----------------------------------------\n",
      "[원문] He plays football.\n",
      "[정답] 그는 축구를 한다.\n",
      "[모델] 그는 축구를 한다.\n",
      "----------------------------------------\n",
      "[원문] She likes coffee.\n",
      "[정답] 그녀는 커피를 좋아한다.\n",
      "[모델] 그녀는 커피를 좋아한다\n",
      "----------------------------------------\n",
      "[원문] We study English.\n",
      "[정답] 우리는 영어를 공부한다.\n",
      "[모델] 우리는 영어를 공부한다\n",
      "----------------------------------------\n",
      "[원문] They are friends.\n",
      "[정답] 그들은 친구이다.\n",
      "[모델] 그들은 친구이다.\n",
      "----------------------------------------\n",
      "[원문] I like pizza.\n",
      "[정답] 나는 피자를 좋아한다.\n",
      "[모델] 나는 피자를 좋아한다.\n",
      "----------------------------------------\n",
      "=== 새 문장 테스트 ===\n",
      "입력: I like pizza.\n",
      "출력: 나는 피자를 좋아한다.\n"
     ]
    }
   ],
   "source": [
    "# 번역함수 & 테스트\n",
    "def translate_sentence(sentence, model, max_len_en=10, max_len_ko=12):\n",
    "    model.eval()\n",
    "\n",
    "    # 1) 입력 문장 인코딩\n",
    "    src_ids = encode_en(sentence, max_len_en).unsqueeze(0).to(device)  # [1, src_len]\n",
    "\n",
    "    # 2) 인코더\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_ids)\n",
    "\n",
    "    # 3) 디코더 시작: <bos> 토큰\n",
    "    input_token = torch.tensor([BOS_IDX_KO], dtype=torch.long, device=device)\n",
    "    generated_ids = [BOS_IDX_KO]\n",
    "\n",
    "    for _ in range(max_len_ko):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(input_token, hidden)\n",
    "            pred_token = output.argmax(1).item()\n",
    "\n",
    "        generated_ids.append(pred_token)\n",
    "\n",
    "        if pred_token == EOS_IDX_KO:\n",
    "            break\n",
    "\n",
    "        input_token = torch.tensor([pred_token], dtype=torch.long, device=device)\n",
    "\n",
    "    # 4) 인덱스를 토큰(글자)로 변환\n",
    "    tokens = [\n",
    "        rev_vocab_ko[idx]\n",
    "        for idx in generated_ids\n",
    "        if idx in rev_vocab_ko and rev_vocab_ko[idx] not in [\"<bos>\", \"<eos>\", \"<pad>\"]\n",
    "    ]\n",
    "\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "\n",
    "# 데이터셋에 있는 문장들로 테스트\n",
    "for i in range(len(df)):\n",
    "    src_text = df.iloc[i][\"en\"]\n",
    "    trg_text = df.iloc[i][\"ko\"]\n",
    "    pred = translate_sentence(src_text, model)\n",
    "\n",
    "    print(f\"[원문] {src_text}\")\n",
    "    print(f\"[정답] {trg_text}\")\n",
    "    print(f\"[모델] {pred}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# 새 문장 테스트\n",
    "custom = \"I like pizza.\"\n",
    "print(\"=== 새 문장 테스트 ===\")\n",
    "print(\"입력:\", custom)\n",
    "print(\"출력:\", translate_sentence(custom, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c9303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai5_project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
