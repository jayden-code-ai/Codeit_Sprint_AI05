"""
ğŸ¯ í•™ìŠµ ëª©í‘œ:
1. OpenAI APIë¥¼ FastAPIë¡œ ë˜í•‘í•˜ëŠ” ë°©ë²•
2. ë¹„ë™ê¸°(async) ì²˜ë¦¬ì˜ í•„ìš”ì„± ì´í•´
3. í™˜ê²½ë³€ìˆ˜ë¡œ API í‚¤ ê´€ë¦¬í•˜ê¸°
4. ì—ëŸ¬ í•¸ë“¤ë§ íŒ¨í„´

ğŸ“Œ ì¤€ë¹„1: pip install openai python-dotenv
ğŸ“Œ ì¤€ë¹„2: .env íŒŒì¼ ìƒì„±: OPENAI_API_KEY=[your-api-key-here]

ğŸ“Œ ì‹¤í–‰ ë°©ë²•:
uvicorn lab3_llm_api_wrapper:app --reload
"""

import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
from dotenv import load_dotenv
from openai import OpenAI

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(title="ë‚˜ë§Œì˜ LLM API ì„œë²„")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ============================================
# 1ë‹¨ê³„: ê¸°ë³¸ ì±„íŒ… ì™„ì„± API
# ============================================
class Message(BaseModel):
    role: str = Field(pattern=r"^(user|assistant|system)$")
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4o-mini"
    temperature: float = Field(default=0.7, ge=0, le=2)
    max_tokens: int = Field(default=1000, ge=1, le=4096)

    model_config = {
        "json_schema_extra":{
            "examples": [{
                "messages": [
                    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
                ]
            }]
        }
    }

class ChatResponse(BaseModel):
    response: str
    model: str
    usage: dict

@app.get("/")
def home():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {"message": "LLM API ì„œë²„ê°€ ì‹¤í–‰ì¤‘ì…ë‹ˆë‹¤..."}

@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """
    ê¸°ë³¸ ì±„íŒ… API

    ğŸ’¡ OpenAI APIë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ì•Šê³  ë˜í•‘í•˜ëŠ” ì¥ì :
    1. API í‚¤ ë³´í˜¸ (í´ë¼ì´ì–¸íŠ¸ì— í‚¤ ë…¸ì¶œ ì•ˆ í•¨)
    2. ìš”ì²­/ì‘ë‹µ í˜•ì‹ ì»¤ìŠ¤í„°ë§ˆì´ì§•
    3. ë¡œê¹…, ëª¨ë‹ˆí„°ë§ ì¶”ê°€ ê°€ëŠ¥
    4. ë¹„ìš© ì œì–´ (max_tokens ì œí•œ ë“±)
    """
    try:
        # https://platform.openai.com/docs/api-reference/chat/create?lang=python
        response = client.chat.completions.create(
            model = request.model,
            messages=[m.model_dump() for m in request.messages],
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        return ChatResponse(
            response=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    

# ============================================
# í˜¼ìí•´ë³´ê¸° 3: ì§ˆë¬¸ ë‹µë³€ API 
# ============================================
"""
ì•„ë˜ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸°:

POST /ask
- ìš”ì²­ ë°”ë””:
  {
    "question": "íŒŒì´ì¬ì´ ë­ì•¼?"
  }
- ì‘ë‹µ:
  {
    "answer": "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤..."
  }

íŒíŠ¸:
- ìš”ì²­ ëª¨ë¸: question í•„ë“œ 1ê°œë§Œ
- ì‘ë‹µ ëª¨ë¸: answer í•„ë“œ 1ê°œë§Œ
- messagesì— user roleë¡œ question ë„£ê¸°
"""
