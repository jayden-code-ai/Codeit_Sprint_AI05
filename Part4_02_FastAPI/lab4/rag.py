"""
ğŸ¯ FastAPI ì‹¤ìŠµ: LangChain RAG ì‹œìŠ¤í…œ êµ¬ì¶•
1. ChromaDB ë²¡í„° ì €ì¥ì†Œë¥¼ í™œìš©í•œ ë¬¸ì„œ ì„ë² ë”© ë° ê²€ìƒ‰
2. LCEL(LangChain Expression Language) ì²´ì¸ êµ¬ì„± íŒ¨í„´
3. Retriever â†’ Prompt â†’ LLM â†’ Parser íŒŒì´í”„ë¼ì¸ 
4. ê°•ì˜ê³„íšì„œ ê¸°ë°˜ Q&A ì±—ë´‡ API êµ¬í˜„

âš ï¸ Python ë²„ì „ ì£¼ì˜:
- Python 3.14ì™€ ChromaDB í˜¸í™˜ ì´ìŠˆ ìˆìŒ!
- Python 3.12 ë²„ì „ ë˜ëŠ” ì´ì „ ë²„ì „ ì‚¬ìš© ê¶Œì¥
 
ğŸ“Œ ì‚¬ì „ ì¤€ë¹„:
1. pip install fastapi uvicorn langchain-openai langchain-community langchain-core langgraph chromadb tiktoken
2. .env íŒŒì¼ì— OPENAI_API_KEY=sk-xxx ì¶”ê°€

ğŸ“Œ ì‹¤í–‰ ë°©ë²•:
python ./lab4/rag_qa.py

ğŸ’¡ RAG íŒŒì´í”„ë¼ì¸ íë¦„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Question â”‚ â†’ â”‚ Retriever â”‚ â†’ â”‚ Prompt â”‚ â†’ â”‚ LLM â”‚ â†’ â”‚ Answer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  (ê²€ìƒ‰)         (ì»¨í…ìŠ¤íŠ¸      (GPT)     (ë¬¸ìì—´)
                                  + ì§ˆë¬¸)

"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager

# LangChain v0.1 Core Imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# VectorStore & Document
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 1. ì§€ì‹ ë² ì´ìŠ¤ (ê°•ì˜ê³„íšì„œë¼ê³  ê°€ì •)
syllabus_text = """
[FastAPI ë° AI ì›¹ ê°œë°œ ê³¼ì •]
1ì£¼ì°¨: Python ê¸°ì´ˆ ë° FastAPI ê¸°ë³¸ êµ¬ì¡° (Hello World, Path Param)
2ì£¼ì°¨: Pydantic ë°ì´í„° ê²€ì¦ ë° ë¹„ë™ê¸° ì²˜ë¦¬ (Async/Await)
3ì£¼ì°¨: Hugging Face Transformers í™œìš© (ê°ì„±ë¶„ì„, ì´ë¯¸ì§€ ë¶„ë¥˜)
4ì£¼ì°¨: OpenAI API ë° LangChain ê¸°ì´ˆ (RAG, Prompt Engineering)
5ì£¼ì°¨: LangGraph ì—ì´ì „íŠ¸ ë° Streamlit ì‹¤ìŠµ
í‰ê°€ ë°©ë²•: ì¶œì„ 20%, ì¤‘ê°„ ê³¼ì œ 30%, ìµœì¢… í”„ë¡œì íŠ¸ 50%
"""

# ì „ì—­ ë³€ìˆ˜ë¡œ ì²´ì¸ 
rag_chain = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸ”„ ë²¡í„° DB êµ¬ì¶• ë° RAG ì²´ì¸ ìƒì„± ì¤‘...")
    global rag_chain

    # 1. ë¬¸ì„œ ìƒì„±
    docs = [Document(page_content=syllabus_text, metadata={"source": "ê°•ì˜ê³„íšì„œ"})]
    
    # 2. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± 
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=OpenAIEmbeddings(),
        collection_name="course_syllabus"
    )
    retriever = vectorstore.as_retriever()

    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ 
    template = """Answer the question based only on the following context:
    {context}

    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))    # type: ignore

    # 4. LCEL ì²´ì¸ êµ¬ì„± (Retriever -> Context ë³‘í•© -> Prompt -> LLM -> String)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )
    print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    yield
    print("ğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ")

app = FastAPI(lifespan=lifespan)

class QuestionRequest(BaseModel):
    question: str

@app.post("/ask-syllabus")
async def ask_syllabus(req: QuestionRequest):
    if not rag_chain:
        raise HTTPException(status_code=500, detail="RAG Chain not init")
    
    #ì²´ì¸ ì‹¤í–‰(ainvoke): # ë¹„ë™ê¸°(async)ë¡œ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê³  ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° ë’¤ ê²°ê³¼ë¥¼ ë°›ìŒ
    response = await rag_chain.ainvoke(req.question)
    return {"answer": response}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)