"""
ğŸ¯ PyTorch ì‹¤ìŠµ: MNIST CNN ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
1. CNN(Convolutional Neural Network) ëª¨ë¸ êµ¬ì¡° ì´í•´
2. MNIST ì†ê¸€ì”¨ ìˆ«ì ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
3. í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ .pth íŒŒì¼ë¡œ ì €ì¥
4. ì €ì¥ëœ ëª¨ë¸ì„ FastAPIì—ì„œ ë¡œë“œí•˜ì—¬ ì„œë¹™í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ì˜ ì²« ë‹¨ê³„

ğŸ“Œ ì‚¬ì „ ì¤€ë¹„:
pip install torch torchvision

ğŸ“Œ ì‹¤í–‰ ë°©ë²•(ëª¨ë¸ì €ì¥ ìœ„ì¹˜ë¥¼ ìœ„í•´ì„œ ë””ë ‰í„°ë¦¬ ì´ë™í›„ì—: "cd lab3") 
python train_mnist.py

ğŸ“Œ ìƒì„±ë˜ëŠ” íŒŒì¼:
- mnist_cnn.pth (í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜)
- ./data/MNIST/ (ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹)

ğŸ’¡ CNN ëª¨ë¸ êµ¬ì¡°:
ì…ë ¥ [1,28,28] â†’ Conv2d â†’ ReLU â†’ MaxPool â†’ Flatten â†’ FC â†’ FC â†’ ì¶œë ¥ [10]
- Conv2d: ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§•(ì—£ì§€, íŒ¨í„´) ì¶”ì¶œ
- MaxPool: íŠ¹ì§• ë§µ í¬ê¸° ì¶•ì†Œ, ì¤‘ìš” ì •ë³´ë§Œ ìœ ì§€
- Flatten: 2D â†’ 1D ë³€í™˜ (FC ë ˆì´ì–´ ì…ë ¥ìš©)
- FC(Linear): ìµœì¢… ë¶„ë¥˜ (0~9 ìˆ«ì)

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ì´ ëª¨ë¸ í´ë˜ìŠ¤ëŠ” ì„œë¹™ ì½”ë“œì—ì„œë„ ë™ì¼í•˜ê²Œ ì •ì˜í•´ì•¼ í•¨!
- 1 Epochë§Œ í•™ìŠµ (ì‹¤ìŠµìš©)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# 1. CNN ëª¨ë¸ ì •ì˜ (ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì í•©í•œ ëª¨ë¸)
class MNISTModel(nn.Module):
    def __init__(self):
        super(MNISTModel, self).__init__()
        # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Convolution)
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) 
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2)
        # ë¶„ë¥˜ (Linear)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(32 * 13 * 13, 128)
        self.fc2 = nn.Linear(128, 10) # 0~9 ìˆ«ì ë¶„ë¥˜

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train():
    # ë°ì´í„°ì…‹ ì¤€ë¹„ (ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•¨)
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

    model = MNISTModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    model.train()
    print("ğŸ§  í•™ìŠµ ì‹œì‘ (ë°ì´í„°ê°€ ë§ì•„ 1~2ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    # ë¹ ë¥´ê²Œ 1 Epochë§Œ í•™ìŠµ (ì‹¤ìŠµìš©)
    for batch_idx, (data, target) in enumerate(loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f"ì§„í–‰ë¥ : {batch_idx}/{len(loader)}")

    # ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "mnist_cnn.pth")
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: mnist_cnn.pth")

if __name__ == "__main__":
    train()