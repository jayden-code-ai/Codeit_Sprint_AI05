{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Reference Notebook\n",
        "\n",
        "이 노트북은 기본 Transformer 구조를 단계별로 다시 정리한 참고용 버전입니다. 기존 실습 노트북과 나란히 비교하면서 구성 요소와 학습 루프를 살펴볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 진행 순서\n",
        "1. 필수 라이브러리와 설정 값 정의\n",
        "2. SentencePiece 기반 토크나이저 & 데이터 파이프라인\n",
        "3. 마스킹, 포지셔널 인코딩, 어텐션 블록\n",
        "4. Encoder / Decoder / 전체 Transformer 구성\n",
        "5. 학습 루프와 생성(Greedy Decoding) 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "eaabcfbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc6f8e9",
      "metadata": {},
      "source": [
        "### 설정 값 (Config)\n",
        "필요한 경로 및 하이퍼파라미터를 한 곳에서 관리합니다. SentencePiece 학습 시 사용할 텍스트 파일도 미리 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "37ade902",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config(dataset_csv='/mnt/nas/jayden_code/Codeit_Practice/Part3_01_자연어처리_실습/train.csv', sp_input_txt='/mnt/nas/jayden_code/Codeit_Practice/Part3_02_TransFormer_실습/train.txt', sp_model_prefix='Part3_02_TransFormer_실습/model/spm_reference', vocab_size=2000, character_coverage=0.9995, max_length=40, num_layers=2, d_model=128, num_heads=4, d_ff=256, dropout=0.1, pad_id=3, pad_piece='<pad>', bos_id=1, eos_id=2)\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    dataset_csv: str = \"/mnt/nas/jayden_code/Codeit_Practice/Part3_01_자연어처리_실습/train.csv\"\n",
        "    sp_input_txt: str = \"/mnt/nas/jayden_code/Codeit_Practice/Part3_02_TransFormer_실습/train.txt\"\n",
        "    sp_model_prefix: str = \"Part3_02_TransFormer_실습/model/spm_reference\"\n",
        "    vocab_size: int = 2000\n",
        "    character_coverage: float = 0.9995\n",
        "    max_length: int = 40\n",
        "    num_layers: int = 2\n",
        "    d_model: int = 128\n",
        "    num_heads: int = 4\n",
        "    d_ff: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pad_id: int = 3\n",
        "    pad_piece: str = \"<pad>\"\n",
        "    bos_id: int = 1\n",
        "    eos_id: int = 2\n",
        "\n",
        "CFG = Config()\n",
        "Path(CFG.sp_model_prefix).parent.mkdir(parents=True, exist_ok=True)\n",
        "print(CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3522ddb",
      "metadata": {},
      "source": [
        "## SentencePiece & 데이터 로딩\n",
        "SentencePiece 모델이 없다면 `train_sentencepiece`를 먼저 실행합니다. 이미 학습된 모델이 있다면 바로 불러오면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d91cc009",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 2000\n"
          ]
        }
      ],
      "source": [
        "def train_sentencepiece(cfg: Config):\n",
        "    sp_input = Path(cfg.sp_input_txt)\n",
        "    if not sp_input.exists():\n",
        "        df = pd.read_csv(cfg.dataset_csv)\n",
        "        with sp_input.open('w', encoding='utf-8') as f:\n",
        "            for text in df['HS01']:\n",
        "                f.write(str(text).strip() + '\\n')\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=str(sp_input),\n",
        "        model_prefix=cfg.sp_model_prefix,\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        character_coverage=cfg.character_coverage,\n",
        "        model_type='unigram',\n",
        "        bos_id=cfg.bos_id,\n",
        "        eos_id=cfg.eos_id,\n",
        "        pad_id=cfg.pad_id,\n",
        "        pad_piece=cfg.pad_piece\n",
        "    )\n",
        "\n",
        "\n",
        "def load_sentencepiece(cfg: Config):\n",
        "    model_path = Path(cfg.sp_model_prefix + '.model')\n",
        "    if not model_path.exists():\n",
        "        print('SentencePiece 모델이 없어 새로 학습합니다...')\n",
        "        train_sentencepiece(cfg)\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(str(model_path))\n",
        "    return sp\n",
        "\n",
        "\n",
        "sp = load_sentencepiece(CFG)\n",
        "print('vocab size:', sp.get_piece_size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db98bf6",
      "metadata": {},
      "source": [
        "### 토큰화 & 데이터셋\n",
        "문장을 `[BOS] + 토큰 + [EOS]` 형태로 만들고, 최대 길이에 맞게 패딩합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "91262003",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset size: 51628\n",
            "torch.Size([8, 40]) torch.Size([8, 40]) torch.Size([8, 40])\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sp, text: str, cfg: Config) -> torch.Tensor:\n",
        "    tokens = sp.encode(text, out_type=int)\n",
        "    tokens = tokens[: cfg.max_length - 2]\n",
        "    tokens = [cfg.bos_id] + tokens + [cfg.eos_id]\n",
        "    pad_len = cfg.max_length - len(tokens)\n",
        "    if pad_len > 0:\n",
        "        tokens.extend([cfg.pad_id] * pad_len)\n",
        "    else:\n",
        "        tokens = tokens[: cfg.max_length]\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "\n",
        "class CounselingDataset(Dataset):\n",
        "    def __init__(self, cfg: Config, sp):\n",
        "        df = pd.read_csv(cfg.dataset_csv)\n",
        "        self.pairs = list(zip(df['HS01'].astype(str), df['SS01'].astype(str)))\n",
        "        self.sp = sp\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "        encoder_input = encode_sentence(self.sp, src, self.cfg)\n",
        "        decoder_input = encode_sentence(self.sp, tgt, self.cfg)\n",
        "        decoder_target = torch.roll(decoder_input, shifts=-1, dims=0)\n",
        "        decoder_target[-1] = self.cfg.pad_id\n",
        "        return encoder_input, decoder_input, decoder_target\n",
        "\n",
        "\n",
        "dataset = CounselingDataset(CFG, sp)\n",
        "print('dataset size:', len(dataset))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "encoder_sample, decoder_sample, target_sample = next(iter(dataloader))\n",
        "print(encoder_sample.shape, decoder_sample.shape, target_sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c8dc81",
      "metadata": {},
      "source": [
        "## 마스킹 & 포지셔널 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "963c281a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_padding_mask(seq: torch.Tensor, pad_id: int) -> torch.Tensor:\n",
        "    return (seq == pad_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size: int, device=None) -> torch.Tensor:\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    mask = torch.triu(torch.ones((size, size), device=device), diagonal=1).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def combine_masks(tgt: torch.Tensor, pad_mask: torch.Tensor) -> torch.Tensor:\n",
        "    seq_len = tgt.size(1)\n",
        "    look_ahead = create_look_ahead_mask(seq_len, tgt.device)\n",
        "    look_ahead = look_ahead.unsqueeze(0).unsqueeze(0)\n",
        "    return pad_mask | look_ahead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0ebad8a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pe[:, : x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971e4898",
      "metadata": {},
      "source": [
        "## 어텐션 블록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "51e05ea1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    dk = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 1, float('-inf'))\n",
        "    attn = torch.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn, v)\n",
        "    return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "bfb5d0bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz, seq_len, d_model = x.shape\n",
        "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def forward(self, v, k, q, mask):\n",
        "        q = self.split_heads(self.wq(q))\n",
        "        k = self.split_heads(self.wk(k))\n",
        "        v = self.split_heads(self.wv(v))\n",
        "\n",
        "        if mask is not None and mask.dim() == 3:\n",
        "            mask_ = mask.unsqueeze(1)\n",
        "        else:\n",
        "            mask_ = mask\n",
        "\n",
        "        scaled_attention, attn = scaled_dot_product_attention(q, k, v, mask_)\n",
        "        scaled_attention = scaled_attention.transpose(1, 2).contiguous()\n",
        "        concat_attention = scaled_attention.view(scaled_attention.size(0), -1, self.num_heads * self.depth)\n",
        "        return self.dense(concat_attention), attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7e26bbfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.linear2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4610083c",
      "metadata": {},
      "source": [
        "## Encoder / Decoder 레이어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "53c0bbe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out, _ = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_out))\n",
        "        ffn_out = self.ffn(x)\n",
        "        return self.norm2(x + self.dropout2(ffn_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "cb92ee6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        x = self.norm1(x + self.dropout1(attn1))\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, x, padding_mask)\n",
        "        x = self.norm2(x + self.dropout2(attn2))\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm3(x + self.dropout3(ffn_out))\n",
        "        return x, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069b805f",
      "metadata": {},
      "source": [
        "## Encoder / Decoder / Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "92b24b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_encoding = PositionalEncoding(cfg.d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(cfg.d_model, cfg.num_heads, cfg.d_ff, cfg.dropout)\n",
        "            for _ in range(cfg.num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5e731bd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_encoding = PositionalEncoding(cfg.d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(cfg.d_model, cfg.num_heads, cfg.d_ff, cfg.dropout)\n",
        "            for _ in range(cfg.num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        attn_weights = {}\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            x, block1, block2 = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "            attn_weights[f'decoder_layer{idx+1}_block1'] = block1\n",
        "            attn_weights[f'decoder_layer{idx+1}_block2'] = block2\n",
        "        return x, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "3abbbf59",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = Encoder(cfg)\n",
        "        self.decoder = Decoder(cfg)\n",
        "        self.final_layer = nn.Linear(cfg.d_model, cfg.vocab_size)\n",
        "\n",
        "    def forward(self, enc_inp, dec_inp, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(enc_inp, enc_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(dec_inp, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        logits = self.final_layer(dec_output)\n",
        "        return logits, attention_weights\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, max_len):\n",
        "        self.eval()\n",
        "        enc_padding_mask = create_padding_mask(src, self.cfg.pad_id)\n",
        "        enc_output = self.encoder(src, enc_padding_mask)\n",
        "\n",
        "        dec_input = torch.full((src.size(0), 1), self.cfg.bos_id, dtype=torch.long, device=src.device)\n",
        "        for _ in range(max_len - 1):\n",
        "            dec_padding_mask = create_padding_mask(dec_input, self.cfg.pad_id)\n",
        "            combined_mask = combine_masks(dec_input, dec_padding_mask)\n",
        "            predictions, _ = self.decoder(dec_input, enc_output, combined_mask, enc_padding_mask)\n",
        "            logits = self.final_layer(predictions[:, -1:, :])\n",
        "            next_token = torch.argmax(logits, dim=-1)\n",
        "            dec_input = torch.cat([dec_input, next_token], dim=1)\n",
        "            if (next_token == self.cfg.eos_id).all():\n",
        "                break\n",
        "        return dec_input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a8312c",
      "metadata": {},
      "source": [
        "## 학습 루프 Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d35a99ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(cfg: Config) -> Transformer:\n",
        "    model = Transformer(cfg)\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def loss_function(predictions, targets, pad_id):\n",
        "    predictions = predictions.view(-1, predictions.size(-1))\n",
        "    targets = targets.view(-1)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "    return criterion(predictions, targets)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, cfg: Config):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for enc_inp, dec_inp, dec_target in dataloader:\n",
        "        enc_inp, dec_inp, dec_target = enc_inp.to(device), dec_inp.to(device), dec_target.to(device)\n",
        "        enc_padding_mask = create_padding_mask(enc_inp, cfg.pad_id)\n",
        "        dec_padding_mask = create_padding_mask(dec_inp, cfg.pad_id)\n",
        "        combined_mask = combine_masks(dec_inp, dec_padding_mask)\n",
        "\n",
        "        logits, _ = model(enc_inp, dec_inp, enc_padding_mask, combined_mask, enc_padding_mask)\n",
        "        loss = loss_function(logits, dec_target, cfg.pad_id)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a12299",
      "metadata": {},
      "source": [
        "## 디코딩 예시\n",
        "학습된 체크포인트가 있다면 불러온 뒤 `greedy_decode`로 간단히 결과를 볼 수 있습니다. 아래 예시는 랜덤 초기화 모델이므로 의미 있는 문장을 만들지는 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0608e9e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: torch.Size([8, 40, 2000])\n",
            "generated token ids: [1, 1026, 1572, 750, 1106, 621, 1785, 1617, 1898, 22, 1728, 1975, 1208, 52, 519, 951, 1672, 923, 1254, 161, 1203, 814, 28, 1844, 1264, 77, 369, 175, 1059, 969, 1037, 1297, 351, 1036, 1078, 1718, 1413, 1782, 1161, 34]\n"
          ]
        }
      ],
      "source": [
        "model = build_model(CFG)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "enc_inp, dec_inp, dec_target = next(iter(dataloader))\n",
        "enc_inp, dec_inp = enc_inp.to(device), dec_inp.to(device)\n",
        "enc_mask = create_padding_mask(enc_inp, CFG.pad_id)\n",
        "dec_mask = create_padding_mask(dec_inp, CFG.pad_id)\n",
        "combined = combine_masks(dec_inp, dec_mask)\n",
        "logits, attn = model(enc_inp, dec_inp, enc_mask, combined, enc_mask)\n",
        "print('logits shape:', logits.shape)\n",
        "\n",
        "sample_src = enc_inp[:1]\n",
        "output_tokens = model.greedy_decode(sample_src, CFG.max_length)\n",
        "print('generated token ids:', output_tokens.squeeze(0).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d0c6ee",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99702a26",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai5_project1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
