{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mission 10 Â· Text Embedding & Classification Reference\n",
        "\n",
        "Baseline ì½”ë“œë¥¼ í™•ì¥í•´ **Word2Vec / FastText / GloVe** ì„ë² ë”© + **RNN ë¶„ë¥˜ê¸°** ì‹¤í—˜ì„ í•œ ë²ˆì— ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤. ê° ë‹¨ê³„ëŠ” ë§ˆí¬ë‹¤ìš´ê³¼ ì£¼ì„ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆì–´, ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©° êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•˜ê±°ë‚˜ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ê¸°ë³¸ ì„¤ì •\n",
        "- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "- GPU / CPU ë””ë°”ì´ìŠ¤ í™•ì¸\n",
        "- ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬: ê²½ë¡œ/ë‚œìˆ˜/ì •ê·œì‹/ì¹´ìš´íŒ…/ë°ì´í„°êµ¬ì¡°/ê²½ë¡œê°ì²´/íƒ€ì…íŒíŠ¸\n",
        "import os                   # ğŸ“ íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì¡°ì‘ (ì´ë²ˆ ì½”ë“œì—ì„  ì£¼ë¡œ ê²½ë¡œ ìƒìˆ˜/ì¡´ì¬í™•ì¸ ë“±ì— í™œìš© ê°€ëŠ¥)\n",
        "import random               # ğŸ² íŒŒì´ì¬ ë‚œìˆ˜ ë°œìƒê¸°(ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± í™•ë³´)\n",
        "import re                   # âœ‚ï¸ ì •ê·œí‘œí˜„ì‹(í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)\n",
        "from collections import Counter  # ğŸ”¢ í† í° ë¹ˆë„ìˆ˜ ì§‘ê³„(ë‹¨ì–´ ë¶„í¬ í™•ì¸ì— ìœ ìš©)\n",
        "from dataclasses import dataclass # ğŸ“¦ ì‹¤í—˜ ì„¤ì •/í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ì¡°ì²´ë¡œ ë¬¶ì„ ë•Œ ìœ ìš©\n",
        "from pathlib import Path     # ğŸ›£ï¸ OS ë…ë¦½ì ì¸ ê²½ë¡œ ì²˜ë¦¬\n",
        "from typing import Dict, List, Tuple  # ğŸ·ï¸ ê°€ë…ì„±Â·ì •í™•ì„±ì„ ìœ„í•œ íƒ€ì… íŒíŠ¸\n",
        "\n",
        "# ìˆ˜ì¹˜/ë”¥ëŸ¬ë‹/í‰ê°€ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import numpy as np           # ğŸ”¢ ë²¡í„°/í–‰ë ¬ ì—°ì‚°(ì„ë² ë”© í‰ê· , ìŠ¤ì¼€ì¼ë§ ë“±)\n",
        "import torch                 # ğŸ”¥ PyTorch í…ì„œ/ìë™ë¯¸ë¶„/ë””ë°”ì´ìŠ¤ ê´€ë¦¬\n",
        "import torch.nn as nn        # ğŸ§  ì‹ ê²½ë§ ë ˆì´ì–´/ì†ì‹¤í•¨ìˆ˜ ë“±\n",
        "from sklearn.datasets import fetch_20newsgroups  # ğŸ“° 20ê°œ ë‰´ìŠ¤ê·¸ë£¹ ë°ì´í„° ë¡œë”\n",
        "from sklearn.metrics import accuracy_score, classification_report  # ğŸ“Š ì„±ëŠ¥ì§€í‘œ ê³„ì‚°\n",
        "from sklearn.model_selection import train_test_split              # ğŸ”€ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
        "from torch.utils.data import DataLoader, Dataset                  # ğŸšš ë°°ì¹˜ ë¡œë”©/ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹\n",
        "\n",
        "# ì„ë² ë”©/í† í°í™”\n",
        "from gensim.models import Word2Vec, FastText   # ğŸ§© ë‹¨ì–´ ì„ë² ë”© í•™ìŠµ(Word2Vec/ì„œë¸Œì›Œë“œ ê¸°ë°˜ FastText)\n",
        "from nltk.corpus import stopwords              # ğŸ§¹ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì „ì²˜ë¦¬)\n",
        "from nltk.tokenize import word_tokenize        # âœ‚ï¸ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”\n",
        "import nltk                                    # ğŸ§° NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ/ê´€ë¦¬\n",
        "\n",
        "# (ì„ íƒ) GloVe ì‚¬ì „í•™ìŠµ ì„ë² ë”©: torchtextê°€ ì„¤ì¹˜/í˜¸í™˜ë  ë•Œë§Œ ì‚¬ìš©\n",
        "try:\n",
        "    from torchtext.vocab import GloVe          # ğŸ“¦ ì‚¬ì „ í•™ìŠµëœ GloVe ë²¡í„° ë¡œë”\n",
        "    TORCHTEXT_AVAILABLE = True                 # âœ… ì‚¬ìš© ê°€ëŠ¥ í”Œë˜ê·¸\n",
        "except Exception:\n",
        "    TORCHTEXT_AVAILABLE = False                # âŒ í™˜ê²½ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì²˜ë¦¬\n",
        "\n",
        "# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ(ìµœì´ˆ 1íšŒ í•„ìš”)\n",
        "nltk.download('punkt')       # ğŸ”» í† í°í™”ì— í•„ìš”í•œ ë°ì´í„°(ë§ˆì¹¨í‘œ/êµ¬ë‘ì  ê·œì¹™ ë“±)\n",
        "nltk.download('stopwords')   # ğŸ”» ë¶ˆìš©ì–´ ì‚¬ì „(ì˜ì–´ ë“± ë‹¤ìˆ˜ ì–¸ì–´)\n",
        "\n",
        "# ì¬í˜„ì„± í™•ë³´: ì‹œë“œ ê³ ì •(ë‚œìˆ˜ì— ì˜ì¡´í•˜ëŠ” í•™ìŠµ/ìƒ˜í”Œë§ ë³€ë™ì„ ìµœì†Œí™”)\n",
        "SEED = 42\n",
        "random.seed(SEED)            # ğŸ² íŒŒì´ì¬ í‘œì¤€ ë‚œìˆ˜ ì‹œë“œ\n",
        "np.random.seed(SEED)         # ğŸ² NumPy ë‚œìˆ˜ ì‹œë“œ\n",
        "torch.manual_seed(SEED)      # ğŸ² PyTorch CPU ì—°ì‚° ë‚œìˆ˜ ì‹œë“œ\n",
        "torch.cuda.manual_seed_all(SEED)  # ğŸ² (ë©€í‹°)GPU ì—°ì‚° ë‚œìˆ˜ ì‹œë“œ(ë“œë¡­ì•„ì›ƒ/ì´ˆê¸°í™” ë“± ì¼ê´€ì„±)\n",
        "\n",
        "# í•™ìŠµ ë””ë°”ì´ìŠ¤ ì„ íƒ: CUDAê°€ ìˆìœ¼ë©´ GPU ì‚¬ìš©, ì—†ìœ¼ë©´ CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)  # ğŸ–¨ï¸ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ ì¶œë ¥(ë””ë²„ê¹…/ë¡œê·¸ìš©)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ë°ì´í„° ë¡œë“œ & íƒìƒ‰\n",
        "`fetch_20newsgroups` ì „ì²´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ í…ìŠ¤íŠ¸/ë ˆì´ë¸”ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "news_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "texts = news_data.data\n",
        "labels = news_data.target\n",
        "label_names = news_data.target_names\n",
        "\n",
        "print(f\"ì´ ë¬¸ì„œ ìˆ˜: {len(texts)}\")\n",
        "print(f\"ë ˆì´ë¸” ê°œìˆ˜: {len(label_names)}\")\n",
        "print('ìƒ˜í”Œ ë¬¸ì„œ:', texts[0][:500])\n",
        "print('ìƒ˜í”Œ ë ˆì´ë¸”:', label_names[labels[0]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "- ì •ê·œí™”/í† í°í™”/ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜ ì •ì˜\n",
        "- í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
        "- ë¬¸ì¥ ê¸¸ì´ í†µê³„ë¥¼ í†µí•´ íŒ¨ë”© ê¸¸ì´ë¥¼ ê²°ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”¹ ì˜ì–´ ë¶ˆìš©ì–´(stopwords) ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ ì§‘í•©(set) í˜•íƒœë¡œ ì €ì¥\n",
        "#   setìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì´ìœ : ê²€ìƒ‰ ì†ë„ê°€ ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê¸° ë•Œë¬¸\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ğŸ”¹ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ì •ì˜\n",
        "#   [^a-zA-Z']+  â†’ ì•ŒíŒŒë²³ê³¼ ì‘ì€ë”°ì˜´í‘œ(')ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë“¤ì„ ì˜ë¯¸\n",
        "#   ì¦‰, íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì/ê³µë°± ë“±ì„ ê³µë°±(' ')ìœ¼ë¡œ ëŒ€ì²´í•˜ê¸° ìœ„í•œ íŒ¨í„´\n",
        "TOKEN_PATTERN = re.compile(r\"[^a-zA-Z']+\")\n",
        "\n",
        "\n",
        "def clean_and_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
        "    ì…ë ¥ ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸(List[str])ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    ì£¼ìš” ë‹¨ê³„:\n",
        "    1ï¸âƒ£ ì†Œë¬¸ì ë³€í™˜(lower)\n",
        "    2ï¸âƒ£ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ)\n",
        "    3ï¸âƒ£ ë‹¨ì–´ í† í°í™” (nltkì˜ word_tokenize ì‚¬ìš©)\n",
        "    4ï¸âƒ£ ë¶ˆìš©ì–´(stopword) ë° í•œ ê¸€ì ë‹¨ì–´ ì œê±°\n",
        "    \"\"\"\n",
        "\n",
        "    text = text.lower()                  # 1ï¸âƒ£ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜ â†’ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì œê±°\n",
        "    text = TOKEN_PATTERN.sub(' ', text)  # 2ï¸âƒ£ ì•ŒíŒŒë²³/â€˜ ì™¸ì˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ â†’ íŠ¹ìˆ˜ë¬¸ì ì •ì œ\n",
        "    tokens = word_tokenize(text)         # 3ï¸âƒ£ ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
        "    # 4ï¸âƒ£ ë¶ˆìš©ì–´ ë° í•œ ê¸€ì(ê¸¸ì´ 1 ì´í•˜) ë‹¨ì–´ ì œê±°\n",
        "    tokens = [tok for tok in tokens if tok not in stop_words and len(tok) > 1]\n",
        "    return tokens                        # 5ï¸âƒ£ ì •ì œëœ í† í° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# âœ… ë°ì´í„°ì…‹ ë¶„ë¦¬ ë‹¨ê³„\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ğŸ”¹ 1ì°¨ ë¶„í• : Train/Test (ì „ì²´ ë°ì´í„° ì¤‘ 20%ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ë¶„ë¦¬)\n",
        "#   stratify=labels â†’ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•œ ì±„ë¡œ ë¶„ë¦¬ (ë¶ˆê· í˜• ë°ì´í„° ë°©ì§€)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=SEED, stratify=labels\n",
        ")\n",
        "\n",
        "# ğŸ”¹ 2ì°¨ ë¶„í• : Train â†’ Train/Validation (í›ˆë ¨ ë°ì´í„° ì¤‘ 10%ë¥¼ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬)\n",
        "#   stratify=train_labels â†’ í•™ìŠµ ë°ì´í„° ë‚´ì—ì„œë„ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=SEED, stratify=train_labels\n",
        ")\n",
        "\n",
        "# ğŸ”¹ ë¶„í•  ê²°ê³¼ í™•ì¸\n",
        "#   train_texts: í•™ìŠµìš© ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "#   val_texts: ê²€ì¦ìš© ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "#   test_texts: í…ŒìŠ¤íŠ¸ìš© ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "print('Train/Val/Test sizes:', len(train_texts), len(val_texts), len(test_texts))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### í† í°í™” & ê¸¸ì´ ë¶„ì„\n",
        "ê¸¸ì´ ë¶„í¬ë¥¼ ë³´ê³  ìµœëŒ€ ê¸¸ì´ë¥¼ ìœ ì—°í•˜ê²Œ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_corpus(text_list: List[str]) -> List[List[str]]:\n",
        "    # âœ… ì…ë ¥: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸(text_list)\n",
        "    # âœ… ì¶œë ¥: ê° ë¬¸ì„œë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¾¼ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸(List[List[str]])\n",
        "    #    clean_and_tokenize = (ì†Œë¬¸ìí™” â†’ íŠ¹ìˆ˜ë¬¸ì ì œê±° â†’ í† í°í™” â†’ ë¶ˆìš©ì–´/1ê¸€ì ì œê±°)\n",
        "    return [clean_and_tokenize(t) for t in text_list]\n",
        "\n",
        "\n",
        "# ğŸ”¹ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "train_tokens = tokenize_corpus(train_texts)  # List[str] â†’ List[List[str]]\n",
        "val_tokens = tokenize_corpus(val_texts)\n",
        "test_tokens = tokenize_corpus(test_texts)\n",
        "\n",
        "# ğŸ”¹ ê° í•™ìŠµ ë¬¸ì„œì˜ í† í° ê¸¸ì´ë¥¼ ìˆ˜ì§‘ (ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬ íŒŒì•…)\n",
        "lengths = [len(toks) for toks in train_tokens]\n",
        "\n",
        "# ğŸ”¹ ê¸¸ì´ ìƒí•œ í›„ë³´ë¥¼ 95í¼ì„¼íƒ€ì¼ë¡œ ì„¤ì •\n",
        "#    ë„ˆë¬´ ê¸´ ê¼¬ë¦¬(long tail) ë¬¸ì„œ ë•Œë¬¸ì— íŒ¨ë”©/ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€\n",
        "max_len = int(np.percentile(lengths, 95))\n",
        "\n",
        "# ğŸ”¹ ìµœì¢… MAX_LEN ê·œì¹™:\n",
        "#    ìµœì†Œ 50í† í°ì€ ë³´ì¥, ìµœëŒ€ 400í† í°ê¹Œì§€ë§Œ í—ˆìš©, ê·¸ ì‚¬ì´ì—ì„œ 95í¼ì„¼íƒ€ì¼ ê°’ì„ ì‚¬ìš©\n",
        "#    â†’ ê³¼ë„í•œ íŒ¨ë”©/ì˜ë¦¼ì„ ì™„í™”í•˜ê³ , ê³„ì‚°ëŸ‰ê³¼ ì •ë³´ ë³´ì¡´ì˜ ê· í˜•ì„ ë§ì¶¤\n",
        "MAX_LEN = max(50, min(max_len, 400))\n",
        "\n",
        "# ğŸ”¹ ì„ íƒëœ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •ì„ ë¡œê·¸ë¡œ í™•ì¸\n",
        "print('95th percentile length:', max_len, '| Using MAX_LEN =', MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vocabulary & ì‹œí€€ìŠ¤ ë³€í™˜\n",
        "- ìì£¼ ë“±ì¥í•˜ëŠ” í† í°ìœ¼ë¡œ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
        "- `<pad>` / `<unk>` í† í° ì˜ˆì•½\n",
        "- í…ìŠ¤íŠ¸ -> ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ -> ê³ ì • ê¸¸ì´ íŒ¨ë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(token_lists: List[List[str]], min_freq: int = 3, max_size: int = 30000) -> Dict[str, int]:\n",
        "    # âœ… ì½”í¼ìŠ¤ ì „ì²´ í† í° ë¹ˆë„ ì§‘ê³„: ì´ì¤‘ ë£¨í”„ ì „ê°œ(comprehension)ë¡œ í‰íƒ„í™” í›„ Counter\n",
        "    counter = Counter(tok for tokens in token_lists for tok in tokens)\n",
        "    # âœ… ìƒìœ„ max_sizeê°œë§Œ ì¶”ì¶œ(ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ)\n",
        "    most_common = counter.most_common(max_size)\n",
        "    # âœ… íŠ¹ë³„ í† í° ì‚¬ì „ ì´ˆê¸°í™”: íŒ¨ë”©(<pad>=0), ë¯¸ë“±ë¡ì–´(<unk>=1) ì¸ë±ìŠ¤ ê³ ì •\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for word, freq in most_common:\n",
        "        # âœ… ìµœì†Œ ë“±ì¥ ë¹ˆë„ ë¯¸ë§Œ(min_freq) ë‹¨ì–´ëŠ” ì œì™¸\n",
        "        if freq < min_freq:\n",
        "            continue\n",
        "        # âœ… ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ ê¸¸ì´(=ë‹¤ìŒ ì¸ë±ìŠ¤)ë¡œ ë“±ë¡\n",
        "        vocab.setdefault(word, len(vocab))\n",
        "    return vocab\n",
        "\n",
        "\n",
        "# âœ… í•™ìŠµ ì½”í¼ìŠ¤ì—ì„œ ë‹¨ì–´ ì‚¬ì „ ìƒì„±: ë§¤ìš° ë“œë¬¸ ë‹¨ì–´ ì œì™¸(min_freq=5), ìƒí•œ 40,000\n",
        "vocab = build_vocab(train_tokens, min_freq=5, max_size=40000)\n",
        "print('Vocabulary size:', len(vocab))\n",
        "\n",
        "\n",
        "def tokens_to_indices(tokens: List[str], vocab: Dict[str, int], max_len: int) -> List[int]:\n",
        "    # âœ… ë¯¸ë“±ë¡ì–´ ì¸ë±ìŠ¤ ìºì‹±(ë¹ ë¥¸ ì ‘ê·¼)\n",
        "    unk = vocab['<unk>']\n",
        "    # âœ… í† í°â†’ì¸ë±ìŠ¤ ë§¤í•‘: vocabì— ì—†ìœ¼ë©´ <unk> ì¸ë±ìŠ¤ ì‚¬ìš©; ê·¸ë¦¬ê³  ìµœëŒ€ ê¸¸ì´ë¡œ ì˜ë¼ëƒ„(truncation)\n",
        "    idxs = [vocab.get(tok, unk) for tok in tokens][:max_len]\n",
        "    # âœ… íŒ¨ë”©: max_lenë³´ë‹¤ ì§§ìœ¼ë©´ <pad>(0)ë¡œ ì˜¤ë¥¸ìª½ íŒ¨ë”©í•˜ì—¬ ê³ ì • ê¸¸ì´ ì‹œí€€ìŠ¤ êµ¬ì„±\n",
        "    if len(idxs) < max_len:\n",
        "        idxs += [vocab['<pad>']] * (max_len - len(idxs))\n",
        "    return idxs\n",
        "\n",
        "\n",
        "# âœ… í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ í† í° ì‹œí€€ìŠ¤ë¥¼ ëª¨ë‘ ì •ìˆ˜ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜(ê¸¸ì´ MAX_LEN ê³ ì •)\n",
        "train_indices = [tokens_to_indices(t, vocab, MAX_LEN) for t in train_tokens]\n",
        "val_indices   = [tokens_to_indices(t, vocab, MAX_LEN) for t in val_tokens]\n",
        "test_indices  = [tokens_to_indices(t, vocab, MAX_LEN) for t in test_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ì„ë² ë”© ìƒì„± ìœ í‹¸ë¦¬í‹°\n",
        "Word2Vec / FastText í•™ìŠµ, GloVe ë¡œë”©ì„ í•¨ìˆ˜í™”í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EMBED_DIM = 100  # ğŸ”§ ì„ë² ë”© ë²¡í„° ì°¨ì› ìˆ˜(Word2Vec/FastText/GloVe ê³µí†µ ê¸°ë³¸ê°’). ëª¨ë¸Â·í–‰ë ¬ í¬ê¸°ë¥¼ ê²°ì •\n",
        "\n",
        "def train_word2vec(sentences: List[List[str]], vector_size: int = EMBED_DIM) -> Word2Vec:\n",
        "    # âœ… Word2Vec ì„ë² ë”© í•™ìŠµ í•¨ìˆ˜\n",
        "    # - sentences: í† í°í™”ëœ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸(ì½”í¼ìŠ¤)\n",
        "    # - vector_size: ë‹¨ì–´ ë²¡í„° ì°¨ì›(ê¸°ë³¸ EMBED_DIM)\n",
        "    # - window=5: ëª©í‘œ ë‹¨ì–´ ì¢Œìš°ë¡œ 5ê°œì˜ ë¬¸ë§¥ ë‹¨ì–´ ê³ ë ¤\n",
        "    # - min_count=5: ë“±ì¥ ë¹ˆë„ 5 ë¯¸ë§Œ ë‹¨ì–´ëŠ” ë¬´ì‹œ(ë…¸ì´ì¦ˆ ê°ì†Œ)\n",
        "    # - workers=4: ë©€í‹°ì½”ì–´ ë³‘ë ¬ í•™ìŠµ ìŠ¤ë ˆë“œ ìˆ˜\n",
        "    # - sg=1: Skip-gram(1) / CBOW(0). Skip-gramì€ í¬ê·€ ë‹¨ì–´ í•™ìŠµì— ìƒëŒ€ì ìœ¼ë¡œ ê°•í•¨\n",
        "    model = Word2Vec(sentences=sentences, vector_size=vector_size, window=5, min_count=5, workers=4, sg=1)\n",
        "    return model  # í•™ìŠµëœ Gensim Word2Vec ëª¨ë¸ ë°˜í™˜\n",
        "\n",
        "def train_fasttext(sentences: List[List[str]], vector_size: int = EMBED_DIM) -> FastText:\n",
        "    # âœ… FastText ì„ë² ë”© í•™ìŠµ í•¨ìˆ˜\n",
        "    # - FastTextëŠ” ì„œë¸Œì›Œë“œ(ë¬¸ì n-gram) ë‹¨ìœ„ë¡œ ì„ë² ë”©ì„ í•™ìŠµ â†’ í¬ê·€/ì‹ ì¡°ì–´ ì²˜ë¦¬ì— ìœ ë¦¬\n",
        "    # - íŒŒë¼ë¯¸í„° ì˜ë¯¸ëŠ” Word2Vecê³¼ ìœ ì‚¬(Window, min_count, workers ë“±)\n",
        "    model = FastText(sentences=sentences, vector_size=vector_size, window=5, min_count=5, workers=4)\n",
        "    return model  # í•™ìŠµëœ Gensim FastText ëª¨ë¸ ë°˜í™˜\n",
        "\n",
        "def load_glove(vector_size: int = EMBED_DIM):\n",
        "    # âœ… ì‚¬ì „í•™ìŠµ(pretrained) GloVe ì„ë² ë”© ë¡œë“œ\n",
        "    # - torchtextê°€ í™˜ê²½ì— ì„¤ì¹˜/í˜¸í™˜ë˜ì–´ ìˆì–´ì•¼ í•¨(TORCHTEXT_AVAILABLE í”Œë˜ê·¸)\n",
        "    if not TORCHTEXT_AVAILABLE:\n",
        "        # ì‚¬ìš© ë¶ˆê°€ ì‹œ ëª…í™•í•œ ì˜ˆì™¸ ë©”ì‹œì§€ë¡œ ìˆ˜ë™ ë¡œë“œ ìœ ë„\n",
        "        raise RuntimeError('torchtextë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GloVe ë²¡í„° íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œí•˜ì„¸ìš”.')\n",
        "    # name='6B': 6 Billion í† í° ì½”í¼ìŠ¤ì—ì„œ í•™ìŠµëœ GloVe ì„¸íŠ¸ ì‚¬ìš©\n",
        "    # dim=vector_size: ì„ë² ë”© ì°¨ì›(50/100/200/300 ë“± ì¤‘ ì œê³µ)\n",
        "    glove = GloVe(name='6B', dim=vector_size)\n",
        "    return glove  # torchtext.vocab.Vectorsì™€ ìœ ì‚¬í•œ ê°ì²´(ë‹¨ì–´â†’ì¸ë±ìŠ¤, ë²¡í„° í…ì„œ í¬í•¨)\n",
        "\n",
        "def build_embedding_matrix(vocab: Dict[str, int], embeddings, kind: str) -> np.ndarray:\n",
        "    # âœ… ì–´íœ˜ì§‘(vocab)ì˜ ê° ë‹¨ì–´ ì¸ë±ìŠ¤ì— ë§ì¶° \"ì„ë² ë”© í–‰ë ¬\"ì„ ìƒì„±\n",
        "    # í–‰ë ¬ í¬ê¸°: (vocab í¬ê¸°, EMBED_DIM). ê¸°ë³¸ì€ ì‘ì€ ì •ê·œë¶„í¬ ë‚œìˆ˜ë¡œ ì´ˆê¸°í™”\n",
        "    matrix = np.random.normal(scale=0.02, size=(len(vocab), EMBED_DIM))\n",
        "    # <pad> í† í°(íŒ¨ë”©)ì€ ë°˜ë“œì‹œ ì˜ë²¡í„°(0)ë¡œ ê³ ì • â†’ íŒ¨ë”©ì´ ëª¨ë¸ì— ì‹ í˜¸ë¥¼ ì£¼ì§€ ì•Šê²Œ í•¨\n",
        "    matrix[vocab['<pad>']] = np.zeros(EMBED_DIM)\n",
        "\n",
        "    # ğŸ” ì–´íœ˜ì§‘ì„ ìˆœíšŒí•˜ë©°, í•´ë‹¹ ë‹¨ì–´ì˜ ì‚¬ì „í•™ìŠµ/í•™ìŠµëœ ì„ë² ë”© ë²¡í„°ë¥¼ ì°¾ì•„ í–‰ë ¬ì— ë§¤í•‘\n",
        "    for word, idx in vocab.items():\n",
        "        if word in ('<pad>', '<unk>'):\n",
        "            # íŒ¨ë”©/ë¯¸ë“±ë¡ í† í°ì€ ìŠ¤í‚µ(<pad>ëŠ” ìœ„ì—ì„œ 0ìœ¼ë¡œ ì„¤ì •, <unk>ëŠ” ì´ˆê¸° ë‚œìˆ˜ ìœ ì§€)\n",
        "            continue\n",
        "        vector = None\n",
        "\n",
        "        # ğŸ”¹ Gensim Word2Vec/FastText: embeddings.wvì— ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ë²¡í„° ì·¨ë“\n",
        "        if kind == 'word2vec' and word in embeddings.wv:\n",
        "            vector = embeddings.wv[word]\n",
        "        elif kind == 'fasttext' and word in embeddings.wv:\n",
        "            vector = embeddings.wv[word]\n",
        "\n",
        "        # ğŸ”¹ torchtext GloVe: stoi(ë‹¨ì–´â†’ì¸ë±ìŠ¤)ë¡œ ìœ„ì¹˜ ì¡°íšŒ í›„ vectors[ì¸ë±ìŠ¤]ì—ì„œ ë²¡í„° ì·¨ë“\n",
        "        elif kind == 'glove':\n",
        "            if hasattr(embeddings, 'vectors'):       # ì•ˆì „ì„± ì²´í¬(í™˜ê²½ë³„ êµ¬í˜„ ì°¨ì´)\n",
        "                token_index = embeddings.stoi.get(word)\n",
        "                if token_index is not None:\n",
        "                    vector = embeddings.vectors[token_index].numpy()\n",
        "\n",
        "        # ğŸ“¥ ë²¡í„°ë¥¼ ì°¾ì•˜ìœ¼ë©´ í•´ë‹¹ ë‹¨ì–´ ì¸ë±ìŠ¤ ìœ„ì¹˜ì— ë³µì‚¬(ì´ˆê¸° ë‚œìˆ˜ê°’ì„ ëŒ€ì²´)\n",
        "        if vector is not None:\n",
        "            matrix[idx] = vector\n",
        "\n",
        "    # âœ… float32ë¡œ ìºìŠ¤íŒ…(ë©”ëª¨ë¦¬ íš¨ìœ¨, PyTorch í˜¸í™˜ì„±)\n",
        "    return matrix.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PyTorch Dataset / DataLoader\n",
        "ì‹œí€€ìŠ¤ì™€ ë ˆì´ë¸”ì„ í…ì„œë¡œ ê°ì‹¸ëŠ” ê°„ë‹¨í•œ Datasetì„ ì •ì˜í•˜ê³ , DataLoaderë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    # âœ… PyTorchì˜ ê¸°ë³¸ Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ \"ë‰´ìŠ¤ ë¶„ë¥˜ìš© ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹\" ì •ì˜\n",
        "    # Datasetì€ DataLoaderê°€ ë°ì´í„°ë¥¼ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ êº¼ë‚¼ ìˆ˜ ìˆë„ë¡ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "    def __init__(self, sequences: List[List[int]], labels: List[int]):\n",
        "        # ğŸ”¹ sequences: ê° ë¬¸ì„œê°€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í† í° ì‹œí€€ìŠ¤ (shape: [num_samples, max_len])\n",
        "        # ğŸ”¹ labels: ê° ë¬¸ì„œì˜ ì •ë‹µ ë¼ë²¨(ì •ìˆ˜í˜• ì¹´í…Œê³ ë¦¬ ID)\n",
        "        # ğŸ”¹ torch.tensor(...) : ë„˜íŒŒì´ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•´ì•¼ GPU ì—°ì‚° ë° DataLoader í˜¸í™˜ ê°€ëŠ¥\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        # âœ… ì „ì²´ ë°ì´í„°ì˜ ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ë°˜í™˜\n",
        "        # DataLoaderê°€ ëª‡ ê°œ ë°°ì¹˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ ê³„ì‚°í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # âœ… í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì¸ë±ìŠ¤ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë©”ì„œë“œ\n",
        "        # DataLoaderëŠ” ë°°ì¹˜ë¥¼ êµ¬ì„±í•  ë•Œ ë‚´ë¶€ì ìœ¼ë¡œ __getitem__ì„ ë°˜ë³µ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
        "        # idxë²ˆì§¸ ë¬¸ì¥ì˜ í† í° ì‹œí€€ìŠ¤ì™€ ì •ë‹µ ë¼ë²¨ì„ (X, y) í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# âœ… NewsDataset ê°ì²´ ìƒì„±\n",
        "# ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ ë¬¸ì¥ê³¼ ë¼ë²¨ì„ ê°ê° í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "train_dataset = NewsDataset(train_indices, train_labels)\n",
        "val_dataset = NewsDataset(val_indices, val_labels)\n",
        "test_dataset = NewsDataset(test_indices, test_labels)\n",
        "\n",
        "# âœ… DataLoader: Datasetì„ ê°ì‹¸ì„œ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ìœ í‹¸ë¦¬í‹°\n",
        "# - batch_size: í•œ ë²ˆì— ëª¨ë¸ì— ê³µê¸‰í•  ìƒ˜í”Œ ìˆ˜\n",
        "# - shuffle=True: í•™ìŠµ ì‹œ ë°ì´í„° ìˆœì„œë¥¼ ì„ì–´ì„œ ê³¼ì í•© ë°©ì§€\n",
        "# - DataLoaderëŠ” __getitem__ì„ ìë™ìœ¼ë¡œ ë°˜ë³µ í˜¸ì¶œí•˜ì—¬ ë°°ì¹˜ í…ì„œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # í•™ìŠµìš© ë°ì´í„° (ëœë¤ ì…”í”Œ)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)                    # ê²€ì¦ìš© ë°ì´í„° (ìˆœì„œ ìœ ì§€)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)                  # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° (ìˆœì„œ ìœ ì§€)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RNN ë¶„ë¥˜ ëª¨ë¸\n",
        "- ì„ë² ë”© ë ˆì´ì–´(ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ê°€ëŠ¥)\n",
        "- ì–‘ë°©í–¥ LSTM\n",
        "- í’€ë§ + FC ë¶„ë¥˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassifierRNN(nn.Module):\n",
        "    # âœ… PyTorch ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ (RNN-LSTM êµ¬ì¡°)\n",
        "    # - ì…ë ¥: ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤(batch_size Ã— seq_len)\n",
        "    # - ì¶œë ¥: í´ë˜ìŠ¤ë³„ ë¡œì§“(logit) ê°’(batch_size Ã— num_classes)\n",
        "\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int,\n",
        "                 num_classes: int, pretrained_weights=None, freeze_embedding=False):\n",
        "        # ------------------------------------------------------------\n",
        "        # vocab_size: ì „ì²´ ë‹¨ì–´ ì‚¬ì „ í¬ê¸° (Embedding lookup table í–‰ ê°œìˆ˜)\n",
        "        # embed_dim: ì„ë² ë”© ë²¡í„° ì°¨ì› ìˆ˜\n",
        "        # hidden_dim: LSTMì˜ ì€ë‹‰ ìƒíƒœ(hidden state) í¬ê¸°\n",
        "        # num_classes: ì˜ˆì¸¡í•  í´ë˜ìŠ¤ ê°œìˆ˜ (ì¶œë ¥ ì°¨ì›)\n",
        "        # pretrained_weights: ì‚¬ì „í•™ìŠµëœ ì„ë² ë”© í–‰ë ¬ (Word2Vec/FastText/GloVe)\n",
        "        # freeze_embedding: Trueë©´ ì„ë² ë”©ì„ í•™ìŠµì‹œí‚¤ì§€ ì•Šê³  ê³ ì •(freeze)\n",
        "        # ------------------------------------------------------------\n",
        "        super().__init__()  # nn.Module ì´ˆê¸°í™” (ìƒìœ„ í´ë˜ìŠ¤ ìƒì„±ì í˜¸ì¶œ)\n",
        "\n",
        "        # ğŸ”¹ ì„ë² ë”© ë ˆì´ì–´: ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë³€í™˜\n",
        "        # padding_idx=0 â†’ '<pad>' í† í°ì€ í•™ìŠµë˜ì§€ ì•Šê³  í•­ìƒ 0ë²¡í„° ìœ ì§€\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # ğŸ”¹ ì‚¬ì „í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤ë©´ í•´ë‹¹ í–‰ë ¬ì„ ë³µì‚¬\n",
        "        if pretrained_weights is not None:\n",
        "            # pretrained_weightsëŠ” numpy ë°°ì—´ í˜•íƒœì´ë¯€ë¡œ torch.tensorë¡œ ë³€í™˜ í›„ ë³µì‚¬\n",
        "            self.embedding.weight.data.copy_(torch.tensor(pretrained_weights))\n",
        "\n",
        "        # ğŸ”¹ freeze ì„¤ì •: False â†’ í•™ìŠµ ê°€ëŠ¥ / True â†’ ê°€ì¤‘ì¹˜ ê³ ì •\n",
        "        # requires_grad=Falseë©´ ì—­ì „íŒŒ(backpropagation) ì‹œ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ\n",
        "        self.embedding.weight.requires_grad = not freeze_embedding\n",
        "\n",
        "        # ğŸ”¹ ì–‘ë°©í–¥ LSTM ì¸ì½”ë”\n",
        "        # - embed_dim: ì…ë ¥ ì°¨ì› (ë‹¨ì–´ ë²¡í„° í¬ê¸°)\n",
        "        # - hidden_dim: ì€ë‹‰ ìƒíƒœ í¬ê¸°\n",
        "        # - batch_first=True â†’ ì…ë ¥ í˜•íƒœë¥¼ (batch, seq, feature)ë¡œ ì§€ì •\n",
        "        # - bidirectional=True â†’ ì–‘ë°©í–¥ LSTM (ì •ë°©í–¥ + ì—­ë°©í–¥) ì‚¬ìš©\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # ğŸ”¹ Dropout: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë¶€ ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ 30% ë”\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # ğŸ”¹ ë¶„ë¥˜ê¸°(Linear Layer): ì€ë‹‰ ìƒíƒœì˜ ì¶œë ¥ì„ í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ ë§¤í•‘\n",
        "        # ì–‘ë°©í–¥ LSTMì˜ ì¶œë ¥ì€ hidden_dim * 2ì´ë¯€ë¡œ ì…ë ¥ ì°¨ì›ì„ 2ë°°ë¡œ ì§€ì •\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # âœ… ìˆœì „íŒŒ(forward) ì—°ì‚° ì •ì˜\n",
        "        # ì…ë ¥ x: (batch_size, seq_len) í˜•íƒœì˜ ë‹¨ì–´ ì¸ë±ìŠ¤ í…ì„œ\n",
        "\n",
        "        # 1ï¸âƒ£ ë‹¨ì–´ ì¸ë±ìŠ¤ â†’ ì„ë² ë”© ë²¡í„° ë³€í™˜\n",
        "        #   ê²°ê³¼: (batch_size, seq_len, embed_dim)\n",
        "        embeds = self.embedding(x)\n",
        "\n",
        "        # 2ï¸âƒ£ LSTM ì¸ì½”ë” í†µê³¼\n",
        "        #   outputs: ê° ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ ì „ì²´ ì‹œí€€ìŠ¤ (batch, seq_len, hidden_dim*2)\n",
        "        #   _: (h_n, c_n) ì€ë‹‰/ì…€ ìƒíƒœ (ì—¬ê¸°ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "        outputs, _ = self.encoder(embeds)\n",
        "\n",
        "        # 3ï¸âƒ£ ì‹œí€€ìŠ¤ ì°¨ì› í‰ê·  í’€ë§\n",
        "        #   ëª¨ë“  íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœë¥¼ í‰ê· ë‚´ì–´ ë¬¸ì¥ ì „ì²´ í‘œí˜„ì„ ìƒì„±\n",
        "        #   (batch_size, hidden_dim*2)\n",
        "        pooled = outputs.mean(dim=1)\n",
        "\n",
        "        # 4ï¸âƒ£ Dropout â†’ Linear ë¶„ë¥˜ê¸° í†µê³¼ â†’ ë¡œì§“(logit) ì¶œë ¥\n",
        "        #   logits: (batch_size, num_classes)\n",
        "        logits = self.classifier(self.dropout(pooled))\n",
        "\n",
        "        # 5ï¸âƒ£ Softmax ì´ì „ ë‹¨ê³„ì˜ ë¡œì§“ ë°˜í™˜\n",
        "        #   (ì†ì‹¤ ê³„ì‚° ì‹œ CrossEntropyLossê°€ ë‚´ë¶€ì—ì„œ softmax ì²˜ë¦¬)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. í•™ìŠµ / ê²€ì¦ ë£¨í”„ Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_epoch(model, loader, criterion, optimizer=None):\n",
        "    # âœ… í•œ epoch(ë°ì´í„° ì „ì²´ 1íšŒ ë°˜ë³µ) ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµ ë˜ëŠ” í‰ê°€í•˜ëŠ” í•¨ìˆ˜\n",
        "    # model: í•™ìŠµì‹œí‚¬ ì‹ ê²½ë§ ëª¨ë¸\n",
        "    # loader: DataLoader (train/val/test ì¤‘ í•˜ë‚˜)\n",
        "    # criterion: ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: CrossEntropyLoss)\n",
        "    # optimizer: ì˜µí‹°ë§ˆì´ì € (ì—†ìœ¼ë©´ í‰ê°€ ëª¨ë“œë¡œ ì‘ë™)\n",
        "\n",
        "    # ğŸ”¹ í•™ìŠµ ëª¨ë“œ ì—¬ë¶€ ê²°ì • (optimizerê°€ ìˆìœ¼ë©´ í•™ìŠµ, ì—†ìœ¼ë©´ í‰ê°€)\n",
        "    is_train = optimizer is not None\n",
        "\n",
        "    # ğŸ”¹ ëª¨ë¸ ëª¨ë“œ ì„¤ì •: í•™ìŠµ(model.train()) / í‰ê°€(model.eval())\n",
        "    # train(): Dropout, BatchNorm í™œì„± / eval(): í‰ê°€ ì‹œ ê³ ì •\n",
        "    model.train() if is_train else model.eval()\n",
        "\n",
        "    # ğŸ”¹ ì „ì²´ ì†ì‹¤ ëˆ„ì ê°’, ì˜ˆì¸¡/ì •ë‹µ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # ğŸ”¹ DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ë°˜ë³µ\n",
        "    for inputs, targets in loader:\n",
        "        # inputs: (batch_size, seq_len) í˜•íƒœì˜ ë‹¨ì–´ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤\n",
        "        # targets: (batch_size,) í˜•íƒœì˜ ì •ë‹µ í´ë˜ìŠ¤ ì¸ë±ìŠ¤\n",
        "        # GPU/CPU ì¥ì¹˜ë¡œ ì´ë™\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # ğŸ”¹ í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ gradient ê³„ì‚° í™œì„±í™”, í‰ê°€ ì‹œ ë¹„í™œì„±í™”(no_gradì™€ ë™ì¼ íš¨ê³¼)\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            # â‘  ìˆœì „íŒŒ(forward)\n",
        "            logits = model(inputs)              # (batch_size, num_classes)\n",
        "            loss = criterion(logits, targets)   # CrossEntropyLoss ê³„ì‚°\n",
        "\n",
        "            # â‘¡ í•™ìŠµ ë‹¨ê³„ì¼ ê²½ìš°: ì—­ì „íŒŒ + ìµœì í™”\n",
        "            if is_train:\n",
        "                optimizer.zero_grad()                 # ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
        "                loss.backward()                       # ì†ì‹¤ì— ëŒ€í•´ ì—­ì „íŒŒ ìˆ˜í–‰\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # ğŸ”§ Gradient í­ì£¼ ë°©ì§€ (max norm=1.0)\n",
        "                optimizer.step()                       # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
        "\n",
        "        # ğŸ”¹ ì†ì‹¤ ëˆ„ì  (í‰ê·  ëŒ€ì‹  ì´í•© * ë°°ì¹˜ í¬ê¸° â†’ ë‚˜ì¤‘ì— ì „ì²´ í‰ê·  ê³„ì‚°)\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # ğŸ”¹ ì˜ˆì¸¡ê°’ ê³„ì‚° (ê°€ì¥ ë†’ì€ logitì˜ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        # ğŸ”¹ CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (ë‚˜ì¤‘ì— ì „ì²´ ì—°ê²°)\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(targets.cpu())\n",
        "\n",
        "    # ğŸ”¹ í‰ê·  ì†ì‹¤ ê³„ì‚° (ì´ ì†ì‹¤ Ã· ì „ì²´ ìƒ˜í”Œ ìˆ˜)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    # ğŸ”¹ Accuracy ê³„ì‚°\n",
        "    # torch.catìœ¼ë¡œ ë°°ì¹˜ë³„ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ í›„ sklearnì˜ accuracy_scoreë¡œ ê³„ì‚°\n",
        "    accuracy = accuracy_score(torch.cat(all_labels), torch.cat(all_preds))\n",
        "\n",
        "    # ğŸ”¹ í‰ê·  ì†ì‹¤ê³¼ ì •í™•ë„ ë°˜í™˜\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    # âœ… ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
        "    # loader: ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© DataLoader\n",
        "    # ì¶œë ¥: classification_report(precision, recall, f1-score ë“±)\n",
        "\n",
        "    model.eval()  # ğŸ”¹ í‰ê°€ ëª¨ë“œ (Dropout ë¹„í™œì„±í™”)\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # ğŸ”¹ no_grad(): í‰ê°€ ì‹œ gradient ê³„ì‚° ë¹„í™œì„±í™” â†’ ë©”ëª¨ë¦¬ ì ˆì•½ & ì†ë„ í–¥ìƒ\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (targetsì€ ë‚˜ì¤‘ì— CPUì—ì„œ ë¹„êµ)\n",
        "            inputs = inputs.to(device)\n",
        "            # ìˆœì „íŒŒ\n",
        "            logits = model(inputs)\n",
        "            # ê° ìƒ˜í”Œë³„ ì˜ˆì¸¡ í´ë˜ìŠ¤ (argmax)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # ê²°ê³¼ ì €ì¥\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(targets)\n",
        "\n",
        "    # ğŸ”¹ ì „ì²´ ì˜ˆì¸¡/ì •ë‹µ ë°°ì—´ ì—°ê²°\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "\n",
        "    # ğŸ”¹ ì •ë°€ë„(Precision), ì¬í˜„ìœ¨(Recall), F1-score ë“± ìƒì„¸ ì§€í‘œ ì¶œë ¥\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ì„ë² ë”©ë³„ ì‹¤í—˜ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(embedding_type: str, epochs: int = 3, freeze_embedding: bool = False):\n",
        "    # âœ… í•˜ë‚˜ì˜ ì„ë² ë”© ë°©ì‹(Word2Vec/FastText/GloVe)ì„ ì„ íƒí•´\n",
        "    #    ë²¡í„° í•™ìŠµ/ë¡œë”© â†’ ì„ë² ë”© í–‰ë ¬ êµ¬ì„± â†’ RNN ë¶„ë¥˜ê¸° í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ê¹Œì§€\n",
        "    #    \"í•œ ë²ˆì˜ ì‹¤í—˜\" ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜\n",
        "    # embedding_type: 'word2vec' | 'fasttext' | 'glove'\n",
        "    # epochs: í•™ìŠµ ì—í­ ìˆ˜\n",
        "    # freeze_embedding: ì„ë² ë”© ê³ ì • ì—¬ë¶€(Trueë©´ í•™ìŠµ ì¤‘ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì•ˆ í•¨)\n",
        "\n",
        "    if embedding_type == 'word2vec':\n",
        "        # ğŸ”¹ í•™ìŠµ ì½”í¼ìŠ¤ë¡œ Word2Vec ì„ë² ë”©ì„ ì§ì ‘ í•™ìŠµ\n",
        "        vectors = train_word2vec(train_tokens)\n",
        "    elif embedding_type == 'fasttext':\n",
        "        # ğŸ”¹ í•™ìŠµ ì½”í¼ìŠ¤ë¡œ FastText ì„ë² ë”©ì„ ì§ì ‘ í•™ìŠµ(ì„œë¸Œì›Œë“œ ê¸°ë°˜)\n",
        "        vectors = train_fasttext(train_tokens)\n",
        "    elif embedding_type == 'glove':\n",
        "        # ğŸ”¹ ì‚¬ì „í•™ìŠµëœ(pretrained) GloVe ë²¡í„° ë¡œë“œ (torchtext í•„ìš”)\n",
        "        vectors = load_glove()\n",
        "    else:\n",
        "        # ğŸ”¹ ì•ˆì „ì¥ì¹˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ìì—´ ì…ë ¥ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨\n",
        "        raise ValueError('ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤.')\n",
        "\n",
        "    # ğŸ”¹ vocab(ë‹¨ì–´â†’ì¸ë±ìŠ¤)ì— ë§ì¶˜ ì„ë² ë”© í–‰ë ¬ ìƒì„±\n",
        "    #    - shape: (vocab_size, EMBED_DIM)\n",
        "    #    - <pad>ëŠ” 0ë²¡í„°, <unk>ëŠ” ì´ˆê¸° ë‚œìˆ˜ ìœ ì§€\n",
        "    #    - vectorsì—ì„œ í•´ë‹¹ ë‹¨ì–´ ë²¡í„°ë¥¼ ì°¾ìœ¼ë©´ ë³µì‚¬, ì—†ìœ¼ë©´ ì´ˆê¸°ê°’ ìœ ì§€\n",
        "    embedding_matrix = build_embedding_matrix(vocab, vectors, embedding_type)\n",
        "\n",
        "    # ğŸ”¹ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸(RNN: BiLSTM) êµ¬ì„±\n",
        "    #    - vocab_size: ì„ë² ë”© í…Œì´ë¸” í¬ê¸°\n",
        "    #    - embed_dim: ì„ë² ë”© ì°¨ì›(EMBED_DIMê³¼ ì¼ì¹˜)\n",
        "    #    - hidden_dim: LSTM ì€ë‹‰ ìƒíƒœ í¬ê¸°(ì–‘ë°©í–¥ì´ë¼ ìµœì¢… ë¶„ë¥˜ ì…ë ¥ì€ hidden_dim*2)\n",
        "    #    - num_classes: ë ˆì´ë¸” ìˆ˜(20 Newsgroups = 20)\n",
        "    #    - pretrained_weights: ìœ„ì—ì„œ ë§Œë“  ì„ë² ë”© í–‰ë ¬ë¡œ ì´ˆê¸°í™”\n",
        "    #    - freeze_embedding: Trueë©´ ì„ë² ë”© ê³ ì •(ë¯¸ì„¸ì¡°ì • ì—†ìŒ)\n",
        "    model = TextClassifierRNN(\n",
        "        vocab_size=len(vocab),\n",
        "        embed_dim=EMBED_DIM,\n",
        "        hidden_dim=128,\n",
        "        num_classes=len(label_names),\n",
        "        pretrained_weights=embedding_matrix,\n",
        "        freeze_embedding=freeze_embedding\n",
        "    ).to(device)  # ğŸ”¹ ëª¨ë¸ì„ GPU/CPU ì¥ì¹˜ë¡œ ì´ë™\n",
        "\n",
        "    # ğŸ”¹ ì†ì‹¤ í•¨ìˆ˜: ë‹¤ì¤‘ë¶„ë¥˜ìš© CrossEntropyLoss(ë‚´ë¶€ì ìœ¼ë¡œ softmax í¬í•¨)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # ğŸ”¹ ì˜µí‹°ë§ˆì´ì €: Adam (ê¸°ë³¸ lr=1e-3)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    history = []  # ğŸ”¹ ì—í­ë³„ ì„±ëŠ¥ ë¡œê·¸ë¥¼ ê¸°ë¡(ì‹œê°í™”/ë¶„ì„ìš©)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ğŸ” í•™ìŠµ ì—í­ ìˆ˜í–‰: train_loaderì—ì„œ ì†ì‹¤/ì •í™•ë„ ê³„ì‚° + ì—­ì „íŒŒ/ì—…ë°ì´íŠ¸\n",
        "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
        "        # ğŸ” ê²€ì¦ ì—í­ ìˆ˜í–‰: val_loaderì—ì„œ ì†ì‹¤/ì •í™•ë„ ê³„ì‚°(ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìŒ)\n",
        "        val_loss, val_acc = run_epoch(model, val_loader, criterion)\n",
        "\n",
        "        # ğŸ”¹ ë¡œê¹…: ë‚˜ì¤‘ì— ìµœê³  ì„±ëŠ¥, í•™ìŠµ ê³¡ì„  ì‹œê°í™” ë“±ì— ì‚¬ìš©\n",
        "        history.append({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "        })\n",
        "\n",
        "        # ğŸ”¹ ì§„í–‰ ìƒí™© ì¶œë ¥(ì„ë² ë”© íƒ€ì…/ì—í­/ì •í™•ë„)\n",
        "        print(f\"[{embedding_type}] Epoch {epoch}: train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ğŸ”¹ ê²€ì¦ ì •í™•ë„ì˜ ìµœê³ ì¹˜ ìš”ì•½ ì¶œë ¥\n",
        "    print(f\"[{embedding_type}] ê²€ì¦ ì„±ëŠ¥ ìµœê³ ì¹˜: {max(h['val_acc'] for h in history):.4f}\")\n",
        "\n",
        "    # ğŸ”¹ ìµœì¢… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì¶œë ¥(precision/recall/f1 ë“± í´ë˜ìŠ¤ë³„ ì§€í‘œ)\n",
        "    print('í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸:')\n",
        "    evaluate_model(model, test_loader)\n",
        "\n",
        "    # ğŸ”¹ í•™ìŠµ ê³¡ì„ (ì†ì‹¤/ì •í™•ë„) ë“± ì‚¬í›„ ë¶„ì„ì„ ìœ„í•´ íˆìŠ¤í† ë¦¬ ë°˜í™˜\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. ì‹¤í–‰ ê°€ì´ë“œ\n",
        "ì•„ë˜ ì…€ì—ì„œ ì›í•˜ëŠ” ì„ë² ë”© ìœ í˜•ì„ ì„ íƒí•´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•™ìŠµ ì‹œê°„ ë¬¸ì œë¡œ í•˜ë‚˜ì”© ì‹¤í–‰ ê¶Œì¥)\n",
        "```python\n",
        "word2vec_history = run_experiment('word2vec', epochs=5)\n",
        "fasttext_history = run_experiment('fasttext', epochs=5)\n",
        "# GloVeëŠ” torchtextê°€ ë²¡í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "# glove_history = run_experiment('glove', epochs=5, freeze_embedding=True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2253a0d",
      "metadata": {},
      "source": [
        "## 11. ê°œë³„ ì‹¤í—˜ ì‹¤í–‰ ì…€\n",
        "ê° ì„ë² ë”©ì„ **ìˆœì°¨ì ìœ¼ë¡œ** ì‹¤í–‰í•´ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ë¨¼ì € ê³µìš© ì´ë ¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”í•œ ë’¤, Word2Vec â†’ FastText â†’ GloVe ìˆœìœ¼ë¡œ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b54da39",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_histories = {}\n",
        "print('ì‹¤í—˜ ì´ë ¥ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d43f7d",
      "metadata": {},
      "source": [
        "### 11-1. Word2Vec + LSTM\n",
        "ê°€ì¥ ë¨¼ì € Word2Vec ì„ë² ë”©ì„ í•™ìŠµí•´ LSTM ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267bc0ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=== Word2Vec ì‹¤í—˜ ì‹œì‘ ===')\n",
        "word2vec_history = run_experiment('word2vec', epochs=5)\n",
        "experiment_histories['word2vec'] = word2vec_history\n",
        "print('Word2Vec ì‹¤í—˜ ì™„ë£Œ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e52d7b4",
      "metadata": {},
      "source": [
        "### 11-2. FastText + LSTM (í•„ìš” ì‹œ GRUë¡œ í™•ì¥ ê°€ëŠ¥)\n",
        "FastText ì„ë² ë”©ì„ ë™ì¼ êµ¬ì¡°ì— ì ìš©í•´ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d548ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=== FastText ì‹¤í—˜ ì‹œì‘ ===')\n",
        "fasttext_history = run_experiment('fasttext', epochs=5)\n",
        "experiment_histories['fasttext'] = fasttext_history\n",
        "print('FastText ì‹¤í—˜ ì™„ë£Œ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67564ddf",
      "metadata": {},
      "source": [
        "### 11-3. GloVe + LSTM\n",
        "TorchTextê°€ ì œê³µí•˜ëŠ” ì‚¬ì „í•™ìŠµ GloVe ë²¡í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ë•Œ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160dbd84",
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCHTEXT_AVAILABLE:\n",
        "    print('=== GloVe ì‹¤í—˜ ì‹œì‘ ===')\n",
        "    glove_history = run_experiment('glove', epochs=5, freeze_embedding=True)\n",
        "    experiment_histories['glove'] = glove_history\n",
        "    print('GloVe ì‹¤í—˜ ì™„ë£Œ')\n",
        "else:\n",
        "    print('torchtextê°€ ì„¤ì¹˜/ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì•„ GloVe ì‹¤í—˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”\n",
        "ì—¬ëŸ¬ ì„ë² ë”©ì˜ `val_acc` ìµœê³ ê°’ì„ ëª¨ì•„ ë¹„êµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for name, history in experiment_histories.items():\n",
        "    best = max(history, key=lambda h: h['val_acc'])\n",
        "    rows.append({\n",
        "        'embedding': name,\n",
        "        'best_epoch': best['epoch'],\n",
        "        'best_val_acc': best['val_acc'],\n",
        "        'train_acc_at_best': best['train_acc']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "display(results_df.sort_values('best_val_acc', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. ì¶”ê°€ ë¶„ì„ & ë©”ëª¨\n",
        "- `classification_report` ì¶œë ¥ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
        "- ë” ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ë°ì´í„° ì •ì œ ê°•í™”, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •, GRU/ì–´í…ì…˜ ì¶”ê°€ ë“±ì„ ì‹¤í—˜í•´ ë³´ì„¸ìš”.\n",
        "- ì•„ë˜ ë§ˆí¬ë‹¤ìš´ ì…€ì— ì„±ëŠ¥ ë¹„êµ ë° ì¸ì‚¬ì´íŠ¸ë¥¼ ì •ë¦¬í•´ ì œì¶œ íŒŒì¼ì— í¬í•¨í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ê²°ê³¼ ì •ë¦¬ ì˜ˆì‹œ**\n",
        "- Word2Vec: Best Val Acc = ... (epoch ...). ì£¼ìš” ì˜¤ì°¨ íŒ¨í„´ì€ ...\n",
        "- FastText: ...\n",
        "- GloVe: ...\n",
        "\n",
        "í–¥í›„ ê°œì„  ì•„ì´ë””ì–´: ..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai5_project1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
