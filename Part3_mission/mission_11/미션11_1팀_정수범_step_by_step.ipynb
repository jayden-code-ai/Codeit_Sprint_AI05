{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미션 11 단계별 실습 노트\n",
    "\n",
    "이 노트는 **데이터 이해 → 간단한 EDA → 전처리 → 단어 사전 만들기 → Seq2Seq 모델 학습** 순서로 차근차근 진행합니다. 각 단계에서 왜 그렇게 하는지 충분히 설명을 달았으니 그대로 따라가면서 빈칸을 스스로 채워 보는 연습으로 활용하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 진행 순서\n",
    "\n",
    "1. 데이터 파일 읽어 구조와 용량을 직접 확인하기\n",
    "2. 문장 길이, 토큰 빈도 등을 시각화로 살펴보기 (EDA)\n",
    "3. 규칙 기반 토크나이징과 단어 사전(Vocabulary) 구성하기\n",
    "4. PyTorch `Dataset`/`DataLoader`로 배치 만들기\n",
    "5. 간단한 Seq2Seq(Encoder-Decoder) 모델 학습 및 검증하기\n",
    "6. 학습된 모델로 임의 문장 번역 테스트하기\n",
    "\n",
    "필요에 따라 각 단계에서 파라미터를 조정하고, 궁금한 지점이 있으면 바로 코드 셀을 추가해 실험해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7d9b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "[CONFIG]\n",
      "- data_dir: Part3_mission_11\n",
      "- train_file: /mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_train_set.json\n",
      "- valid_file: /mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_valid_set.json\n",
      "- eda_sample: 10000\n",
      "- train_limit: 20000\n",
      "- valid_limit: 2000\n",
      "- min_freq: 2\n",
      "- max_length: 40\n",
      "- batch_size: 64\n",
      "- embedding_dim: 128\n",
      "- hidden_dim: 256\n",
      "- teacher_forcing: 0.5\n",
      "- learning_rate: 0.001\n",
      "- epochs: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': Path('Part3_mission_11'),\n",
    "    'train_file': '/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_train_set.json',\n",
    "    'valid_file': '/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_valid_set.json',\n",
    "    'eda_sample': 10000,\n",
    "    'train_limit': 20000,\n",
    "    'valid_limit': 2000,\n",
    "    'min_freq': 2,\n",
    "    'max_length': 40,\n",
    "    'batch_size': 64,\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'teacher_forcing': 0.5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'epochs': 5,\n",
    "}\n",
    "print(\"\\n[CONFIG]\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'- {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d859031",
   "metadata": {},
   "source": [
    "## 1. 데이터 파일 확인하기\n",
    "\n",
    "원본 JSON 파일은 `{\"data\": [{\"ko\": ..., \"mt\": ...}, ...]}` 형태입니다. 아래 함수로 파일을 읽고, EDA에 쓸 만큼만 무작위 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fc7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA용 학습 데이터 샘플 수: 10000\n",
      "예시 3건:\n",
      "KO: 그리고 더블 크립 제품이 하자 없는 배송 부탁드립니다.\n",
      "EN: Also, please ensure that the double creep product is delivered without defects.\n",
      "------------------------------\n",
      "KO: 날씨 보호 테크 시스템, 특허받은 용접 모서리 및 반전된 솔기가 물을 막아줍니다.\n",
      "EN: Weather protection tech system, patented welded edges and inverted seams keep water out.\n",
      "------------------------------\n",
      "KO: 무엇을 도와드릴까요?\n",
      "EN: How may I help you?\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def load_json_pairs(path: Path, limit: int = None, seed: int = SEED) -> List[Dict[str, str]]:\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        data = json.load(f)['data']\n",
    "    if limit is not None and limit < len(data):\n",
    "        rng = random.Random(seed)\n",
    "        idx = rng.sample(range(len(data)), limit)\n",
    "        data = [data[i] for i in idx]\n",
    "    return data\n",
    "\n",
    "\n",
    "data_dir = CONFIG['data_dir']\n",
    "raw_train = load_json_pairs(data_dir / CONFIG['train_file'], limit=CONFIG['eda_sample'])\n",
    "print(f'EDA용 학습 데이터 샘플 수: {len(raw_train)}')\n",
    "print('예시 3건:')\n",
    "for sample in raw_train[:3]:\n",
    "    print('KO:', sample['ko'])\n",
    "    print('EN:', sample['mt'])\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ff49e",
   "metadata": {},
   "source": [
    "## 2. 가볍게 EDA 해보기\n",
    "\n",
    "문장 길이 분포나 자주 등장하는 단어를 보면 **최대 길이**, **정규화 방식**을 정하는 데 도움이 됩니다. 간단히 아래 내용을 확인해 봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150d9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ko_len        en_len\n",
      "count  10000.000000  10000.000000\n",
      "mean       6.575200      9.870400\n",
      "std        3.944782      5.910135\n",
      "min        1.000000      1.000000\n",
      "25%        4.000000      5.000000\n",
      "50%        6.000000      9.000000\n",
      "75%        9.000000     13.000000\n",
      "max       37.000000     48.000000\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(text: str, lang: str = 'ko') -> List[str]:\n",
    "    text = text.strip().lower()\n",
    "    if lang == 'ko':\n",
    "        text = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ!?.,'\\\" ]+\", \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-z0-9!?.,'\\\" ]+\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return [tok for tok in text.split(' ') if tok]\n",
    "\n",
    "eda_df = pd.DataFrame(raw_train)\n",
    "eda_df['ko_len'] = eda_df['ko'].apply(lambda x: len(simple_tokenize(x, 'ko')))\n",
    "eda_df['en_len'] = eda_df['mt'].apply(lambda x: len(simple_tokenize(x, 'en')))\n",
    "\n",
    "print(eda_df[['ko_len', 'en_len']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf92884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib 한글 폰트 설정 완료\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.font_manager as fm, matplotlib as mpl\n",
    "font_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "mpl.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "print(\"Matplotlib 한글 폰트 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb80411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(eda_df['ko_len'], bins=30, color='#5DADE2')\n",
    "axes[0].set_title('한국어 문장 길이(토큰 수)')\n",
    "axes[0].set_xlabel('길이')\n",
    "axes[0].set_ylabel('빈도')\n",
    "\n",
    "axes[1].hist(eda_df['en_len'], bins=30, color='#F5B041')\n",
    "axes[1].set_title('영어 문장 길이(토큰 수)')\n",
    "axes[1].set_xlabel('길이')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5f2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자주 등장하는 한국어 단어 10개:\n",
      "     있습니다. : 833\n",
      "         수 : 825\n",
      "        저희 : 387\n",
      "         이 : 319\n",
      "        있는 : 270\n",
      "      합니다. : 236\n",
      "         더 : 231\n",
      "         것 : 202\n",
      "         한 : 187\n",
      "        대해 : 183\n",
      "\n",
      "자주 등장하는 영어 단어 10개:\n",
      "       the : 4359\n",
      "        to : 2869\n",
      "         a : 2348\n",
      "       you : 2336\n",
      "       and : 1997\n",
      "         i : 1902\n",
      "        is : 1718\n",
      "        of : 1654\n",
      "        it : 1521\n",
      "        in : 1248\n"
     ]
    }
   ],
   "source": [
    "ko_counter = Counter()\n",
    "en_counter = Counter()\n",
    "for row in raw_train:\n",
    "    ko_counter.update(simple_tokenize(row['ko'], 'ko'))\n",
    "    en_counter.update(simple_tokenize(row['mt'], 'en'))\n",
    "\n",
    "print('자주 등장하는 한국어 단어 10개:')\n",
    "for word, freq in ko_counter.most_common(10):\n",
    "    print(f'{word:>10} : {freq}')\n",
    "\n",
    "print(\"\\n자주 등장하는 영어 단어 10개:\")\n",
    "for word, freq in en_counter.most_common(10):\n",
    "    print(f'{word:>10} : {freq}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4467d9",
   "metadata": {},
   "source": [
    "> 위 EDA 결과를 보고 **MAX_LENGTH=40** 정도면 전체 문장의 대부분을 커버할 수 있겠다고 판단했습니다. 빈도 상위 단어를 보면 영어는 소문자로 통일하는 것이 적절하고, 한글도 특수문자를 많이 제거해도 무방해 보입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd79461",
   "metadata": {},
   "source": [
    "## 3. 전처리 함수 정의하기\n",
    "\n",
    "여기부터는 학습에 쓸 전체 데이터를 다룹니다. (너무 오래 걸리면 `train_limit`, `valid_limit`를 더 줄이세요.) 한 단계씩 함수로 나눠 두면 디버깅이 쉬워집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db66038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pairs: 20000, valid_pairs: 2000\n",
      "첫 번째 샘플: (['그리고', '더블', '크립', '제품이', '하자', '없는', '배송', '부탁드립니다.'], ['also,', 'please', 'ensure', 'that', 'the', 'double', 'creep', 'product', 'is', 'delivered', 'without', 'defects.'])\n"
     ]
    }
   ],
   "source": [
    "def clean_and_tokenize(text: str, lang: str) -> List[str]:\n",
    "    text = text.strip().lower()\n",
    "    if lang == 'ko':\n",
    "        text = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ!?.,'\\\" ]+\", \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-z0-9!?.,'\\\" ]+\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return [tok for tok in text.split(' ') if tok]\n",
    "\n",
    "\n",
    "def build_dataset(raw_items: List[Dict[str, str]], lang_pair: Tuple[str, str]) -> List[Tuple[List[str], List[str]]]:\n",
    "    src_lang, tgt_lang = lang_pair\n",
    "    pairs = []\n",
    "    for item in raw_items:\n",
    "        src_tokens = clean_and_tokenize(item[src_lang], 'ko')\n",
    "        tgt_tokens = clean_and_tokenize(item[tgt_lang], 'en')\n",
    "        if src_tokens and tgt_tokens:\n",
    "            pairs.append((src_tokens, tgt_tokens))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "train_raw = load_json_pairs(data_dir / CONFIG['train_file'], limit=CONFIG['train_limit'])\n",
    "valid_raw = load_json_pairs(data_dir / CONFIG['valid_file'], limit=CONFIG['valid_limit'])\n",
    "\n",
    "train_pairs = build_dataset(train_raw, ('ko', 'mt'))\n",
    "valid_pairs = build_dataset(valid_raw, ('ko', 'mt'))\n",
    "\n",
    "print(f'train_pairs: {len(train_pairs)}, valid_pairs: {len(valid_pairs)}')\n",
    "print('첫 번째 샘플:', train_pairs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd5dcd",
   "metadata": {},
   "source": [
    "## 4. Vocabulary 만들기\n",
    "\n",
    "가장 많이 쓰이는 단어부터 인덱스를 부여합니다. `<pad>`, `<sos>`, `<eos>`, `<unk>`를 항상 포함하도록 했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq: int = 1):\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        self.token2idx: Dict[str, int] = {}\n",
    "        self.idx2token: Dict[int, str] = {}\n",
    "        for token in self.special_tokens:\n",
    "            self.add_token(token)\n",
    "\n",
    "    def add_token(self, token: str) -> None:\n",
    "        if token not in self.token2idx:\n",
    "            idx = len(self.token2idx)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "\n",
    "    def build(self, sentences: List[List[str]]) -> None:\n",
    "        counter = Counter()\n",
    "        for sent in sentences:\n",
    "            counter.update(sent)\n",
    "        for token, freq in counter.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.add_token(token)\n",
    "\n",
    "    def encode(self, tokens: List[str], max_len: int) -> List[int]:\n",
    "        tokens = tokens[: max_len - 2]\n",
    "        ids = [self.token2idx['<sos>']]\n",
    "        ids += [self.token2idx.get(tok, self.token2idx['<unk>']) for tok in tokens]\n",
    "        ids.append(self.token2idx['<eos>'])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        words = []\n",
    "        for idx in ids:\n",
    "            token = self.idx2token.get(int(idx), '<unk>')\n",
    "            if token == '<eos>':\n",
    "                break\n",
    "            if token not in {'<pad>', '<sos>'}:\n",
    "                words.append(token)\n",
    "        return words\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self.token2idx['<pad>']\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.token2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cc6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 12823\n",
      "Target vocab size: 9091\n"
     ]
    }
   ],
   "source": [
    "src_vocab = Vocabulary(min_freq=CONFIG['min_freq'])\n",
    "src_vocab.build([src for src, _ in train_pairs])\n",
    "\n",
    "tgt_vocab = Vocabulary(min_freq=CONFIG['min_freq'])\n",
    "tgt_vocab.build([tgt for _, tgt in train_pairs])\n",
    "\n",
    "print('Source vocab size:', src_vocab.size)\n",
    "print('Target vocab size:', tgt_vocab.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826557a0",
   "metadata": {},
   "source": [
    "## 5. Dataset & DataLoader 구성\n",
    "\n",
    "토치에서 바로 읽을 수 있도록 `Dataset` 클래스를 만들어 정수 시퀀스를 반환합니다. 패딩은 `collate_fn`에서 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8a0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[List[str], List[str]]], src_vocab: Vocabulary, tgt_vocab: Vocabulary, max_length: int):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        src_tokens, tgt_tokens = self.pairs[idx]\n",
    "        src_ids = torch.tensor(self.src_vocab.encode(src_tokens, self.max_length), dtype=torch.long)\n",
    "        tgt_ids = torch.tensor(self.tgt_vocab.encode(tgt_tokens, self.max_length), dtype=torch.long)\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(seq) for seq in src_batch])\n",
    "    tgt_lengths = torch.tensor([len(seq) for seq in tgt_batch])\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_id)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_id)\n",
    "    decoder_inputs = tgt_padded[:, :-1]\n",
    "    decoder_targets = tgt_padded[:, 1:]\n",
    "    return src_padded, src_lengths, decoder_inputs, decoder_targets, tgt_lengths\n",
    "\n",
    "\n",
    "train_dataset = TranslationDataset(train_pairs, src_vocab, tgt_vocab, CONFIG['max_length'])\n",
    "valid_dataset = TranslationDataset(valid_pairs, src_vocab, tgt_vocab, CONFIG['max_length'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('DataLoader 준비 완료!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52853369",
   "metadata": {},
   "source": [
    "## 6. Seq2Seq 모델 정의\n",
    "\n",
    "Encoder와 Decoder 모두 GRU 한 층으로 구성했습니다. 입문 단계에서는 `pack_padded_sequence` 없이 **단순히 패딩된 시퀀스**를 넣고, 손실 계산 시 패딩 토큰을 무시하도록 했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5f45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_step: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        embedded = self.embedding(input_step.unsqueeze(1))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_lengths: torch.Tensor, decoder_inputs: torch.Tensor, teacher_forcing: float = 0.5) -> torch.Tensor:\n",
    "        batch_size = src.size(0)\n",
    "        target_len = decoder_inputs.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        _, hidden = self.encoder(src, src_lengths)\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size, device=self.device)\n",
    "        input_token = decoder_inputs[:, 0]\n",
    "\n",
    "        for t in range(target_len):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher = random.random() < teacher_forcing\n",
    "            if teacher and t + 1 < target_len:\n",
    "                input_token = decoder_inputs[:, t + 1]\n",
    "            else:\n",
    "                input_token = output.argmax(dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782453bc",
   "metadata": {},
   "source": [
    "## 7. 학습 및 검증 루프\n",
    "\n",
    "손실 함수는 `CrossEntropyLoss(ignore_index=pad_id)`를 사용해 패딩 토큰은 무시합니다. 검증 시에는 teacher forcing 없이 greedy decoding을 적용해 간단한 예측 품질을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8791a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_example(model: Seq2Seq, sentence: str, max_len: int = 40) -> str:\n",
    "    model.eval()\n",
    "    tokens = clean_and_tokenize(sentence, 'ko')\n",
    "    ids = src_vocab.encode(tokens, max_len)\n",
    "    src_tensor = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_lengths = torch.tensor([len(ids)])\n",
    "    _, hidden = model.encoder(src_tensor, src_lengths)\n",
    "    outputs = []\n",
    "    input_token = torch.tensor([tgt_vocab.token2idx['<sos>']], device=device)\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        next_id = output.argmax(dim=1)\n",
    "        if next_id.item() == tgt_vocab.token2idx['<eos>']:\n",
    "            break\n",
    "        outputs.append(next_id.item())\n",
    "        input_token = next_id\n",
    "    return ' '.join(tgt_vocab.decode(outputs))\n",
    "\n",
    "\n",
    "def train_model(model: Seq2Seq, train_loader: DataLoader, valid_loader: DataLoader, epochs: int = 5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_id)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for src, src_lengths, dec_inputs, dec_targets, _ in train_loader:\n",
    "            src = src.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            dec_targets = dec_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, src_lengths, dec_inputs, teacher_forcing=CONFIG['teacher_forcing'])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), dec_targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for src, src_lengths, dec_inputs, dec_targets, _ in valid_loader:\n",
    "                src = src.to(device)\n",
    "                dec_inputs = dec_inputs.to(device)\n",
    "                dec_targets = dec_targets.to(device)\n",
    "                outputs = model(src, src_lengths, dec_inputs, teacher_forcing=0.0)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), dec_targets.reshape(-1))\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader)\n",
    "\n",
    "        history.append({'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss})\n",
    "        print(f'Epoch {epoch}/{epochs} | train_loss={train_loss:.3f} | valid_loss={valid_loss:.3f}')\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febb82a",
   "metadata": {},
   "source": [
    "## 8. 학습 실행\n",
    "\n",
    "데이터 양이 많으므로 GPU 환경이 아니라면 `train_limit`, `valid_limit`, `epochs`를 줄여서 먼저 파이프라인만 확인하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e73a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train_loss=6.237 | valid_loss=5.842\n",
      "Epoch 2/5 | train_loss=5.636 | valid_loss=5.695\n",
      "Epoch 3/5 | train_loss=5.317 | valid_loss=5.659\n",
      "Epoch 4/5 | train_loss=5.071 | valid_loss=5.620\n",
      "Epoch 5/5 | train_loss=4.866 | valid_loss=5.608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 6.237261700554016,\n",
       "  'valid_loss': 5.841859340667725},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 5.6364312644202865,\n",
       "  'valid_loss': 5.695423737168312},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 5.317289967887318,\n",
       "  'valid_loss': 5.659234777092934},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 5.071347740892405,\n",
       "  'valid_loss': 5.619599476456642},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 4.865617473285419,\n",
       "  'valid_loss': 5.608177483081818}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(src_vocab.size, CONFIG['embedding_dim'], CONFIG['hidden_dim'], src_vocab.pad_id)\n",
    "decoder = Decoder(tgt_vocab.size, CONFIG['embedding_dim'], CONFIG['hidden_dim'], tgt_vocab.pad_id)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "training_history = train_model(model, train_loader, valid_loader, epochs=CONFIG['epochs'])\n",
    "training_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7987af1",
   "metadata": {},
   "source": [
    "## 9. 번역 예시 살펴보기\n",
    "\n",
    "학습된 모델로 임의 문장을 번역해 보면서 어떤 부분이 잘 되고 부족한지 직접 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6f9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO: 오늘 저녁에 시간 있어?\n",
      "EN(pred): i you you the <unk>\n",
      "------------------------------\n",
      "KO: 커피 대신 차를 마시고 싶어요.\n",
      "EN(pred): i am the <unk> of the <unk> of the <unk>\n",
      "------------------------------\n",
      "KO: 회의 자료를 영어로 다시 보내 줄 수 있어?\n",
      "EN(pred): we can you you you can you can you the\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    '오늘 저녁에 시간 있어?',\n",
    "    '커피 대신 차를 마시고 싶어요.',\n",
    "    '회의 자료를 영어로 다시 보내 줄 수 있어?',\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print('KO:', sent)\n",
    "    print('EN(pred):', translate_example(model, sent))\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de74b0c4",
   "metadata": {},
   "source": [
    "## 10. 다음 단계 아이디어\n",
    "\n",
    "- 형태소 분석기나 SentencePiece를 사용해 더 나은 토큰화 시도\n",
    "- Encoder를 양방향 GRU로 바꾸기\n",
    "- Decoder에 Attention을 도입해 긴 문장 처리력 향상\n",
    "- 학습 로그/시각화(TensorBoard 등)를 추가해 학습 곡선을 분석\n",
    "- BLEU/ROUGE 같은 정량 평가 함수 직접 구현해 비교\n",
    "\n",
    "이 노트북을 기반으로 자신의 실험 노트와 보고서를 작성하면, **어떤 데이터를 어떤 방식으로 다뤘는지**를 명확하게 설명하기 좋습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0869c",
   "metadata": {},
   "source": [
    "## 11. SentencePiece/형태소 기반 토큰화 (선택)\n",
    "\n",
    "간단한 규칙 기반 토큰화로도 학습 파이프라인을 돌릴 수 있지만, 희귀 단어 처리에 한계가 있습니다. 아래 셀은 `sentencepiece`를 활용한 서브워드 모델을 학습하는 예시입니다. GPU/시간 여유가 있을 때만 실행하면 되고, 형태소 분석기를 연결하는 경우에도 이 자리에 해당 코드를 넣으면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41cd7a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Part3_mission_11/spm_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33msentencepiece 패키지가 필요합니다.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m corpus_path = data_dir / \u001b[33m'\u001b[39m\u001b[33mspm_corpus.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m train_raw:\n\u001b[32m     17\u001b[39m         f.write(pair[\u001b[33m'\u001b[39m\u001b[33mko\u001b[39m\u001b[33m'\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m).strip() + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai5_project1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Part3_mission_11/spm_corpus.txt'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import sentencepiece as spm\n",
    "except ImportError:\n",
    "    spm = None\n",
    "    print(\"sentencepiece가 설치되어 있지 않습니다. 필요하다면 `pip install sentencepiece` 후 다시 실행하세요.\")\n",
    "\n",
    "RUN_SENTENCEPIECE = True\n",
    "SPM_MODEL_PREFIX = 'spm_ko_en'\n",
    "SPM_VOCAB_SIZE = 8000\n",
    "\n",
    "if RUN_SENTENCEPIECE:\n",
    "    if spm is None:\n",
    "        raise ImportError('sentencepiece 패키지가 필요합니다.')\n",
    "    corpus_path = data_dir / 'spm_corpus.txt'\n",
    "    with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for pair in train_raw:\n",
    "            f.write(pair['ko'].replace(\"\\n\", ' ').strip() + \"\\n\")\n",
    "            f.write(pair['mt'].lower().replace(\"\\n\", ' ').strip() + \"\\n\")\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(corpus_path),\n",
    "        model_prefix=SPM_MODEL_PREFIX,\n",
    "        vocab_size=SPM_VOCAB_SIZE,\n",
    "        model_type='unigram',\n",
    "        character_coverage=0.9995,\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{SPM_MODEL_PREFIX}.model\")\n",
    "    print('SentencePiece 학습 완료! 예시 토큰:', sp.encode('안녕하세요 만나서 반가워요.', out_type=str)[:15])\n",
    "else:\n",
    "    print('SentencePiece 실험은 RUN_SENTENCEPIECE=True 로 변경 후 실행하세요.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd172dc",
   "metadata": {},
   "source": [
    "## 12. BLEU/ROUGE 등 정량 평가 함수\n",
    "\n",
    "검증 손실 외에 번역 품질을 수치로 비교하기 위해 간단한 BLEU, ROUGE-L 계산 함수를 추가합니다. 또한 학습된 모델을 빠르게 평가할 수 있도록 배치 단위 greedy decoding 헬퍼를 제공합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ae2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_bleu(references, hypotheses, max_n: int = 4, smooth_eps: float = 1e-9):\n",
    "    clipped = [0] * max_n\n",
    "    totals = [0] * max_n\n",
    "    ref_length = 0\n",
    "    hyp_length = 0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_length += max(len(ref), 1)\n",
    "        hyp_length += max(len(hyp), 1)\n",
    "        for n in range(1, max_n + 1):\n",
    "            ref_ngrams = Counter(tuple(ref[i:i + n]) for i in range(max(len(ref) - n + 1, 0)))\n",
    "            hyp_ngrams = Counter(tuple(hyp[i:i + n]) for i in range(max(len(hyp) - n + 1, 0)))\n",
    "            totals[n - 1] += sum(hyp_ngrams.values())\n",
    "            for ng, count in hyp_ngrams.items():\n",
    "                clipped[n - 1] += min(count, ref_ngrams.get(ng, 0))\n",
    "    precisions = []\n",
    "    for n in range(max_n):\n",
    "        numerator = clipped[n] + smooth_eps\n",
    "        denominator = totals[n] + smooth_eps\n",
    "        precisions.append(numerator / denominator)\n",
    "    geo_mean = math.exp(sum((1 / max_n) * math.log(p) for p in precisions))\n",
    "    bp = 1.0 if hyp_length > ref_length else math.exp(1 - ref_length / max(hyp_length, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "\n",
    "def lcs_length(a, b):\n",
    "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i in range(1, len(a) + 1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "            if a[i - 1] == b[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "def rouge_l_score(references, hypotheses):\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        lcs = lcs_length(ref, hyp)\n",
    "        prec = lcs / max(len(hyp), 1)\n",
    "        rec = lcs / max(len(ref), 1)\n",
    "        if prec + rec == 0:\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            scores.append((2 * prec * rec) / (prec + rec))\n",
    "    return float(sum(scores) / max(len(scores), 1))\n",
    "\n",
    "\n",
    "def greedy_decode_batch(model, src, src_lengths, max_len: int = 50):\n",
    "    model.eval()\n",
    "    batch_size = src.size(0)\n",
    "    sos_id = tgt_vocab.token2idx['<sos>']\n",
    "    eos_id = tgt_vocab.token2idx['<eos>']\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model.decoder, 'attention'):\n",
    "            encoder_outputs, hidden = model.encoder(src, src_lengths)\n",
    "            mask = (src != src_vocab.pad_id)\n",
    "            input_token = torch.full((batch_size,), sos_id, dtype=torch.long, device=src.device)\n",
    "            for _ in range(max_len):\n",
    "                prediction, hidden, _ = model.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                next_token = prediction.argmax(dim=1)\n",
    "                outputs.append(next_token)\n",
    "                input_token = next_token\n",
    "        else:\n",
    "            _, hidden = model.encoder(src, src_lengths)\n",
    "            input_token = torch.full((batch_size,), sos_id, dtype=torch.long, device=src.device)\n",
    "            for _ in range(max_len):\n",
    "                prediction, hidden = model.decoder(input_token, hidden)\n",
    "                next_token = prediction.argmax(dim=1)\n",
    "                outputs.append(next_token)\n",
    "                input_token = next_token\n",
    "    if not outputs:\n",
    "        return torch.zeros((batch_size, 0), dtype=torch.long)\n",
    "    stacked = torch.stack(outputs, dim=1)\n",
    "    result = []\n",
    "    for row in stacked:\n",
    "        tokens = []\n",
    "        for idx in row.tolist():\n",
    "            if idx == eos_id:\n",
    "                break\n",
    "            tokens.append(idx)\n",
    "        result.append(tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_bleu_rouge(model, data_loader, num_batches: int = 5, max_len: int = 60):\n",
    "    refs, hyps = [], []\n",
    "    for i, (src, src_lengths, _, dec_targets, _) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        src = src.to(device)\n",
    "        predictions = greedy_decode_batch(model, src, src_lengths, max_len=max_len)\n",
    "        for target_ids, pred_ids in zip(dec_targets, predictions):\n",
    "            refs.append(tgt_vocab.decode(target_ids.tolist()))\n",
    "            hyps.append(tgt_vocab.decode(pred_ids))\n",
    "    bleu = corpus_bleu(refs, hyps)\n",
    "    rouge = rouge_l_score(refs, hyps)\n",
    "    return bleu, rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f18f1",
   "metadata": {},
   "source": [
    "## 13. TensorBoard/학습 로그 기록 (옵션)\n",
    "\n",
    "학습 과정을 시각화하려면 `SummaryWriter`를 활용해 손실과 BLEU/ROUGE를 기록할 수 있습니다. 아래 함수는 기존 학습 루프에 로깅과 간단한 metric 계산을 추가한 버전입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "599fdbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard를 사용하려면 `pip install tensorboard` 후 다시 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    SummaryWriter = None\n",
    "    print('TensorBoard를 사용하려면 `pip install tensorboard` 후 다시 실행하세요.')\n",
    "\n",
    "\n",
    "def train_model_with_logging(model, train_loader, valid_loader, epochs: int = 5, run_name: str = 'experiment', writer=None, metrics_fn=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_id)\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for src, src_lengths, dec_inputs, dec_targets, _ in train_loader:\n",
    "            src = src.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            dec_targets = dec_targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, src_lengths, dec_inputs, teacher_forcing=CONFIG['teacher_forcing'])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), dec_targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for src, src_lengths, dec_inputs, dec_targets, _ in valid_loader:\n",
    "                src = src.to(device)\n",
    "                dec_inputs = dec_inputs.to(device)\n",
    "                dec_targets = dec_targets.to(device)\n",
    "                outputs = model(src, src_lengths, dec_inputs, teacher_forcing=0.0)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), dec_targets.reshape(-1))\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader)\n",
    "\n",
    "        bleu = rouge = None\n",
    "        if metrics_fn is not None:\n",
    "            bleu, rouge = metrics_fn(model, valid_loader)\n",
    "        history.append({'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss, 'bleu': bleu, 'rouge_l': rouge})\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f'{run_name}/train_loss', train_loss, epoch)\n",
    "            writer.add_scalar(f'{run_name}/valid_loss', valid_loss, epoch)\n",
    "            if bleu is not None:\n",
    "                writer.add_scalar(f'{run_name}/bleu', bleu, epoch)\n",
    "            if rouge is not None:\n",
    "                writer.add_scalar(f'{run_name}/rouge_l', rouge, epoch)\n",
    "        print(f'[LOG] {run_name} Epoch {epoch}/{epochs} | train {train_loss:.3f} | valid {valid_loss:.3f} | BLEU {bleu if bleu is not None else 0:.3f} | ROUGE-L {rouge if rouge is not None else 0:.3f}')\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.flush()\n",
    "    return history\n",
    "\n",
    "USE_TENSORBOARD = False\n",
    "writer = SummaryWriter(log_dir='runs/mission11') if (USE_TENSORBOARD and SummaryWriter is not None) else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b0d03",
   "metadata": {},
   "source": [
    "## 14. 양방향 Encoder + Attention Decoder\n",
    "\n",
    "이제 Encoder를 양방향 GRU로 확장하고, Decoder에 Bahdanau Attention을 추가해 긴 문장에서도 문맥 정보를 더 잘 활용하도록 합니다. 모델 아키텍처는 아래 클래스들을 참고하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c924b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiGRU(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pad_idx: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, lengths: torch.Tensor):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2], hidden[-1]), dim=1))).unsqueeze(0)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_dim: int, dec_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim + dec_dim, dec_dim)\n",
    "        self.v = nn.Linear(dec_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attn_scores = self.v(energy).squeeze(-1)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, enc_dim: int, pad_idx: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.attention = BahdanauAttention(enc_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(embedding_dim + enc_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim + enc_dim + embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask):\n",
    "        embedded = self.dropout(self.embedding(input_token)).unsqueeze(1)\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=2).squeeze(1))\n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, src_lengths, decoder_inputs, teacher_forcing: float = 0.5):\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
    "        mask = (src != self.pad_idx)\n",
    "        batch_size = src.size(0)\n",
    "        target_len = decoder_inputs.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size, device=src.device)\n",
    "        input_token = decoder_inputs[:, 0]\n",
    "        for t in range(target_len):\n",
    "            prediction, hidden, _ = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "            outputs[:, t, :] = prediction\n",
    "            teacher = random.random() < teacher_forcing\n",
    "            if teacher and t + 1 < target_len:\n",
    "                input_token = decoder_inputs[:, t + 1]\n",
    "            else:\n",
    "                input_token = prediction.argmax(dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575026e",
   "metadata": {},
   "source": [
    "## 15. BiGRU + Attention 학습 실행 (옵션)\n",
    "\n",
    "고급 모델은 연산량이 많으므로 기본 설정에서는 비활성화했습니다. GPU가 있다면 아래 플래그를 `True`로 바꿔 학습해 보세요. TensorBoard 로깅과 BLEU/ROUGE 계산도 함께 활용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ff28f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiGRU + Attention 학습은 생략되었습니다. RUN_BIGRU_ATTENTION=True 로 변경하여 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "RUN_BIGRU_ATTENTION = False\n",
    "attn_model = None\n",
    "attn_history = []\n",
    "\n",
    "if RUN_BIGRU_ATTENTION:\n",
    "    encoder_bigru = EncoderBiGRU(\n",
    "        vocab_size=src_vocab.size,\n",
    "        embedding_dim=CONFIG['embedding_dim'],\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        pad_idx=src_vocab.pad_id,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "    decoder_attn = AttnDecoderGRU(\n",
    "        vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=CONFIG['embedding_dim'],\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        enc_dim=CONFIG['hidden_dim'] * 2,\n",
    "        pad_idx=tgt_vocab.pad_id,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "    attn_model = Seq2SeqAttn(encoder_bigru.to(device), decoder_attn.to(device), src_vocab.pad_id).to(device)\n",
    "    attn_history = train_model_with_logging(\n",
    "        attn_model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        run_name='bigru_attention',\n",
    "        writer=writer,\n",
    "        metrics_fn=lambda m, loader: evaluate_bleu_rouge(m, loader, num_batches=3),\n",
    "    )\n",
    "else:\n",
    "    print('BiGRU + Attention 학습은 생략되었습니다. RUN_BIGRU_ATTENTION=True 로 변경하여 실행하세요.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ee2ea",
   "metadata": {},
   "source": [
    "## 16. 모델 성능 요약\n",
    "\n",
    "아래 셀은 현재 학습된 모델(기본 Seq2Seq, BiGRU+Attention 등)에 대해 BLEU/ROUGE 지표를 계산해 줍니다. 실험을 진행할 때마다 값을 기록해 두면 보고서에 바로 활용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16da67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 Seq2Seq | BLEU: 0.027 | ROUGE-L: 0.245\n",
      "BiGRU+Attention 모델은 아직 학습되지 않았습니다.\n"
     ]
    }
   ],
   "source": [
    "if 'model' in globals():\n",
    "    bleu_base, rouge_base = evaluate_bleu_rouge(model, valid_loader, num_batches=5)\n",
    "    print(f'기본 Seq2Seq | BLEU: {bleu_base:.3f} | ROUGE-L: {rouge_base:.3f}')\n",
    "else:\n",
    "    print('기본 Seq2Seq 모델이 학습되지 않았습니다.')\n",
    "\n",
    "if attn_model is not None:\n",
    "    bleu_attn, rouge_attn = evaluate_bleu_rouge(attn_model, valid_loader, num_batches=5)\n",
    "    print(f'BiGRU+Attention | BLEU: {bleu_attn:.3f} | ROUGE-L: {rouge_attn:.3f}')\n",
    "else:\n",
    "    print('BiGRU+Attention 모델은 아직 학습되지 않았습니다.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72361e45",
   "metadata": {},
   "source": [
    "## 마무리 노트\n",
    "\n",
    "- **EDA/전처리 → 기본 모델 → 고급 실험**의 순서를 한 노트에서 유지했으므로, 실험기를 제출할 때도 어떤 과정을 거쳤는지 명확하게 설명할 수 있습니다.\n",
    "- 학습 데이터 수(`train_limit`), SentencePiece/형태소 토큰화, BiGRU/Attention, TensorBoard 로깅, BLEU/ROUGE 평가를 조합해가며 여러 실험 로그를 남겨 보세요.\n",
    "- GPU 자원이 부족하면 먼저 소규모 데이터로 파이프라인을 검증한 뒤, 설정을 늘려가며 결과를 갱신하면 됩니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai5_project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
