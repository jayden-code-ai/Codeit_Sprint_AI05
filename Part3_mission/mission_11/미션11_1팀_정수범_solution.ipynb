{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92296ca8",
   "metadata": {},
   "source": [
    "# 미션 11: Seq2Seq + Attention 기반 한-영 번역 실습\n",
    "\n",
    "제공된 일상 회화 한-영 병렬 코퍼스를 이용해 기본 Seq2Seq 모델과 Attention이 포함된 모델을 모두 구현하고 비교합니다. 노트북 전체 흐름은 데이터 로딩 → 전처리 → 어휘 사전 구축 → PyTorch Dataset/DataLoader 구성 → Encoder/Decoder 구현 → 학습 및 평가 → 번역 예시 확인 순서로 이어집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffdc65",
   "metadata": {},
   "source": [
    "## 노트북 구성\n",
    "\n",
    "1. **데이터 준비**: JSON 원본을 불러와 간단한 정규화/토크나이즈를 수행합니다.\n",
    "2. **어휘 사전**: 토큰 빈도를 기준으로 `<pad>/<sos>/<eos>/<unk>` 토큰을 포함한 Vocabulary를 만듭니다.\n",
    "3. **Dataset & DataLoader**: `<sos>`/`<eos>`가 붙은 시퀀스를 텐서로 변환하고, 배치 단위로 패딩합니다.\n",
    "4. **모델 정의**: GRU 기반 Encoder/Decoder, Bahdanau Attention을 구현합니다.\n",
    "5. **학습/평가 루프**: Teacher Forcing, Gradient Clipping, 간단한 BLEU 계산을 포함한 공용 함수로 두 모델을 모두 돌립니다.\n",
    "6. **결과 확인**: 검증 세트와 임의 문장의 번역 결과를 비교합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e55bcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[CONFIG]\n",
      "{\n",
      "  \"train_file\": \"/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_train_set.json\",\n",
      "  \"valid_file\": \"/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_valid_set.json\",\n",
      "  \"train_limit\": 60000,\n",
      "  \"valid_limit\": 5000,\n",
      "  \"min_freq\": 3,\n",
      "  \"max_length\": 40,\n",
      "  \"batch_size\": 128,\n",
      "  \"embedding_dim\": 256,\n",
      "  \"hidden_dim\": 384,\n",
      "  \"dropout\": 0.2,\n",
      "  \"num_layers\": 1,\n",
      "  \"learning_rate\": 0.001,\n",
      "  \"epochs\": 8,\n",
      "  \"teacher_forcing\": 0.5,\n",
      "  \"grad_clip\": 1.0,\n",
      "  \"num_workers\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"Part3_mission_11\")\n",
    "CONFIG = {\n",
    "    \"train_file\": \"/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_train_set.json\",\n",
    "    \"valid_file\": \"/mnt/nas/jayden_code/Codeit_Practice/Part3_mission_11/일상생활및구어체_한영_valid_set.json\",\n",
    "    \"train_limit\": 60000,  # 너무 오래 걸리면 값을 더 줄여 사용하세요.\n",
    "    \"valid_limit\": 5000,\n",
    "    \"min_freq\": 3,\n",
    "    \"max_length\": 40,\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_dim\": 384,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_layers\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"epochs\": 8,\n",
    "    \"teacher_forcing\": 0.5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"[CONFIG]\")\n",
    "print(json.dumps({k: (str(v) if isinstance(v, Path) else v) for k, v in CONFIG.items()}, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d135ffe",
   "metadata": {},
   "source": [
    "## 데이터 로딩 및 간단 전처리\n",
    "\n",
    "- JSON 파일을 불러온 뒤 한국어/영어 문장을 각각 정규화하고 공백 단위로 토크나이즈합니다.\n",
    "- 전체 120만 문장으로는 학습 시간이 오래 걸리므로, 기본값으로는 무작위 6만/5천 문장만 사용하도록 제한해 두었습니다. 필요하면 `CONFIG['train_limit']` 값을 조정하면 됩니다.\n",
    "- 문장 길이 분포를 확인해 Encoder/Decoder 최대 길이(`max_length`)가 적절한지 점검합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4730339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60000 training pairs / 5000 validation pairs\n",
      "Train | samples = 60000\n",
      " KO lengths: {'p50': 6.0, 'p75': 9.0, 'p90': 12.0, 'p95': 14.0}\n",
      " EN lengths: {'p50': 9.0, 'p75': 13.0, 'p90': 18.0, 'p95': 21.0}\n",
      "Valid | samples = 5000\n",
      " KO lengths: {'p50': 6.0, 'p75': 9.0, 'p90': 12.0, 'p95': 14.0}\n",
      " EN lengths: {'p50': 9.0, 'p75': 13.0, 'p90': 18.0, 'p95': 21.0}\n",
      "샘플 데이터 3건\n",
      "KO: 그리고 더블 크립 제품이 하자 없는 배송 부탁드립니다.\n",
      "EN: Also, please ensure that the double creep product is delivered without defects.\n",
      "-\n",
      "KO: 날씨 보호 테크 시스템, 특허받은 용접 모서리 및 반전된 솔기가 물을 막아줍니다.\n",
      "EN: Weather protection tech system, patented welded edges and inverted seams keep water out.\n",
      "-\n",
      "KO: 무엇을 도와드릴까요?\n",
      "EN: How may I help you?\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_ko(text: str) -> List[str]:\n",
    "    text = normalize_text(text)\n",
    "    text = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ!?.,']+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return [tok for tok in text.split(\" \") if tok]\n",
    "\n",
    "\n",
    "def tokenize_en(text: str) -> List[str]:\n",
    "    text = normalize_text(text.lower())\n",
    "    text = re.sub(r\"[^a-z0-9!?.,']+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return [tok for tok in text.split(\" \") if tok]\n",
    "\n",
    "\n",
    "def load_sentence_pairs(path: Path, limit: int = None, seed: int = SEED) -> List[Dict[str, List[str]]]:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)[\"data\"]\n",
    "    if limit is not None and limit < len(raw):\n",
    "        rng = random.Random(seed)\n",
    "        indices = rng.sample(range(len(raw)), limit)\n",
    "        raw = [raw[i] for i in indices]\n",
    "    pairs = []\n",
    "    for item in raw:\n",
    "        ko_text = item[\"ko\"].strip()\n",
    "        en_text = item[\"mt\"].strip()\n",
    "        ko_tokens = tokenize_ko(ko_text)\n",
    "        en_tokens = tokenize_en(en_text)\n",
    "        if not ko_tokens or not en_tokens:\n",
    "            continue\n",
    "        pairs.append({\"ko_text\": ko_text, \"en_text\": en_text,\"ko_tokens\": ko_tokens, \"en_tokens\": en_tokens,})\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def describe_lengths(pairs: List[Dict[str, List[str]]], label: str) -> None:\n",
    "    ko_lengths = [len(p[\"ko_tokens\"]) for p in pairs]\n",
    "    en_lengths = [len(p[\"en_tokens\"]) for p in pairs]\n",
    "\n",
    "    def _summary(lengths: List[int]) -> Dict[str, float]:\n",
    "        percentiles = [50, 75, 90, 95]\n",
    "        return {f\"p{p}\": float(np.percentile(lengths, p)) for p in percentiles}\n",
    "\n",
    "    print(f\"{label} | samples = {len(pairs)}\")\n",
    "    print(\" KO lengths:\", _summary(ko_lengths))\n",
    "    print(\" EN lengths:\", _summary(en_lengths))\n",
    "\n",
    "\n",
    "train_examples = load_sentence_pairs(CONFIG[\"train_file\"], CONFIG[\"train_limit\"], SEED)\n",
    "valid_examples = load_sentence_pairs(CONFIG[\"valid_file\"], CONFIG[\"valid_limit\"], SEED + 1)\n",
    "print(f\"Loaded {len(train_examples)} training pairs / {len(valid_examples)} validation pairs\")\n",
    "describe_lengths(train_examples, \"Train\")\n",
    "describe_lengths(valid_examples, \"Valid\")\n",
    "\n",
    "print(\"샘플 데이터 3건\")\n",
    "for sample in train_examples[:3]:\n",
    "    print(\"KO:\", sample[\"ko_text\"])\n",
    "    print(\"EN:\", sample[\"en_text\"])\n",
    "    print(\"-\")\n",
    "\n",
    "train_pairs = [(ex[\"ko_tokens\"], ex[\"en_tokens\"]) for ex in train_examples]\n",
    "valid_pairs = [(ex[\"ko_tokens\"], ex[\"en_tokens\"]) for ex in valid_examples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8284e7d",
   "metadata": {},
   "source": [
    "## Vocabulary 생성\n",
    "\n",
    "- `<pad>`, `<sos>`, `<eos>`, `<unk>` 4개의 특수 토큰을 고정으로 추가합니다.\n",
    "- 토큰 빈도가 `min_freq` 미만이면 `<unk>`로 치환합니다.\n",
    "- 학습 데이터를 기반으로 소스/타깃 Vocabulary를 각각 구축합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919b6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq: int = 1):\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.token2idx: Dict[str, int] = {}\n",
    "        self.idx2token: Dict[int, str] = {}\n",
    "        self.pad_token, self.sos_token, self.eos_token, self.unk_token = self.special_tokens\n",
    "        self._built = False\n",
    "\n",
    "    def build(self, sentences: List[List[str]]) -> None:\n",
    "        counter = Counter()\n",
    "        for sentence in sentences:\n",
    "            counter.update(sentence)\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        for token in self.special_tokens:\n",
    "            self._add_token(token)\n",
    "        for token, freq in sorted(counter.items(), key=lambda x: (-x[1], x[0])):\n",
    "            if freq < self.min_freq:\n",
    "                continue\n",
    "            if token in self.token2idx:\n",
    "                continue\n",
    "            self._add_token(token)\n",
    "        self._built = True\n",
    "\n",
    "    def _add_token(self, token: str) -> None:\n",
    "        idx = len(self.token2idx)\n",
    "        self.token2idx[token] = idx\n",
    "        self.idx2token[idx] = token\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.token2idx)\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self.token2idx[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def sos_id(self) -> int:\n",
    "        return self.token2idx[self.sos_token]\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self.token2idx[self.eos_token]\n",
    "\n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self.token2idx[self.unk_token]\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        tokens: List[str],\n",
    "        add_sos: bool = True,\n",
    "        add_eos: bool = True,\n",
    "        max_length: int = None,\n",
    "    ) -> List[int]:\n",
    "        assert self._built, \"Vocabulary.build()를 먼저 호출하세요.\"\n",
    "        limit = max_length - 2 if max_length else None\n",
    "        truncated = tokens[:limit] if limit is not None else tokens\n",
    "        seq = []\n",
    "        if add_sos:\n",
    "            seq.append(self.sos_id)\n",
    "        seq.extend(self.token2idx.get(tok, self.unk_id) for tok in truncated)\n",
    "        if add_eos:\n",
    "            seq.append(self.eos_id)\n",
    "        return seq\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            token = self.idx2token.get(int(idx), self.unk_token)\n",
    "            if token in {self.pad_token, self.sos_token}:\n",
    "                continue\n",
    "            if token == self.eos_token:\n",
    "                break\n",
    "            tokens.append(token)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42963790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size : 18907\n",
      "Target vocab size : 12566\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_vocab = Vocabulary(min_freq=CONFIG[\"min_freq\"])\n",
    "src_vocab.build([src for src, _ in train_pairs])\n",
    "\n",
    "tgt_vocab = Vocabulary(min_freq=CONFIG[\"min_freq\"])\n",
    "tgt_vocab.build([tgt for _, tgt in train_pairs])\n",
    "\n",
    "print(f\"Source vocab size : {src_vocab.size}\")\n",
    "print(f\"Target vocab size : {tgt_vocab.size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f989245",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader\n",
    "\n",
    "- Dataset은 (소스, 타깃) 토큰 리스트를 숫자 시퀀스로 변환합니다.\n",
    "- Collate 함수가 배치 단위로 패딩, 길이 텐서 계산, Decoder input/target 분리를 담당합니다.\n",
    "- 학습 시 셔플, 검증 시 셔플을 끕니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b394b8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes | train=60000, valid=5000\n",
      "DataLoader 준비 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[List[str], List[str]]], src_vocab: Vocabulary, tgt_vocab: Vocabulary, max_length: int):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        src_tokens, tgt_tokens = self.pairs[idx]\n",
    "        src_ids = self.src_vocab.encode(src_tokens, add_sos=True, add_eos=True, max_length=self.max_length)\n",
    "        tgt_ids = self.tgt_vocab.encode(tgt_tokens, add_sos=True, add_eos=True, max_length=self.max_length)\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(x) for x in src_batch], dtype=torch.long)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_id)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_id)\n",
    "    decoder_inputs = tgt_padded[:, :-1]\n",
    "    decoder_targets = tgt_padded[:, 1:]\n",
    "    return {\n",
    "        \"src\": src_padded,\n",
    "        \"src_lengths\": src_lengths,\n",
    "        \"decoder_inputs\": decoder_inputs,\n",
    "        \"decoder_targets\": decoder_targets,\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = TranslationDataset(train_pairs, src_vocab, tgt_vocab, CONFIG[\"max_length\"])\n",
    "valid_dataset = TranslationDataset(valid_pairs, src_vocab, tgt_vocab, CONFIG[\"max_length\"])\n",
    "print(f\"Dataset sizes | train={len(train_dataset)}, valid={len(valid_dataset)}\")\n",
    "\n",
    "pin_memory = device.type == \"cuda\"\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=pin_memory,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "print(\"DataLoader 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb4bca",
   "metadata": {},
   "source": [
    "## 모델 정의: Encoder, Decoder, Attention\n",
    "\n",
    "- Encoder와 Decoder 모두 GRU 1층 구조를 기본으로 합니다.\n",
    "- 기본 Decoder는 Context 없이 이전 Hidden state만 사용합니다.\n",
    "- Attention Decoder는 Bahdanau 스타일 가중합을 통해 Context vector를 계산합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c564d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, pad_idx: int, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, pad_idx: int, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.output_dim = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor, hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask: torch.Tensor = None):\n",
    "        embedded = self.dropout(self.embedding(input_token)).unsqueeze(1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden, None\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attn_scores = self.v(energy).squeeze(-1)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, pad_idx: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.output_dim = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.attention = BahdanauAttention(hidden_dim)\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 + embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor, hidden: torch.Tensor, encoder_outputs: torch.Tensor, mask: torch.Tensor):\n",
    "        embedded = self.dropout(self.embedding(input_token)).unsqueeze(1)\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        combined = torch.cat((output, context, embedded), dim=2)\n",
    "        prediction = self.fc_out(combined.squeeze(1))\n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module, pad_idx: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        self.latest_attentions = None\n",
    "\n",
    "    def forward(self, src: torch.Tensor, lengths: torch.Tensor, decoder_inputs: torch.Tensor, teacher_forcing_ratio: float = 0.5):\n",
    "        encoder_outputs, hidden = self.encoder(src, lengths)\n",
    "        mask = (src != self.pad_idx)\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = decoder_inputs.size(1)\n",
    "        vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
    "        input_token = decoder_inputs[:, 0]\n",
    "        attentions = []\n",
    "        for t in range(trg_len):\n",
    "            prediction, hidden, attn_weights = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "            outputs[:, t, :] = prediction\n",
    "            attentions.append(attn_weights)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force and t + 1 < trg_len:\n",
    "                input_token = decoder_inputs[:, t + 1]\n",
    "            else:\n",
    "                input_token = prediction.argmax(dim=1)\n",
    "        self.latest_attentions = attentions\n",
    "        return outputs\n",
    "\n",
    "    def greedy_decode(self, src: torch.Tensor, lengths: torch.Tensor, max_len: int, start_token_id: int, end_token_id: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, hidden = self.encoder(src, lengths)\n",
    "            mask = (src != self.pad_idx)\n",
    "            batch_size = src.size(0)\n",
    "            input_token = torch.full((batch_size,), start_token_id, dtype=torch.long, device=self.device)\n",
    "            finished = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
    "            outputs = []\n",
    "            for _ in range(max_len):\n",
    "                prediction, hidden, attn_weights = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                next_token = prediction.argmax(dim=1)\n",
    "                outputs.append(next_token)\n",
    "                finished |= next_token.eq(end_token_id)\n",
    "                input_token = next_token\n",
    "                if finished.all():\n",
    "                    break\n",
    "            if outputs:\n",
    "                predictions = torch.stack(outputs, dim=1)\n",
    "            else:\n",
    "                predictions = torch.zeros((batch_size, 0), dtype=torch.long, device=self.device)\n",
    "        return predictions.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d33e7",
   "metadata": {},
   "source": [
    "## 학습/평가 유틸리티\n",
    "\n",
    "- `corpus_bleu()`는 간단한 N-gram 정밀도를 사용한 BLEU 추정치입니다.\n",
    "- `train_epoch()`와 `evaluate_epoch()`에서 공통 손실 계산, Teacher Forcing, Grad Clipping을 수행합니다.\n",
    "- `translate_sentence()`와 `show_predictions()`로 정성적 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4dad3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corpus_bleu(references: List[List[str]], hypotheses: List[List[str]], max_n: int = 4, smooth_eps: float = 1e-9) -> float:\n",
    "    clipped = [0] * max_n\n",
    "    totals = [0] * max_n\n",
    "    ref_length = 0\n",
    "    hyp_length = 0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_length += max(len(ref), 1)\n",
    "        hyp_length += max(len(hyp), 1)\n",
    "        for n in range(1, max_n + 1):\n",
    "            ref_ngrams = Counter(tuple(ref[i:i + n]) for i in range(max(len(ref) - n + 1, 0)))\n",
    "            hyp_ngrams = Counter(tuple(hyp[i:i + n]) for i in range(max(len(hyp) - n + 1, 0)))\n",
    "            totals[n - 1] += sum(hyp_ngrams.values())\n",
    "            for ng, count in hyp_ngrams.items():\n",
    "                clipped[n - 1] += min(count, ref_ngrams.get(ng, 0))\n",
    "    precisions = []\n",
    "    for n in range(max_n):\n",
    "        numerator = clipped[n] + smooth_eps\n",
    "        denominator = totals[n] + smooth_eps\n",
    "        precisions.append(numerator / denominator)\n",
    "    geo_mean = math.exp(sum((1 / max_n) * math.log(p) for p in precisions))\n",
    "    bp = 1.0 if hyp_length > ref_length else math.exp(1 - ref_length / max(hyp_length, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "\n",
    "def train_epoch(model: Seq2Seq, dataloader: DataLoader, optimizer: torch.optim.Optimizer, criterion: nn.Module, teacher_forcing: float) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        src_lengths = batch[\"src_lengths\"]\n",
    "        decoder_inputs = batch[\"decoder_inputs\"].to(device)\n",
    "        decoder_targets = batch[\"decoder_targets\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lengths, decoder_inputs, teacher_forcing_ratio=teacher_forcing)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_epoch(model: Seq2Seq, dataloader: DataLoader, criterion: nn.Module, tgt_vocab: Vocabulary) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    references, hypotheses = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch[\"src\"].to(device)\n",
    "            src_lengths = batch[\"src_lengths\"]\n",
    "            decoder_inputs = batch[\"decoder_inputs\"].to(device)\n",
    "            decoder_targets = batch[\"decoder_targets\"].to(device)\n",
    "            outputs = model(src, src_lengths, decoder_inputs, teacher_forcing_ratio=0.0)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            max_len = decoder_targets.size(1) + 5\n",
    "            preds = model.greedy_decode(\n",
    "                src,\n",
    "                src_lengths,\n",
    "                max_len=max_len,\n",
    "                start_token_id=tgt_vocab.sos_id,\n",
    "                end_token_id=tgt_vocab.eos_id,\n",
    "            )\n",
    "            for target_ids, pred_ids in zip(decoder_targets.cpu(), preds):\n",
    "                references.append(tgt_vocab.decode(target_ids.tolist()))\n",
    "                hypotheses.append(tgt_vocab.decode(pred_ids.tolist()))\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    return total_loss / len(dataloader), bleu\n",
    "\n",
    "\n",
    "def train_model(model: Seq2Seq, model_name: str, train_loader: DataLoader, valid_loader: DataLoader, tgt_vocab: Vocabulary, epochs: int) -> List[Dict[str, float]]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_id)\n",
    "    history = []\n",
    "    best_bleu = -1.0\n",
    "    best_state = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, CONFIG[\"teacher_forcing\"])\n",
    "        valid_loss, valid_bleu = evaluate_epoch(model, valid_loader, criterion, tgt_vocab)\n",
    "        elapsed = time.time() - start_time\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": valid_loss,\n",
    "                \"valid_bleu\": valid_bleu,\n",
    "                \"time\": elapsed,\n",
    "            }\n",
    "        )\n",
    "        print(f\"[{model_name}] Epoch {epoch}/{epochs} | train {train_loss:.3f} | valid {valid_loss:.3f} | BLEU {valid_bleu:.3f} | {elapsed:.1f}s\")\n",
    "        if valid_bleu > best_bleu:\n",
    "            best_bleu = valid_bleu\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return history\n",
    "\n",
    "\n",
    "def detokenize(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def translate_sentence(text: str, model: Seq2Seq, src_vocab: Vocabulary, tgt_vocab: Vocabulary, max_len: int = None) -> str:\n",
    "    tokens = tokenize_ko(text)\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    seq = src_vocab.encode(tokens, add_sos=True, add_eos=True, max_length=CONFIG[\"max_length\"])\n",
    "    src_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    lengths = torch.tensor([len(seq)], dtype=torch.long)\n",
    "    max_len = max_len or CONFIG[\"max_length\"] + 5\n",
    "    pred_ids = model.greedy_decode(\n",
    "        src_tensor,\n",
    "        lengths,\n",
    "        max_len=max_len,\n",
    "        start_token_id=tgt_vocab.sos_id,\n",
    "        end_token_id=tgt_vocab.eos_id,\n",
    "    )\n",
    "    tokens = tgt_vocab.decode(pred_ids[0].tolist())\n",
    "    return detokenize(tokens)\n",
    "\n",
    "\n",
    "def show_predictions(model: Seq2Seq, sentences: List[str], title: str) -> None:\n",
    "    print(f\"[{title}] 번역 예시\")\n",
    "    for sent in sentences:\n",
    "        pred = translate_sentence(sent, model, src_vocab, tgt_vocab)\n",
    "        print(f\"KO: {sent}\")\n",
    "        print(f\"EN(pred): {pred}\")\n",
    "        print(\"-\")\n",
    "\n",
    "\n",
    "def show_validation_samples(model: Seq2Seq, num_samples: int = 5) -> None:\n",
    "    samples = random.sample(valid_examples, num_samples)\n",
    "    for sample in samples:\n",
    "        pred = translate_sentence(sample[\"ko_text\"], model, src_vocab, tgt_vocab)\n",
    "        print(f\"KO: {sample['ko_text']}\")\n",
    "        print(f\"EN(gt): {sample['en_text']}\")\n",
    "        print(f\"EN(pred): {pred}\")\n",
    "        print(\"=\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123531d",
   "metadata": {},
   "source": [
    "## Seq2Seq (Encoder-Decoder) 학습\n",
    "\n",
    "- Attention 없이 기본 GRU Encoder/Decoder만 사용합니다.\n",
    "- 학습 시간이 길면 `CONFIG['epochs']`나 `CONFIG['train_limit']`를 줄이면서 빠르게 확인해도 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba63d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seq2Seq] Epoch 1/8 | train 5.916 | valid 5.755 | BLEU 0.024 | 65.3s\n",
      "[Seq2Seq] Epoch 2/8 | train 5.206 | valid 5.563 | BLEU 0.035 | 64.6s\n",
      "[Seq2Seq] Epoch 3/8 | train 4.853 | valid 5.507 | BLEU 0.046 | 64.9s\n",
      "[Seq2Seq] Epoch 4/8 | train 4.601 | valid 5.427 | BLEU 0.051 | 64.5s\n",
      "[Seq2Seq] Epoch 5/8 | train 4.389 | valid 5.406 | BLEU 0.058 | 64.9s\n",
      "[Seq2Seq] Epoch 6/8 | train 4.205 | valid 5.389 | BLEU 0.061 | 64.7s\n",
      "[Seq2Seq] Epoch 7/8 | train 4.044 | valid 5.411 | BLEU 0.064 | 64.8s\n",
      "[Seq2Seq] Epoch 8/8 | train 3.884 | valid 5.423 | BLEU 0.066 | 64.8s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RUN_SEQ2SEQ = True\n",
    "seq2seq_model = None\n",
    "seq2seq_history = []\n",
    "\n",
    "if RUN_SEQ2SEQ:\n",
    "    encoder = Encoder(\n",
    "        vocab_size=src_vocab.size,\n",
    "        embed_dim=CONFIG[\"embedding_dim\"],\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        pad_idx=src_vocab.pad_id,\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        dropout=CONFIG[\"dropout\"],\n",
    "    )\n",
    "    decoder = DecoderRNN(\n",
    "        vocab_size=tgt_vocab.size,\n",
    "        embed_dim=CONFIG[\"embedding_dim\"],\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        pad_idx=tgt_vocab.pad_id,\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        dropout=CONFIG[\"dropout\"],\n",
    "    )\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, tgt_vocab.pad_id, device).to(device)\n",
    "    seq2seq_history = train_model(seq2seq_model, \"Seq2Seq\", train_loader, valid_loader, tgt_vocab, CONFIG[\"epochs\"])\n",
    "else:\n",
    "    print(\"Seq2Seq 학습이 비활성화되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95477c",
   "metadata": {},
   "source": [
    "## Seq2Seq + Bahdanau Attention 학습\n",
    "\n",
    "- Attention Decoder가 Encoder 출력 전체를 매 Step 참고하므로 긴 문장 번역에 유리합니다.\n",
    "- 동일한 하이퍼파라미터를 유지한 채 Decoder만 교체하여 효과를 비교합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95606a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seq2Seq+Attention] Epoch 1/8 | train 5.728 | valid 5.521 | BLEU 0.038 | 116.7s\n",
      "[Seq2Seq+Attention] Epoch 2/8 | train 4.808 | valid 5.302 | BLEU 0.056 | 117.0s\n",
      "[Seq2Seq+Attention] Epoch 3/8 | train 4.281 | valid 5.185 | BLEU 0.065 | 116.7s\n",
      "[Seq2Seq+Attention] Epoch 4/8 | train 3.831 | valid 5.224 | BLEU 0.071 | 117.0s\n",
      "[Seq2Seq+Attention] Epoch 5/8 | train 3.501 | valid 5.269 | BLEU 0.076 | 117.0s\n",
      "[Seq2Seq+Attention] Epoch 6/8 | train 3.224 | valid 5.331 | BLEU 0.083 | 117.1s\n",
      "[Seq2Seq+Attention] Epoch 7/8 | train 3.055 | valid 5.379 | BLEU 0.085 | 116.7s\n",
      "[Seq2Seq+Attention] Epoch 8/8 | train 2.901 | valid 5.418 | BLEU 0.085 | 116.9s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RUN_ATTENTION = True\n",
    "attn_model = None\n",
    "attn_history = []\n",
    "\n",
    "if RUN_ATTENTION:\n",
    "    encoder_attn = Encoder(\n",
    "        vocab_size=src_vocab.size,\n",
    "        embed_dim=CONFIG[\"embedding_dim\"],\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        pad_idx=src_vocab.pad_id,\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        dropout=CONFIG[\"dropout\"],\n",
    "    )\n",
    "    decoder_attn = AttnDecoderRNN(\n",
    "        vocab_size=tgt_vocab.size,\n",
    "        embed_dim=CONFIG[\"embedding_dim\"],\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        pad_idx=tgt_vocab.pad_id,\n",
    "        dropout=CONFIG[\"dropout\"],\n",
    "    )\n",
    "    attn_model = Seq2Seq(encoder_attn, decoder_attn, tgt_vocab.pad_id, device).to(device)\n",
    "    attn_history = train_model(attn_model, \"Seq2Seq+Attention\", train_loader, valid_loader, tgt_vocab, CONFIG[\"epochs\"])\n",
    "else:\n",
    "    print(\"Attention 모델 학습이 비활성화되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77152a97",
   "metadata": {},
   "source": [
    "## 번역 예시 확인\n",
    "\n",
    "학습된 모델 가중치가 있다면 아래 셀을 실행해 검증 세트 또는 임의 문장의 번역 품질을 직접 살펴볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3634a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seq2Seq] 번역 예시\n",
      "KO: 오늘 저녁에 시간 있어?\n",
      "EN(pred): do you have to do today?\n",
      "-\n",
      "KO: 비가 올 것 같으니까 우산을 챙겨.\n",
      "EN(pred): i think you can just a <unk>\n",
      "-\n",
      "KO: 이번 프로젝트 일정은 어떻게 조정할까요?\n",
      "EN(pred): how can i ask for this week?\n",
      "-\n",
      "[Seq2Seq] 검증 샘플\n",
      "KO: 케이스 포함 제품입니다.\n",
      "EN(gt): This product includes a case.\n",
      "EN(pred): it is a product product.\n",
      "=\n",
      "KO: >대한민국, 대한민국이 아니고 진짜 솔직히 제가 볼 때 진짜 최고 수준인 것 같아요.\n",
      "EN(gt): >Korea, not Korea, but honestly, I think it's at the highest level.\n",
      "EN(pred): i think it's a really <unk> but i think i really like to eat it in the\n",
      "=\n",
      "KO: 진화하는 라이프스타일에서 영감받은 라이트라이드 테니스 신발 컬렉션입니다.\n",
      "EN(gt): A collection of lightride tennis shoes inspired by the evolving lifestyle.\n",
      "EN(pred): the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "=\n",
      "KO: >양복 사고 그랬지.\n",
      "EN(gt): We bought suits.\n",
      "EN(pred): i also <unk> the <unk> of the <unk>\n",
      "=\n",
      "KO: 컬러 선택도 가능하며 사이즈 선택도 가능합니다.\n",
      "EN(gt): You can choose a color and a size.\n",
      "EN(pred): it can be operated with a <unk>\n",
      "=\n",
      "[Seq2Seq+Attention] 번역 예시\n",
      "KO: 오늘 저녁에 시간 있어?\n",
      "EN(pred): do you have a meeting at the\n",
      "-\n",
      "KO: 비가 올 것 같으니까 우산을 챙겨.\n",
      "EN(pred): it looks like it looks like it looks like it looks like\n",
      "-\n",
      "KO: 이번 프로젝트 일정은 어떻게 조정할까요?\n",
      "EN(pred): how do this this this month?\n",
      "-\n",
      "[Seq2Seq+Attention] 검증 샘플\n",
      "KO: >이야.\n",
      "EN(gt): It's\n",
      "EN(pred): wow\n",
      "=\n",
      "KO: AAA가 학교에 집중 할 수 있을까요?\n",
      "EN(gt): Do you think AAA can focus on school?\n",
      "EN(pred): can i do the <unk>\n",
      "=\n",
      "KO: 은행에서 전송 시스템이 복구되는 대로 대금이 지급될 예정입니다.\n",
      "EN(gt): Payments will be made as soon as the transfer system is restored by the bank.\n",
      "EN(pred): as a the the as the as the as the as a as as as a as a as a as a as a as a as a as possible.\n",
      "=\n",
      "KO: 신청서에는 참가 가격에 대한 안내도 기재되어있습니다.\n",
      "EN(gt): The application also contains information on the participation price.\n",
      "EN(pred): the price of the is is also the the the price of the <unk>\n",
      "=\n",
      "KO: >10분만 자자, 여기서 자자.\n",
      "EN(gt): >Let's sleep for 10 minutes. Let's sleep here.\n",
      "EN(pred): i'm just a a peak season,\n",
      "=\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_sentences = [\n",
    "    \"오늘 저녁에 시간 있어?\",\n",
    "    \"비가 올 것 같으니까 우산을 챙겨.\",\n",
    "    \"이번 프로젝트 일정은 어떻게 조정할까요?\",\n",
    "]\n",
    "\n",
    "if seq2seq_model is not None:\n",
    "    show_predictions(seq2seq_model, custom_sentences, title=\"Seq2Seq\")\n",
    "    print(\"[Seq2Seq] 검증 샘플\")\n",
    "    show_validation_samples(seq2seq_model, num_samples=5)\n",
    "\n",
    "if attn_model is not None:\n",
    "    show_predictions(attn_model, custom_sentences, title=\"Seq2Seq+Attention\")\n",
    "    print(\"[Seq2Seq+Attention] 검증 샘플\")\n",
    "    show_validation_samples(attn_model, num_samples=5)\n",
    "\n",
    "if seq2seq_model is None and attn_model is None:\n",
    "    print(\"먼저 모델을 학습시켜 주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8cb91",
   "metadata": {},
   "source": [
    "## 다음에 시도해 볼 아이디어\n",
    "\n",
    "- SentencePiece/BPE 등을 적용해 희귀 단어 문제를 줄여 보기\n",
    "- Encoder/Decoder 다층화, 양방향 Encoder, Dropout 비율 조정\n",
    "- Teacher Forcing 비율을 Epoch에 따라 점감시키기\n",
    "- BLEU 외에 chrF, ROUGE 등 다른 정량 지표 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa5303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
