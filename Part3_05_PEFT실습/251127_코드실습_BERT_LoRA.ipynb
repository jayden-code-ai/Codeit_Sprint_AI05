{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb5fbac",
   "metadata": {},
   "source": [
    "# 1. LoRA - BERT 모델을 사용한 PEFT\n",
    "BERT 모델을 사용하여 PEFT(LoRA) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ceff9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Using cached triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "Using cached torchaudio-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, triton, tqdm, sympy, safetensors, regex, pyyaml, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface_hub, torch, tokenizers, transformers, torchvision, torchaudio\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "\u001b[2K  Attempting uninstall: mpmath━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/43\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0[0m \u001b[32m 0/43\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/43\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0━\u001b[0m \u001b[32m 0/43\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/43\u001b[0m [mpmath]12]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━\u001b[0m \u001b[32m 1/43\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/43\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━\u001b[0m \u001b[32m 1/43\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: triton━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: triton 3.5.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling triton-3.5.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled triton-3.5.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/43\u001b[0m [urllib3]\n",
      "\u001b[2K  Attempting uninstall: tqdm0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/43\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: safetensors━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: safetensors 0.7.0━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling safetensors-0.7.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: regex━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: regex 2025.11.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling regex-2025.11.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled regex-2025.11.3━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: pyyaml━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/43\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: pillow\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: pillow 12.0.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling pillow-12.0.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled pillow-12.0.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/43\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: packaging90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.90━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.90━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvshmem-cu12━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvshmem-cu12 3.3.20━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvshmem-cu12-3.3.20:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvshmem-cu12-3.3.20━━━━━\u001b[0m \u001b[32m10/43\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.8.93:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93━━\u001b[0m \u001b[32m13/43\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.5━━━━━━\u001b[0m \u001b[32m14/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.5:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.5━━━━━━━━\u001b[0m \u001b[32m14/43\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/43\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.90━\u001b[0m \u001b[32m15/43\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/43\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.90━━━\u001b[0m \u001b[32m15/43\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.13.1.3━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.13.1.3:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93━\u001b[0m \u001b[32m16/43\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/43\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90[0m \u001b[32m19/43\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/43\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90━\u001b[0m \u001b[32m19/43\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu1290m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/43\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.1━━\u001b[0m \u001b[32m20/43\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/43\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1━━━━\u001b[0m \u001b[32m20/43\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: numpy0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/43\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: networkx\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: networkx 3.4.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.4.2:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.4.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/43\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.3━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.3:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: idnam\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: idna 3.110m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling idna-3.11:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled idna-3.11[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/43\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: hf-xet━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.2.0m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.2.0:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.2.090m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspecm\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.10.0━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.10.0:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.10.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: filelock[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: filelock 3.20.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling filelock-3.20.0:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.20.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.4━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.4:[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.4━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: certifi\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: certifi 2025.11.12━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling certifi-2025.11.12:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.11.12━━━━━━━━━━━━━\u001b[0m \u001b[32m25/43\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: requests━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K    Found existing installation: requests 2.32.50m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.8.93[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.8.93:0m━━━━━━━━━━━━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93━\u001b[0m \u001b[32m30/43\u001b[0m [certifi]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.83━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.83:0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83━━━━\u001b[0m \u001b[32m32/43\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m33/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21━━\u001b[0m \u001b[32m33/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m33/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21━━━━\u001b[0m \u001b[32m33/43\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: jinja2━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.6\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.6m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m34/43\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [jinja2]n-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.3.90[0m \u001b[32m35/43\u001b[0m [jinja2]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.3.90:0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m35/43\u001b[0m [jinja2]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90━\u001b[0m \u001b[32m35/43\u001b[0m [jinja2]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.090m━━━━━━\u001b[0m \u001b[32m36/43\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m37/43\u001b[0m [huggingface_hub]]\n",
      "\u001b[2K    Found existing installation: torch 2.9.190m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m37/43\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling torch-2.9.1:━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]e_hub]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.1━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: tokenizers━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.20.3[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling tokenizers-0.20.3:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.20.3╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: transformers━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: transformers 4.45.2m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling transformers-4.45.2:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m38/43\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.45.2[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m40/43\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m40/43\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: torchvision 0.24.1\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m40/43\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling torchvision-0.24.1:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m40/43\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.24.1m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m40/43\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torchaudio━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41/43\u001b[0m [torchvision]\n",
      "\u001b[2K    Found existing installation: torchaudio 2.9.10m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41/43\u001b[0m [torchvision]\n",
      "\u001b[2K    Uninstalling torchaudio-2.9.1:━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41/43\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchaudio-2.9.1[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41/43\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43/43\u001b[0m [torchaudio]rchaudio]ision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface_hub-0.36.0 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pillow-12.0.0 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1 tqdm-4.67.1 transformers-4.57.3 triton-3.5.1 typing-extensions-4.15.0 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 현재 연결된 커널 환경에 강제로 재설치합니다.\n",
    "%pip install --force-reinstall torch torchvision torchaudio transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77883848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# 위젯 대신 login 함수 안에 토큰을 직접 넣어서 로그인합니다.\n",
    "# \"hf_...\" 부분에 복사해둔 토큰을 따옴표 안에 넣어주세요.\n",
    "login(token=\"hf_...\", write_permission=True)\n",
    "\n",
    "# 만약 모델을 업로드(저장)까지 해야 한다면 아래처럼 옵션을 추가할 수 있습니다.\n",
    "# login(token=\"hf_...\", write_permission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f84a97",
   "metadata": {},
   "source": [
    "## BERT 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eca0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# BERT 로드\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6\n",
    ")\n",
    "\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8737cb8",
   "metadata": {},
   "source": [
    "## 데이터세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f45284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터세트\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56343e5",
   "metadata": {},
   "source": [
    "## 토크나이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ead5f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed817ac726b44950ab5c813432d7096b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_ds = tokenized[\"train\"]\n",
    "eval_ds  = tokenized[\"validation\"]\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1be9f3",
   "metadata": {},
   "source": [
    "##  PEFT - Lora 설정과 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc346c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: evaluate in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: transformers in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: dill in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from responses<0.19->evaluate) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers->peft) (0.22.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# peft와 evaluate 라이브러리 설치\n",
    "%pip install peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761b1872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.2\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (0.7.0)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from transformers==4.45.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jayden86/miniconda3/envs/codeit_1/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2025.11.12)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.57.3\n",
      "\u001b[2K    Uninstalling transformers-4.57.3:\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.3━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "trl 0.25.1 requires transformers>=4.56.1, but you have transformers 4.45.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.20.3 transformers-4.45.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 호환성 문제를 해결하기 위해 transformers 버전을 4.45.2로 재설치합니다.\n",
    "%pip install transformers==4.45.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d812d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jayden86/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Fri Nov 21 11:26:24 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "/tmp/ipykernel_243916/1542683702.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 42:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.826700</td>\n",
       "      <td>1.771703</td>\n",
       "      <td>0.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.741900</td>\n",
       "      <td>1.721143</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.718600</td>\n",
       "      <td>1.673490</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.671500</td>\n",
       "      <td>1.630572</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.640300</td>\n",
       "      <td>1.597500</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.608100</td>\n",
       "      <td>1.585284</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>1.579700</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.616100</td>\n",
       "      <td>1.579118</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.594400</td>\n",
       "      <td>1.576738</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.562400</td>\n",
       "      <td>1.575608</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.557000</td>\n",
       "      <td>1.574632</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.578100</td>\n",
       "      <td>1.576175</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.576800</td>\n",
       "      <td>1.574569</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.541300</td>\n",
       "      <td>1.580319</td>\n",
       "      <td>0.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.548800</td>\n",
       "      <td>1.589111</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.626400</td>\n",
       "      <td>1.573719</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.564700</td>\n",
       "      <td>1.567913</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.596000</td>\n",
       "      <td>1.563261</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.515500</td>\n",
       "      <td>1.560443</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.560200</td>\n",
       "      <td>1.552886</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.512200</td>\n",
       "      <td>1.536177</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.534100</td>\n",
       "      <td>1.524970</td>\n",
       "      <td>0.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.489100</td>\n",
       "      <td>1.496861</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.473900</td>\n",
       "      <td>1.469519</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.459500</td>\n",
       "      <td>1.442700</td>\n",
       "      <td>0.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.471800</td>\n",
       "      <td>1.416737</td>\n",
       "      <td>0.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.408400</td>\n",
       "      <td>1.390690</td>\n",
       "      <td>0.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.408300</td>\n",
       "      <td>1.366421</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.348000</td>\n",
       "      <td>1.353800</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.318400</td>\n",
       "      <td>1.337625</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.397600</td>\n",
       "      <td>1.322944</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.354000</td>\n",
       "      <td>1.310810</td>\n",
       "      <td>0.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.413200</td>\n",
       "      <td>1.299814</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.255100</td>\n",
       "      <td>1.284277</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.283700</td>\n",
       "      <td>1.270397</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.265100</td>\n",
       "      <td>1.263196</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.190500</td>\n",
       "      <td>1.252749</td>\n",
       "      <td>0.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.250800</td>\n",
       "      <td>1.245249</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.226000</td>\n",
       "      <td>1.230046</td>\n",
       "      <td>0.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.361000</td>\n",
       "      <td>1.223553</td>\n",
       "      <td>0.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.181100</td>\n",
       "      <td>1.206757</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.201400</td>\n",
       "      <td>1.206324</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.255300</td>\n",
       "      <td>1.193936</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.172400</td>\n",
       "      <td>1.185766</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.242200</td>\n",
       "      <td>1.178060</td>\n",
       "      <td>0.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.229000</td>\n",
       "      <td>1.200446</td>\n",
       "      <td>0.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.208800</td>\n",
       "      <td>1.157272</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.178300</td>\n",
       "      <td>1.155825</td>\n",
       "      <td>0.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.194400</td>\n",
       "      <td>1.150314</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.259500</td>\n",
       "      <td>1.145794</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.218900</td>\n",
       "      <td>1.140203</td>\n",
       "      <td>0.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.195400</td>\n",
       "      <td>1.138372</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.197100</td>\n",
       "      <td>1.132498</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.170400</td>\n",
       "      <td>1.128779</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.079800</td>\n",
       "      <td>1.129640</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.126048</td>\n",
       "      <td>0.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.180500</td>\n",
       "      <td>1.121075</td>\n",
       "      <td>0.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.054700</td>\n",
       "      <td>1.114319</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>1.113884</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.111600</td>\n",
       "      <td>1.100430</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>1.098849</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.090800</td>\n",
       "      <td>1.090415</td>\n",
       "      <td>0.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.059000</td>\n",
       "      <td>1.087762</td>\n",
       "      <td>0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.113900</td>\n",
       "      <td>1.083005</td>\n",
       "      <td>0.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.092600</td>\n",
       "      <td>1.090943</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.081027</td>\n",
       "      <td>0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.147900</td>\n",
       "      <td>1.067226</td>\n",
       "      <td>0.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.122900</td>\n",
       "      <td>1.063548</td>\n",
       "      <td>0.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.156500</td>\n",
       "      <td>1.062368</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.053100</td>\n",
       "      <td>1.059774</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.173200</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.025200</td>\n",
       "      <td>1.049987</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.126600</td>\n",
       "      <td>1.045193</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.092400</td>\n",
       "      <td>1.045164</td>\n",
       "      <td>0.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>1.039859</td>\n",
       "      <td>0.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>1.042343</td>\n",
       "      <td>0.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.084300</td>\n",
       "      <td>1.030419</td>\n",
       "      <td>0.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>1.031834</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.018800</td>\n",
       "      <td>1.025622</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.069600</td>\n",
       "      <td>1.021716</td>\n",
       "      <td>0.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.061200</td>\n",
       "      <td>1.020351</td>\n",
       "      <td>0.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.060300</td>\n",
       "      <td>1.017137</td>\n",
       "      <td>0.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.063000</td>\n",
       "      <td>1.016558</td>\n",
       "      <td>0.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.063200</td>\n",
       "      <td>0.999617</td>\n",
       "      <td>0.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.030400</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.053900</td>\n",
       "      <td>0.993218</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.986109</td>\n",
       "      <td>0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.984148</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>0.982954</td>\n",
       "      <td>0.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.952500</td>\n",
       "      <td>0.981220</td>\n",
       "      <td>0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.090900</td>\n",
       "      <td>0.973418</td>\n",
       "      <td>0.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.007800</td>\n",
       "      <td>0.972016</td>\n",
       "      <td>0.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.009600</td>\n",
       "      <td>0.968466</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.972400</td>\n",
       "      <td>0.971195</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.967734</td>\n",
       "      <td>0.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>0.962064</td>\n",
       "      <td>0.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.035700</td>\n",
       "      <td>0.959486</td>\n",
       "      <td>0.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.047100</td>\n",
       "      <td>0.958916</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.044100</td>\n",
       "      <td>0.960927</td>\n",
       "      <td>0.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.902300</td>\n",
       "      <td>0.953379</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.983100</td>\n",
       "      <td>0.949919</td>\n",
       "      <td>0.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>0.951027</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>0.947069</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>0.946730</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>0.946486</td>\n",
       "      <td>0.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.946800</td>\n",
       "      <td>0.943897</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.913400</td>\n",
       "      <td>0.940316</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.029700</td>\n",
       "      <td>0.940355</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.939226</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>0.938097</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.937053</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.965400</td>\n",
       "      <td>0.934706</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.930421</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.934763</td>\n",
       "      <td>0.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.945600</td>\n",
       "      <td>0.933008</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.927024</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.926748</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.929014</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.919536</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.916574</td>\n",
       "      <td>0.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917957</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.935800</td>\n",
       "      <td>0.917137</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.914570</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.912365</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.881400</td>\n",
       "      <td>0.914393</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>0.914086</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.913117</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.914203</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>0.914248</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.913770</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.911118</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.874300</td>\n",
       "      <td>0.909890</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.021500</td>\n",
       "      <td>0.910058</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.909070</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.952900</td>\n",
       "      <td>0.910279</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.910889</td>\n",
       "      <td>0.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.908950</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.907779</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.906601</td>\n",
       "      <td>0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>0.905711</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.946800</td>\n",
       "      <td>0.904982</td>\n",
       "      <td>0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.904948</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.914600</td>\n",
       "      <td>0.905252</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.935700</td>\n",
       "      <td>0.905203</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.904088</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.903004</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.948400</td>\n",
       "      <td>0.902781</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.851300</td>\n",
       "      <td>0.902819</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>0.902805</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=1.1467801710764567, metrics={'train_runtime': 2575.1337, 'train_samples_per_second': 18.64, 'train_steps_per_second': 1.165, 'total_flos': 3192948965376000.0, 'train_loss': 1.1467801710764567, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "# LoRA 구성\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # 시퀀스 분류\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n",
    ")\n",
    "\n",
    "# LoRA 적용\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Trainer 구성\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_bert_mrpc\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 평가 메트릭\n",
    "metric = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 8. 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76fbf9a",
   "metadata": {},
   "source": [
    "## 성능 평가(Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49575de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 정확도(Accuracy): 0.6565\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"최종 정확도(Accuracy): {eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb1155",
   "metadata": {},
   "source": [
    "## 모델 저장\n",
    "\n",
    "LoRA는 모델 전체가 아니라 학습된'어댑터'부분만 저정하면 됩니다. 용량이 매우 작어서 효율적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3349c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 ./bert-emotion-lora에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습된 LoRA 모델 Adapter 저장\n",
    "save_dir='./bert-emotion-lora'\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f'모델이 {save_dir}에 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c07924",
   "metadata": {},
   "source": [
    "## 실제 테스트 (Inference)\n",
    "저장된 모델이 새로운 문장을 보고 감정을 잘 맞추는지 직접 테스트해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a86a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: I feel so happy and excited today!\n",
      "예측 감정: joy\n"
     ]
    }
   ],
   "source": [
    "# 테스트: 문장을 넣어 감정 에측해보기\n",
    "import torch\n",
    "\n",
    "# 테스트 문장\n",
    "text = 'I feel so happy and excited today!'\n",
    "\n",
    "# 입력 변환(토큰화)\n",
    "inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "\n",
    "# 모델 예측\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "# 결과 출력 (Emotion 데이터셋 라벨 매핑)\n",
    "# 0: sadness, 1: joy, 2: love, 3: anger, 4: fear, 5: surprise\n",
    "labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "print(f\"입력 문장: {text}\")\n",
    "print(f\"예측 감정: {labels[predicted_class_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12754c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeit_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
